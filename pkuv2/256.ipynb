{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "256.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "accelerator": "GPU",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "kHwmHF3W-ycN",
    "colab_type": "code",
    "outputId": "162a59d2-43b3-49f8-c530-5c04b9112ee7",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    }
   },
   "source": [
    "import os\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.layers import Dense, Dropout, Activation, LSTM, Bidirectional, Input, Reshape, Permute, Lambda, TimeDistributed, SpatialDropout1D\n",
    "from keras.layers.merge import Add\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fTHrv35G-3Vt",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def load_seq_lst(lst_name, num_seq, ovr_num=None, num_joints=25, num_class=52):\n",
    "    assert (os.path.isfile(lst_name + '.txt') and os.path.isfile(lst_name + '.h5'))\n",
    "    if ovr_num is None:\n",
    "        ovr_num = int(num_seq / 2)\n",
    "    keyname_lst = [item.strip().split() for item in open(lst_name + '.txt', 'r').readlines()]\n",
    "    X = []\n",
    "    Y = []\n",
    "    vid_list = []\n",
    "    start_list = []\n",
    "    with h5py.File(lst_name + '.h5', 'r') as hf:\n",
    "        for item in keyname_lst:\n",
    "            skeleton = np.asarray(hf.get(item[0]))\n",
    "            skeleton1 = skeleton[:, 0:75].reshape((skeleton.shape[0], num_joints, 3))\n",
    "            skeleton2 = skeleton[:, 75:].reshape((skeleton.shape[0], num_joints, 3))\n",
    "            skeleton = np.concatenate((skeleton1, skeleton2), axis=-1)\n",
    "\n",
    "            labels = np.asarray(hf.get(item[1]), dtype=int)\n",
    "            labels_pertime = np.zeros((skeleton.shape[0]), dtype=np.int32)\n",
    "            for clip_idx in range(len(labels)):\n",
    "                labels_pertime[labels[clip_idx][1]:labels[clip_idx][2]] = labels[clip_idx][0] + 1\n",
    "            labels_pertime = labels_pertime.astype(np.int32)\n",
    "            labels_pertime = np_utils.to_categorical(labels_pertime, num_class)\n",
    "\n",
    "            if skeleton.shape[0] > num_seq:\n",
    "                start = 0\n",
    "                while start + num_seq < skeleton.shape[0]:\n",
    "                    X.append(skeleton[start:start + num_seq])\n",
    "                    Y.append(labels_pertime[start + num_seq])\n",
    "                    vid_list.append(item[0])\n",
    "                    start_list.append(start)\n",
    "                    start = start + ovr_num\n",
    "                X.append(skeleton[-num_seq:])\n",
    "                Y.append(labels_pertime[-1])\n",
    "                vid_list.append(item[0])\n",
    "                start_list.append(skeleton.shape[0] - num_seq)\n",
    "            else:\n",
    "                skeleton = np.concatenate((np.zeros((num_seq - skeleton.shape[0], skeleton.shape[1], skeleton.shape[2])), skeleton), axis=0)\n",
    "                labels_pertime = np.concatenate((np.zeros((num_seq - labels_pertime.shape[0], labels_pertime.shape[1])), labels_pertime), axis=0)\n",
    "                X.append(skeleton)\n",
    "                Y.append(labels_pertime[-1])\n",
    "                vid_list.append(item[0])\n",
    "                start_list.append(0)\n",
    "\n",
    "    X = np.asarray(X).astype(np.float32)\n",
    "    Y = np.asarray(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y, vid_list, start_list"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rZOuSHXx-5cT",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def multi_pertime(param, sub_mean=False):\n",
    "    skt_input = Input(shape=(param['num_seq'], param['num_joints'], param['dim_point']))\n",
    "    data = skt_input\n",
    "\n",
    "    if sub_mean:\n",
    "        data = Permute((1, 3, 2))(data)\n",
    "        data2 = TimeDistributed(TimeDistributed(Dense(1, trainable=False)))(data)\n",
    "        data2 = Lambda(lambda x: K.repeat_elements(x, param['num_joints'], axis=-1), output_shape=lambda s: (s[0], s[1], s[2], s[3] * param['num_joints']))(data2)\n",
    "        data = Add()([data, data2])\n",
    "\n",
    "    data = Reshape((param['num_seq'], param['num_joints'] * param['dim_point']))(data)\n",
    "\n",
    "    hid_size = 256\n",
    "    data = SpatialDropout1D(0.05)(data)\n",
    "    out = Bidirectional(LSTM(hid_size, return_sequences=True))(data)\n",
    "    #out = SpatialDropout1D(0.05)(out)\n",
    "    out = Bidirectional(LSTM(hid_size, return_sequences=True))(out)\n",
    "    #out = SpatialDropout1D(0.05)(out)\n",
    "    out = Bidirectional(LSTM(hid_size, return_sequences=False))(out)\n",
    "\n",
    "    # out = BatchNormalization()(out)\n",
    "    out = Activation('relu')(Dropout(0.5)(out))\n",
    "    prob = Dense(param['num_class'], activation='softmax')(out)\n",
    "\n",
    "    opt = SGD(lr=param['base_learn_rate'], decay=param['weight_regular'], momentum=0.9, nesterov=True)\n",
    "    model = Model(skt_input, prob)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oOpHGRYd_FEA",
    "colab_type": "code",
    "outputId": "83a6cdaf-a974-4734-8965-bb46a75ae474",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    }
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5afvV-DdtUcL",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def schedule(epoch):\n",
    "    lr = K.get_value(model.optimizer.lr)\n",
    "    if epoch % param['step_inter'] == 0 and epoch > 0:\n",
    "        lr = lr * param['lr_gamma']\n",
    "    return np.float(lr)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PFe5IAPg-7gU",
    "colab_type": "code",
    "outputId": "ba8c02db-8dd7-414e-ce61-2edee35bc7c6",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    }
   },
   "source": [
    "param = {'max_iter': 500, 'step_inter': 80, 'base_learn_rate': 0.01, 'lr_gamma': 0.5, 'weight_regular': 0, 'num_seq': 50, 'batchsize': 1024, 'write_file': True,\n",
    "         'num_joints': 25, 'num_class': 51 + 1, 'dim_point': 3 * 2, 'save_model': True}\n",
    "param['trn_file'] = F\"/content/drive/My Drive/crs_view_trn_skt\"\n",
    "param['val_file'] = F\"/content/drive/My Drive/crs_view_val_skt\"\n",
    "param['save_path'] = F\"/content/drive/My Drive/save_param_temp/gcn/weights.hdf5\"\n",
    "param['write_file_name'] = F\"/content/drive/My Drive/save_param_temp/gcn.txt\"\n",
    "# no batch size, as one batch only\n",
    "\n",
    "trainX, trainY, train_vid_list, train_start_list = load_seq_lst(param['trn_file'], param['num_seq'])\n",
    "valX, valY, val_vid_list, val_start_list = load_seq_lst(param['val_file'], param['num_seq'])"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "(22708, 50, 25, 6) (22708, 52)\n",
      "(12767, 50, 25, 6) (12767, 52)\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rJ_AM7qAAgEg",
    "colab_type": "code",
    "outputId": "8fe3652c-bd42-4455-c97d-2e480573405a",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "source": [
    "model = multi_pertime(param, sub_mean=True)\n",
    "reduce_lr = LearningRateScheduler(schedule)\n",
    "checkpoint = ModelCheckpoint(param['save_path'], verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=5)\n",
    "\n",
    "model.fit(trainX, trainY, batch_size=param['batchsize'], epochs=param['max_iter'], shuffle=True, validation_data=(valX, valY),callbacks=[checkpoint], verbose=1)"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 50, 25, 6)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "permute_3 (Permute)             (None, 50, 6, 25)    0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_6 (TimeDistrib (None, 50, 6, 1)     26          permute_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 50, 6, 25)    0           time_distributed_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 50, 6, 25)    0           permute_3[0][0]                  \n",
      "                                                                 lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 50, 150)      0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_7 (SpatialDro (None, 50, 150)      0           reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 50, 512)      833536      spatial_dropout1d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, 50, 512)      1574912     bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) (None, 512)          1574912     bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 512)          0           bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 512)          0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 52)           26676       activation_2[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 4,010,062\n",
      "Trainable params: 4,010,062\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 22708 samples, validate on 12767 samples\n",
      "Epoch 1/500\n",
      "22708/22708 [==============================] - 20s 863us/step - loss: 3.2587 - acc: 0.3682 - val_loss: 2.9846 - val_acc: 0.4072\n",
      "Epoch 2/500\n",
      "22708/22708 [==============================] - 15s 681us/step - loss: 2.9104 - acc: 0.4029 - val_loss: 2.8242 - val_acc: 0.4072\n",
      "Epoch 3/500\n",
      "22708/22708 [==============================] - 16s 686us/step - loss: 2.8459 - acc: 0.4029 - val_loss: 2.7900 - val_acc: 0.4072\n",
      "Epoch 4/500\n",
      "22708/22708 [==============================] - 15s 681us/step - loss: 2.8193 - acc: 0.4029 - val_loss: 2.7673 - val_acc: 0.4072\n",
      "Epoch 5/500\n",
      "22708/22708 [==============================] - 15s 682us/step - loss: 2.8047 - acc: 0.4029 - val_loss: 2.7725 - val_acc: 0.4072\n",
      "Epoch 6/500\n",
      "22708/22708 [==============================] - 15s 681us/step - loss: 2.7939 - acc: 0.4029 - val_loss: 2.7525 - val_acc: 0.4072\n",
      "Epoch 7/500\n",
      "22708/22708 [==============================] - 15s 677us/step - loss: 2.7810 - acc: 0.4029 - val_loss: 2.7533 - val_acc: 0.4072\n",
      "Epoch 8/500\n",
      "22708/22708 [==============================] - 15s 680us/step - loss: 2.7606 - acc: 0.4029 - val_loss: 2.7548 - val_acc: 0.4072\n",
      "Epoch 9/500\n",
      "22708/22708 [==============================] - 15s 678us/step - loss: 2.7334 - acc: 0.4029 - val_loss: 2.7527 - val_acc: 0.4072\n",
      "Epoch 10/500\n",
      "22708/22708 [==============================] - 16s 684us/step - loss: 2.7042 - acc: 0.4029 - val_loss: 2.7642 - val_acc: 0.4072\n",
      "Epoch 11/500\n",
      "22708/22708 [==============================] - 16s 683us/step - loss: 2.6851 - acc: 0.4029 - val_loss: 2.7725 - val_acc: 0.4072\n",
      "Epoch 12/500\n",
      "22708/22708 [==============================] - 15s 679us/step - loss: 2.6643 - acc: 0.4029 - val_loss: 2.7940 - val_acc: 0.4072\n",
      "Epoch 13/500\n",
      "22708/22708 [==============================] - 15s 679us/step - loss: 2.6446 - acc: 0.4030 - val_loss: 2.7879 - val_acc: 0.4072\n",
      "Epoch 14/500\n",
      "22708/22708 [==============================] - 15s 681us/step - loss: 2.6340 - acc: 0.4031 - val_loss: 2.7743 - val_acc: 0.4072\n",
      "Epoch 15/500\n",
      "22708/22708 [==============================] - 15s 678us/step - loss: 2.6157 - acc: 0.4028 - val_loss: 2.8137 - val_acc: 0.4072\n",
      "Epoch 16/500\n",
      "22708/22708 [==============================] - 16s 684us/step - loss: 2.6036 - acc: 0.4029 - val_loss: 2.8178 - val_acc: 0.4072\n",
      "Epoch 17/500\n",
      "22708/22708 [==============================] - 15s 677us/step - loss: 2.5973 - acc: 0.4026 - val_loss: 2.7885 - val_acc: 0.4072\n",
      "Epoch 18/500\n",
      "22708/22708 [==============================] - 15s 679us/step - loss: 2.5863 - acc: 0.4028 - val_loss: 2.8104 - val_acc: 0.4072\n",
      "Epoch 19/500\n",
      "22708/22708 [==============================] - 15s 675us/step - loss: 2.5721 - acc: 0.4025 - val_loss: 2.8043 - val_acc: 0.4072\n",
      "Epoch 20/500\n",
      "22708/22708 [==============================] - 15s 679us/step - loss: 2.5613 - acc: 0.4027 - val_loss: 2.7193 - val_acc: 0.4072\n",
      "Epoch 21/500\n",
      "22708/22708 [==============================] - 15s 681us/step - loss: 2.5563 - acc: 0.4025 - val_loss: 2.7669 - val_acc: 0.4062\n",
      "Epoch 22/500\n",
      "22708/22708 [==============================] - 15s 679us/step - loss: 2.5429 - acc: 0.4039 - val_loss: 2.7859 - val_acc: 0.4048\n",
      "Epoch 23/500\n",
      "22708/22708 [==============================] - 16s 683us/step - loss: 2.5255 - acc: 0.4034 - val_loss: 2.7563 - val_acc: 0.4062\n",
      "Epoch 24/500\n",
      "22708/22708 [==============================] - 15s 679us/step - loss: 2.5294 - acc: 0.4036 - val_loss: 2.7507 - val_acc: 0.4063\n",
      "Epoch 25/500\n",
      "22708/22708 [==============================] - 15s 678us/step - loss: 2.5112 - acc: 0.4034 - val_loss: 2.7299 - val_acc: 0.4060\n",
      "Epoch 26/500\n",
      "22708/22708 [==============================] - 15s 681us/step - loss: 2.5063 - acc: 0.4040 - val_loss: 2.7805 - val_acc: 0.4062\n",
      "Epoch 27/500\n",
      "22708/22708 [==============================] - 15s 678us/step - loss: 2.5050 - acc: 0.4045 - val_loss: 2.6660 - val_acc: 0.4056\n",
      "Epoch 28/500\n",
      "22708/22708 [==============================] - 15s 679us/step - loss: 2.4813 - acc: 0.4049 - val_loss: 2.7224 - val_acc: 0.4029\n",
      "Epoch 29/500\n",
      "22708/22708 [==============================] - 15s 678us/step - loss: 2.4929 - acc: 0.4033 - val_loss: 2.7039 - val_acc: 0.4065\n",
      "Epoch 30/500\n",
      "22708/22708 [==============================] - 15s 676us/step - loss: 2.4648 - acc: 0.4052 - val_loss: 2.6547 - val_acc: 0.4046\n",
      "Epoch 31/500\n",
      "22708/22708 [==============================] - 16s 683us/step - loss: 2.4722 - acc: 0.4050 - val_loss: 2.7768 - val_acc: 0.4068\n",
      "Epoch 32/500\n",
      "22708/22708 [==============================] - 15s 678us/step - loss: 2.4511 - acc: 0.4065 - val_loss: 2.7509 - val_acc: 0.4031\n",
      "Epoch 33/500\n",
      "22708/22708 [==============================] - 15s 676us/step - loss: 2.4509 - acc: 0.4061 - val_loss: 2.6203 - val_acc: 0.4061\n",
      "Epoch 34/500\n",
      "22708/22708 [==============================] - 15s 680us/step - loss: 2.4447 - acc: 0.4068 - val_loss: 2.7346 - val_acc: 0.4072\n",
      "Epoch 35/500\n",
      "22708/22708 [==============================] - 15s 680us/step - loss: 2.4374 - acc: 0.4062 - val_loss: 2.6849 - val_acc: 0.4055\n",
      "Epoch 36/500\n",
      "22708/22708 [==============================] - 15s 682us/step - loss: 2.4220 - acc: 0.4069 - val_loss: 2.6836 - val_acc: 0.4071\n",
      "Epoch 37/500\n",
      "22708/22708 [==============================] - 15s 679us/step - loss: 2.4159 - acc: 0.4062 - val_loss: 2.5940 - val_acc: 0.4069\n",
      "Epoch 38/500\n",
      "22708/22708 [==============================] - 15s 677us/step - loss: 2.4171 - acc: 0.4066 - val_loss: 2.5734 - val_acc: 0.4062\n",
      "Epoch 39/500\n",
      "22708/22708 [==============================] - 15s 676us/step - loss: 2.4030 - acc: 0.4085 - val_loss: 2.6710 - val_acc: 0.4003\n",
      "Epoch 40/500\n",
      "22708/22708 [==============================] - 15s 677us/step - loss: 2.4053 - acc: 0.4092 - val_loss: 2.7020 - val_acc: 0.4059\n",
      "Epoch 41/500\n",
      "22708/22708 [==============================] - 15s 676us/step - loss: 2.3926 - acc: 0.4095 - val_loss: 2.5728 - val_acc: 0.4071\n",
      "Epoch 42/500\n",
      "22708/22708 [==============================] - 15s 678us/step - loss: 2.3906 - acc: 0.4090 - val_loss: 2.6075 - val_acc: 0.3927\n",
      "Epoch 43/500\n",
      "22708/22708 [==============================] - 16s 684us/step - loss: 2.4132 - acc: 0.4050 - val_loss: 2.6296 - val_acc: 0.4068\n",
      "Epoch 44/500\n",
      "22708/22708 [==============================] - 15s 679us/step - loss: 2.3832 - acc: 0.4093 - val_loss: 2.6548 - val_acc: 0.4046\n",
      "Epoch 45/500\n",
      "22708/22708 [==============================] - 15s 677us/step - loss: 2.3768 - acc: 0.4092 - val_loss: 2.5969 - val_acc: 0.4071\n",
      "Epoch 46/500\n",
      "22708/22708 [==============================] - 15s 681us/step - loss: 2.3723 - acc: 0.4110 - val_loss: 2.5869 - val_acc: 0.4078\n",
      "Epoch 47/500\n",
      "22708/22708 [==============================] - 15s 682us/step - loss: 2.3606 - acc: 0.4120 - val_loss: 2.6382 - val_acc: 0.4067\n",
      "Epoch 48/500\n",
      "22708/22708 [==============================] - 15s 681us/step - loss: 2.3603 - acc: 0.4103 - val_loss: 2.6331 - val_acc: 0.4055\n",
      "Epoch 49/500\n",
      "22708/22708 [==============================] - 15s 677us/step - loss: 2.3563 - acc: 0.4136 - val_loss: 2.6081 - val_acc: 0.4073\n",
      "Epoch 50/500\n",
      "22708/22708 [==============================] - 15s 678us/step - loss: 2.3483 - acc: 0.4127 - val_loss: 2.5830 - val_acc: 0.4101\n",
      "Epoch 51/500\n",
      "22708/22708 [==============================] - 15s 682us/step - loss: 2.3654 - acc: 0.4137 - val_loss: 2.5799 - val_acc: 0.4069\n",
      "Epoch 52/500\n",
      "22708/22708 [==============================] - 15s 681us/step - loss: 2.3407 - acc: 0.4139 - val_loss: 2.5529 - val_acc: 0.4110\n",
      "Epoch 53/500\n",
      "22708/22708 [==============================] - 15s 680us/step - loss: 2.3377 - acc: 0.4131 - val_loss: 2.6012 - val_acc: 0.4089\n",
      "Epoch 54/500\n",
      "22708/22708 [==============================] - 15s 678us/step - loss: 2.3345 - acc: 0.4119 - val_loss: 2.5798 - val_acc: 0.4122\n",
      "Epoch 55/500\n",
      "22708/22708 [==============================] - 16s 688us/step - loss: 2.3192 - acc: 0.4168 - val_loss: 2.5132 - val_acc: 0.4129\n",
      "Epoch 56/500\n",
      "22708/22708 [==============================] - 16s 684us/step - loss: 2.3287 - acc: 0.4165 - val_loss: 2.5419 - val_acc: 0.4078\n",
      "Epoch 57/500\n",
      "22708/22708 [==============================] - 16s 685us/step - loss: 2.3668 - acc: 0.4128 - val_loss: 2.5590 - val_acc: 0.4068\n",
      "Epoch 58/500\n",
      "22708/22708 [==============================] - 15s 678us/step - loss: 2.3194 - acc: 0.4154 - val_loss: 2.6282 - val_acc: 0.4103\n",
      "Epoch 59/500\n",
      "22708/22708 [==============================] - 15s 680us/step - loss: 2.3131 - acc: 0.4162 - val_loss: 2.6388 - val_acc: 0.4009\n",
      "Epoch 60/500\n",
      "22708/22708 [==============================] - 15s 678us/step - loss: 2.3048 - acc: 0.4182 - val_loss: 2.6770 - val_acc: 0.4071\n",
      "Epoch 61/500\n",
      "22708/22708 [==============================] - 15s 679us/step - loss: 2.2983 - acc: 0.4197 - val_loss: 2.6451 - val_acc: 0.4098\n",
      "Epoch 62/500\n",
      "22708/22708 [==============================] - 16s 683us/step - loss: 2.2955 - acc: 0.4196 - val_loss: 2.7552 - val_acc: 0.4076\n",
      "Epoch 63/500\n",
      "22708/22708 [==============================] - 16s 687us/step - loss: 2.3318 - acc: 0.4164 - val_loss: 2.6351 - val_acc: 0.4109\n",
      "Epoch 64/500\n",
      "22708/22708 [==============================] - 15s 682us/step - loss: 2.2873 - acc: 0.4198 - val_loss: 2.6020 - val_acc: 0.4093\n",
      "Epoch 65/500\n",
      "22708/22708 [==============================] - 15s 679us/step - loss: 2.2821 - acc: 0.4199 - val_loss: 2.6197 - val_acc: 0.4088\n",
      "Epoch 66/500\n",
      "22708/22708 [==============================] - 15s 680us/step - loss: 2.2825 - acc: 0.4205 - val_loss: 2.6247 - val_acc: 0.4104\n",
      "Epoch 67/500\n",
      "22708/22708 [==============================] - 15s 680us/step - loss: 2.3931 - acc: 0.4106 - val_loss: 2.5777 - val_acc: 0.4092\n",
      "Epoch 68/500\n",
      "22708/22708 [==============================] - 15s 679us/step - loss: 2.3015 - acc: 0.4169 - val_loss: 2.5990 - val_acc: 0.4086\n",
      "Epoch 69/500\n",
      "22708/22708 [==============================] - 15s 678us/step - loss: 2.2827 - acc: 0.4201 - val_loss: 2.5737 - val_acc: 0.4071\n",
      "Epoch 70/500\n",
      "22708/22708 [==============================] - 15s 675us/step - loss: 2.2745 - acc: 0.4215 - val_loss: 2.6326 - val_acc: 0.3916\n",
      "Epoch 71/500\n",
      "22708/22708 [==============================] - 15s 677us/step - loss: 2.2680 - acc: 0.4218 - val_loss: 2.5843 - val_acc: 0.4095\n",
      "Epoch 72/500\n",
      "22708/22708 [==============================] - 15s 678us/step - loss: 2.2617 - acc: 0.4210 - val_loss: 2.5956 - val_acc: 0.4046\n",
      "Epoch 73/500\n",
      "22708/22708 [==============================] - 15s 678us/step - loss: 2.2699 - acc: 0.4215 - val_loss: 2.6820 - val_acc: 0.4053\n",
      "Epoch 74/500\n",
      "22708/22708 [==============================] - 15s 679us/step - loss: 2.2703 - acc: 0.4210 - val_loss: 2.7613 - val_acc: 0.4066\n",
      "Epoch 75/500\n",
      "22708/22708 [==============================] - 15s 682us/step - loss: 2.2552 - acc: 0.4220 - val_loss: 3.1258 - val_acc: 0.3996\n",
      "Epoch 76/500\n",
      "22708/22708 [==============================] - 15s 673us/step - loss: 2.7860 - acc: 0.3963 - val_loss: 2.7917 - val_acc: 0.4012\n",
      "Epoch 77/500\n",
      "22708/22708 [==============================] - 15s 678us/step - loss: 2.6131 - acc: 0.4039 - val_loss: 2.9034 - val_acc: 0.3571\n",
      "Epoch 78/500\n",
      "22708/22708 [==============================] - 16s 683us/step - loss: 2.5597 - acc: 0.4029 - val_loss: 2.8662 - val_acc: 0.3937\n",
      "Epoch 79/500\n",
      "22708/22708 [==============================] - 15s 680us/step - loss: 2.5159 - acc: 0.4041 - val_loss: 2.8875 - val_acc: 0.3984\n",
      "Epoch 80/500\n",
      "22708/22708 [==============================] - 15s 675us/step - loss: 2.4887 - acc: 0.4047 - val_loss: 2.9499 - val_acc: 0.3454\n",
      "Epoch 81/500\n",
      "22708/22708 [==============================] - 15s 677us/step - loss: 2.4574 - acc: 0.4059 - val_loss: 2.8504 - val_acc: 0.3768\n",
      "Epoch 82/500\n",
      "22708/22708 [==============================] - 15s 675us/step - loss: 2.4224 - acc: 0.4077 - val_loss: 2.8257 - val_acc: 0.3902\n",
      "Epoch 83/500\n",
      "22708/22708 [==============================] - 15s 680us/step - loss: 2.3994 - acc: 0.4074 - val_loss: 2.7617 - val_acc: 0.4099\n",
      "Epoch 84/500\n",
      "22708/22708 [==============================] - 15s 678us/step - loss: 2.3770 - acc: 0.4080 - val_loss: 2.7614 - val_acc: 0.3934\n",
      "Epoch 85/500\n",
      "22708/22708 [==============================] - 15s 676us/step - loss: 2.3784 - acc: 0.4088 - val_loss: 2.7152 - val_acc: 0.4078\n",
      "Epoch 86/500\n",
      "22708/22708 [==============================] - 15s 674us/step - loss: 2.3499 - acc: 0.4099 - val_loss: 2.5449 - val_acc: 0.4115\n",
      "Epoch 87/500\n",
      "22708/22708 [==============================] - 15s 676us/step - loss: 2.3569 - acc: 0.4109 - val_loss: 2.6621 - val_acc: 0.4068\n",
      "Epoch 88/500\n",
      "22708/22708 [==============================] - 15s 677us/step - loss: 2.3079 - acc: 0.4179 - val_loss: 2.5559 - val_acc: 0.4096\n",
      "Epoch 89/500\n",
      "22708/22708 [==============================] - 15s 674us/step - loss: 2.2804 - acc: 0.4194 - val_loss: 2.6468 - val_acc: 0.4065\n",
      "Epoch 90/500\n",
      "22708/22708 [==============================] - 15s 675us/step - loss: 2.2744 - acc: 0.4216 - val_loss: 2.6537 - val_acc: 0.4063\n",
      "Epoch 91/500\n",
      "22708/22708 [==============================] - 15s 675us/step - loss: 2.2584 - acc: 0.4229 - val_loss: 2.6895 - val_acc: 0.3999\n",
      "Epoch 92/500\n",
      "22708/22708 [==============================] - 15s 674us/step - loss: 2.2517 - acc: 0.4247 - val_loss: 2.5795 - val_acc: 0.4082\n",
      "Epoch 93/500\n",
      "22708/22708 [==============================] - 15s 677us/step - loss: 2.2385 - acc: 0.4257 - val_loss: 2.6005 - val_acc: 0.4019\n",
      "Epoch 94/500\n",
      "22708/22708 [==============================] - 15s 675us/step - loss: 2.2347 - acc: 0.4262 - val_loss: 2.6124 - val_acc: 0.3981\n",
      "Epoch 95/500\n",
      "22708/22708 [==============================] - 15s 677us/step - loss: 2.2244 - acc: 0.4246 - val_loss: 2.6170 - val_acc: 0.4031\n",
      "Epoch 96/500\n",
      "22708/22708 [==============================] - 15s 675us/step - loss: 2.2285 - acc: 0.4248 - val_loss: 2.4356 - val_acc: 0.4109\n",
      "Epoch 97/500\n",
      "22708/22708 [==============================] - 15s 675us/step - loss: 2.2524 - acc: 0.4241 - val_loss: 2.5885 - val_acc: 0.4010\n",
      "Epoch 98/500\n",
      "22708/22708 [==============================] - 15s 673us/step - loss: 2.2148 - acc: 0.4261 - val_loss: 2.6559 - val_acc: 0.4029\n",
      "Epoch 99/500\n",
      "22708/22708 [==============================] - 15s 676us/step - loss: 2.2016 - acc: 0.4300 - val_loss: 2.5582 - val_acc: 0.4041\n",
      "Epoch 100/500\n",
      "22708/22708 [==============================] - 15s 677us/step - loss: 2.2111 - acc: 0.4279 - val_loss: 2.6696 - val_acc: 0.4021\n",
      "Epoch 101/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 2.2037 - acc: 0.4252 - val_loss: 2.5827 - val_acc: 0.4078\n",
      "Epoch 102/500\n",
      "22708/22708 [==============================] - 15s 676us/step - loss: 2.1792 - acc: 0.4320 - val_loss: 2.7490 - val_acc: 0.4088\n",
      "Epoch 103/500\n",
      "22708/22708 [==============================] - 15s 677us/step - loss: 2.1950 - acc: 0.4290 - val_loss: 2.5139 - val_acc: 0.4053\n",
      "Epoch 104/500\n",
      "22708/22708 [==============================] - 16s 683us/step - loss: 2.1773 - acc: 0.4323 - val_loss: 2.5600 - val_acc: 0.3976\n",
      "Epoch 105/500\n",
      "22708/22708 [==============================] - 15s 674us/step - loss: 2.1739 - acc: 0.4323 - val_loss: 2.5066 - val_acc: 0.4055\n",
      "Epoch 106/500\n",
      "22708/22708 [==============================] - 15s 676us/step - loss: 2.1556 - acc: 0.4354 - val_loss: 2.5194 - val_acc: 0.4017\n",
      "Epoch 107/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 2.1615 - acc: 0.4339 - val_loss: 2.5681 - val_acc: 0.3990\n",
      "Epoch 108/500\n",
      "22708/22708 [==============================] - 15s 676us/step - loss: 2.1532 - acc: 0.4335 - val_loss: 2.5308 - val_acc: 0.4083\n",
      "Epoch 109/500\n",
      "22708/22708 [==============================] - 15s 677us/step - loss: 2.1633 - acc: 0.4320 - val_loss: 2.6750 - val_acc: 0.3945\n",
      "Epoch 110/500\n",
      "22708/22708 [==============================] - 15s 677us/step - loss: 2.2036 - acc: 0.4309 - val_loss: 2.6029 - val_acc: 0.3825\n",
      "Epoch 111/500\n",
      "22708/22708 [==============================] - 15s 674us/step - loss: 2.1670 - acc: 0.4316 - val_loss: 2.6377 - val_acc: 0.3923\n",
      "Epoch 112/500\n",
      "22708/22708 [==============================] - 15s 676us/step - loss: 2.1367 - acc: 0.4372 - val_loss: 2.7781 - val_acc: 0.4056\n",
      "Epoch 113/500\n",
      "22708/22708 [==============================] - 15s 675us/step - loss: 2.2235 - acc: 0.4273 - val_loss: 2.5552 - val_acc: 0.4001\n",
      "Epoch 114/500\n",
      "22708/22708 [==============================] - 15s 678us/step - loss: 2.1405 - acc: 0.4360 - val_loss: 2.4547 - val_acc: 0.4039\n",
      "Epoch 115/500\n",
      "22708/22708 [==============================] - 15s 676us/step - loss: 2.1455 - acc: 0.4368 - val_loss: 2.4716 - val_acc: 0.4083\n",
      "Epoch 116/500\n",
      "22708/22708 [==============================] - 15s 676us/step - loss: 2.1204 - acc: 0.4383 - val_loss: 2.6178 - val_acc: 0.3927\n",
      "Epoch 117/500\n",
      "22708/22708 [==============================] - 15s 674us/step - loss: 2.1125 - acc: 0.4383 - val_loss: 2.6170 - val_acc: 0.3921\n",
      "Epoch 118/500\n",
      "22708/22708 [==============================] - 15s 672us/step - loss: 2.1172 - acc: 0.4390 - val_loss: 2.4433 - val_acc: 0.4143\n",
      "Epoch 119/500\n",
      "22708/22708 [==============================] - 15s 677us/step - loss: 2.0999 - acc: 0.4429 - val_loss: 2.6819 - val_acc: 0.3866\n",
      "Epoch 120/500\n",
      "22708/22708 [==============================] - 15s 680us/step - loss: 2.1100 - acc: 0.4405 - val_loss: 2.4952 - val_acc: 0.4086\n",
      "Epoch 121/500\n",
      "22708/22708 [==============================] - 15s 675us/step - loss: 2.1286 - acc: 0.4390 - val_loss: 2.6333 - val_acc: 0.3935\n",
      "Epoch 122/500\n",
      "22708/22708 [==============================] - 15s 675us/step - loss: 2.0877 - acc: 0.4451 - val_loss: 2.7226 - val_acc: 0.3906\n",
      "Epoch 123/500\n",
      "22708/22708 [==============================] - 15s 673us/step - loss: 2.1047 - acc: 0.4427 - val_loss: 2.6341 - val_acc: 0.4038\n",
      "Epoch 124/500\n",
      "22708/22708 [==============================] - 15s 680us/step - loss: 2.0756 - acc: 0.4490 - val_loss: 3.1179 - val_acc: 0.4070\n",
      "Epoch 125/500\n",
      "22708/22708 [==============================] - 15s 675us/step - loss: 2.7116 - acc: 0.3963 - val_loss: 2.8494 - val_acc: 0.4080\n",
      "Epoch 126/500\n",
      "22708/22708 [==============================] - 15s 674us/step - loss: 2.5689 - acc: 0.4036 - val_loss: 2.8961 - val_acc: 0.4089\n",
      "Epoch 127/500\n",
      "22708/22708 [==============================] - 15s 675us/step - loss: 2.5420 - acc: 0.4062 - val_loss: 2.7670 - val_acc: 0.4095\n",
      "Epoch 128/500\n",
      "22708/22708 [==============================] - 15s 673us/step - loss: 2.5306 - acc: 0.4054 - val_loss: 2.6458 - val_acc: 0.4097\n",
      "Epoch 129/500\n",
      "22708/22708 [==============================] - 15s 673us/step - loss: 2.5104 - acc: 0.4066 - val_loss: 2.8889 - val_acc: 0.3893\n",
      "Epoch 130/500\n",
      "22708/22708 [==============================] - 15s 676us/step - loss: 2.5001 - acc: 0.4057 - val_loss: 2.9096 - val_acc: 0.4013\n",
      "Epoch 131/500\n",
      "22708/22708 [==============================] - 15s 672us/step - loss: 2.4720 - acc: 0.4080 - val_loss: 2.6327 - val_acc: 0.4060\n",
      "Epoch 132/500\n",
      "22708/22708 [==============================] - 15s 673us/step - loss: 2.4670 - acc: 0.4067 - val_loss: 2.8863 - val_acc: 0.4023\n",
      "Epoch 133/500\n",
      "22708/22708 [==============================] - 15s 673us/step - loss: 2.4451 - acc: 0.4089 - val_loss: 2.7859 - val_acc: 0.3945\n",
      "Epoch 134/500\n",
      "22708/22708 [==============================] - 15s 673us/step - loss: 2.4401 - acc: 0.4073 - val_loss: 2.8058 - val_acc: 0.3910\n",
      "Epoch 135/500\n",
      "22708/22708 [==============================] - 15s 678us/step - loss: 2.4188 - acc: 0.4088 - val_loss: 2.9286 - val_acc: 0.3979\n",
      "Epoch 136/500\n",
      "22708/22708 [==============================] - 15s 673us/step - loss: 2.4052 - acc: 0.4099 - val_loss: 2.9218 - val_acc: 0.3583\n",
      "Epoch 137/500\n",
      "22708/22708 [==============================] - 15s 674us/step - loss: 2.3977 - acc: 0.4108 - val_loss: 2.8709 - val_acc: 0.4012\n",
      "Epoch 138/500\n",
      "22708/22708 [==============================] - 15s 672us/step - loss: 2.3817 - acc: 0.4105 - val_loss: 2.8937 - val_acc: 0.4021\n",
      "Epoch 139/500\n",
      "22708/22708 [==============================] - 15s 674us/step - loss: 2.3811 - acc: 0.4105 - val_loss: 2.9973 - val_acc: 0.3461\n",
      "Epoch 140/500\n",
      "22708/22708 [==============================] - 15s 676us/step - loss: 2.3785 - acc: 0.4098 - val_loss: 3.0287 - val_acc: 0.3861\n",
      "Epoch 141/500\n",
      "22708/22708 [==============================] - 15s 680us/step - loss: 2.3632 - acc: 0.4107 - val_loss: 2.9593 - val_acc: 0.3984\n",
      "Epoch 142/500\n",
      "22708/22708 [==============================] - 15s 672us/step - loss: 2.3520 - acc: 0.4121 - val_loss: 2.9229 - val_acc: 0.3951\n",
      "Epoch 143/500\n",
      "22708/22708 [==============================] - 15s 674us/step - loss: 2.3578 - acc: 0.4110 - val_loss: 2.8684 - val_acc: 0.3869\n",
      "Epoch 144/500\n",
      "22708/22708 [==============================] - 15s 675us/step - loss: 2.3458 - acc: 0.4115 - val_loss: 2.9492 - val_acc: 0.3891\n",
      "Epoch 145/500\n",
      "22708/22708 [==============================] - 15s 677us/step - loss: 2.3415 - acc: 0.4122 - val_loss: 2.8238 - val_acc: 0.3970\n",
      "Epoch 146/500\n",
      "22708/22708 [==============================] - 15s 674us/step - loss: 2.3300 - acc: 0.4134 - val_loss: 2.8796 - val_acc: 0.3818\n",
      "Epoch 147/500\n",
      "22708/22708 [==============================] - 15s 673us/step - loss: 2.3517 - acc: 0.4117 - val_loss: 2.8821 - val_acc: 0.3934\n",
      "Epoch 148/500\n",
      "22708/22708 [==============================] - 15s 672us/step - loss: 2.3240 - acc: 0.4142 - val_loss: 2.7303 - val_acc: 0.3897\n",
      "Epoch 149/500\n",
      "22708/22708 [==============================] - 15s 675us/step - loss: 2.3243 - acc: 0.4141 - val_loss: 2.8964 - val_acc: 0.4029\n",
      "Epoch 150/500\n",
      "22708/22708 [==============================] - 15s 674us/step - loss: 2.3191 - acc: 0.4136 - val_loss: 2.7853 - val_acc: 0.4033\n",
      "Epoch 151/500\n",
      "22708/22708 [==============================] - 15s 678us/step - loss: 2.3143 - acc: 0.4161 - val_loss: 2.9306 - val_acc: 0.3670\n",
      "Epoch 152/500\n",
      "22708/22708 [==============================] - 15s 672us/step - loss: 2.3148 - acc: 0.4159 - val_loss: 2.9765 - val_acc: 0.4031\n",
      "Epoch 153/500\n",
      "22708/22708 [==============================] - 15s 673us/step - loss: 2.3063 - acc: 0.4158 - val_loss: 2.9301 - val_acc: 0.4042\n",
      "Epoch 154/500\n",
      "22708/22708 [==============================] - 15s 674us/step - loss: 2.2961 - acc: 0.4177 - val_loss: 2.8696 - val_acc: 0.4051\n",
      "Epoch 155/500\n",
      "22708/22708 [==============================] - 15s 678us/step - loss: 2.2876 - acc: 0.4170 - val_loss: 2.8089 - val_acc: 0.3786\n",
      "Epoch 156/500\n",
      "22708/22708 [==============================] - 15s 674us/step - loss: 2.2936 - acc: 0.4183 - val_loss: 2.7963 - val_acc: 0.3904\n",
      "Epoch 157/500\n",
      "22708/22708 [==============================] - 15s 678us/step - loss: 2.2854 - acc: 0.4182 - val_loss: 2.8568 - val_acc: 0.4021\n",
      "Epoch 158/500\n",
      "22708/22708 [==============================] - 15s 672us/step - loss: 2.2755 - acc: 0.4196 - val_loss: 2.7033 - val_acc: 0.4035\n",
      "Epoch 159/500\n",
      "22708/22708 [==============================] - 15s 674us/step - loss: 2.2791 - acc: 0.4193 - val_loss: 2.8568 - val_acc: 0.3833\n",
      "Epoch 160/500\n",
      "22708/22708 [==============================] - 15s 673us/step - loss: 2.2720 - acc: 0.4188 - val_loss: 2.7672 - val_acc: 0.3984\n",
      "Epoch 161/500\n",
      "22708/22708 [==============================] - 15s 677us/step - loss: 2.2602 - acc: 0.4205 - val_loss: 2.7962 - val_acc: 0.4076\n",
      "Epoch 162/500\n",
      "22708/22708 [==============================] - 15s 674us/step - loss: 2.2548 - acc: 0.4207 - val_loss: 2.7104 - val_acc: 0.4057\n",
      "Epoch 163/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 2.2561 - acc: 0.4203 - val_loss: 2.6757 - val_acc: 0.4050\n",
      "Epoch 164/500\n",
      "22708/22708 [==============================] - 15s 679us/step - loss: 2.2387 - acc: 0.4257 - val_loss: 2.7434 - val_acc: 0.4031\n",
      "Epoch 165/500\n",
      "22708/22708 [==============================] - 15s 675us/step - loss: 2.2288 - acc: 0.4242 - val_loss: 2.9567 - val_acc: 0.3935\n",
      "Epoch 166/500\n",
      "22708/22708 [==============================] - 15s 674us/step - loss: 2.2354 - acc: 0.4238 - val_loss: 2.7116 - val_acc: 0.4029\n",
      "Epoch 167/500\n",
      "22708/22708 [==============================] - 15s 674us/step - loss: 2.2053 - acc: 0.4279 - val_loss: 2.6810 - val_acc: 0.3747\n",
      "Epoch 168/500\n",
      "22708/22708 [==============================] - 15s 673us/step - loss: 2.1847 - acc: 0.4326 - val_loss: 2.8388 - val_acc: 0.3746\n",
      "Epoch 169/500\n",
      "22708/22708 [==============================] - 15s 674us/step - loss: 2.1859 - acc: 0.4314 - val_loss: 2.5698 - val_acc: 0.4053\n",
      "Epoch 170/500\n",
      "22708/22708 [==============================] - 15s 673us/step - loss: 2.1519 - acc: 0.4332 - val_loss: 2.6990 - val_acc: 0.3850\n",
      "Epoch 171/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 2.1228 - acc: 0.4380 - val_loss: 2.5378 - val_acc: 0.4066\n",
      "Epoch 172/500\n",
      "22708/22708 [==============================] - 15s 675us/step - loss: 2.1131 - acc: 0.4407 - val_loss: 2.6286 - val_acc: 0.3816\n",
      "Epoch 173/500\n",
      "22708/22708 [==============================] - 15s 670us/step - loss: 2.0905 - acc: 0.4462 - val_loss: 2.6574 - val_acc: 0.3724\n",
      "Epoch 174/500\n",
      "22708/22708 [==============================] - 15s 673us/step - loss: 2.0761 - acc: 0.4459 - val_loss: 2.4284 - val_acc: 0.4120\n",
      "Epoch 175/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 2.0963 - acc: 0.4423 - val_loss: 2.6001 - val_acc: 0.4051\n",
      "Epoch 176/500\n",
      "22708/22708 [==============================] - 15s 674us/step - loss: 2.0642 - acc: 0.4493 - val_loss: 2.6048 - val_acc: 0.3941\n",
      "Epoch 177/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 2.0553 - acc: 0.4489 - val_loss: 2.5243 - val_acc: 0.3919\n",
      "Epoch 178/500\n",
      "22708/22708 [==============================] - 15s 674us/step - loss: 2.0635 - acc: 0.4458 - val_loss: 2.5528 - val_acc: 0.3982\n",
      "Epoch 179/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 2.0521 - acc: 0.4505 - val_loss: 2.4409 - val_acc: 0.4183\n",
      "Epoch 180/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 2.0400 - acc: 0.4532 - val_loss: 2.5756 - val_acc: 0.3972\n",
      "Epoch 181/500\n",
      "22708/22708 [==============================] - 15s 667us/step - loss: 2.0373 - acc: 0.4520 - val_loss: 2.5733 - val_acc: 0.4022\n",
      "Epoch 182/500\n",
      "22708/22708 [==============================] - 15s 672us/step - loss: 2.0334 - acc: 0.4548 - val_loss: 2.6550 - val_acc: 0.3934\n",
      "Epoch 183/500\n",
      "22708/22708 [==============================] - 15s 672us/step - loss: 2.0290 - acc: 0.4554 - val_loss: 2.5162 - val_acc: 0.4029\n",
      "Epoch 184/500\n",
      "22708/22708 [==============================] - 15s 668us/step - loss: 2.0289 - acc: 0.4514 - val_loss: 2.6252 - val_acc: 0.3978\n",
      "Epoch 185/500\n",
      "22708/22708 [==============================] - 15s 674us/step - loss: 2.0279 - acc: 0.4521 - val_loss: 2.4964 - val_acc: 0.4031\n",
      "Epoch 186/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 2.0084 - acc: 0.4554 - val_loss: 2.4184 - val_acc: 0.4065\n",
      "Epoch 187/500\n",
      "22708/22708 [==============================] - 15s 672us/step - loss: 2.0125 - acc: 0.4570 - val_loss: 2.4859 - val_acc: 0.4060\n",
      "Epoch 188/500\n",
      "22708/22708 [==============================] - 15s 674us/step - loss: 1.9953 - acc: 0.4598 - val_loss: 2.5066 - val_acc: 0.4039\n",
      "Epoch 189/500\n",
      "22708/22708 [==============================] - 15s 670us/step - loss: 1.9911 - acc: 0.4624 - val_loss: 2.6307 - val_acc: 0.3899\n",
      "Epoch 190/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 2.0002 - acc: 0.4581 - val_loss: 2.6119 - val_acc: 0.4122\n",
      "Epoch 191/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 2.0519 - acc: 0.4483 - val_loss: 2.5775 - val_acc: 0.4018\n",
      "Epoch 192/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 2.0074 - acc: 0.4596 - val_loss: 2.4731 - val_acc: 0.4221\n",
      "Epoch 193/500\n",
      "22708/22708 [==============================] - 15s 670us/step - loss: 1.9820 - acc: 0.4594 - val_loss: 2.4165 - val_acc: 0.4160\n",
      "Epoch 194/500\n",
      "22708/22708 [==============================] - 15s 673us/step - loss: 1.9853 - acc: 0.4627 - val_loss: 2.5931 - val_acc: 0.3814\n",
      "Epoch 195/500\n",
      "22708/22708 [==============================] - 15s 672us/step - loss: 1.9660 - acc: 0.4642 - val_loss: 2.4727 - val_acc: 0.4133\n",
      "Epoch 196/500\n",
      "22708/22708 [==============================] - 15s 673us/step - loss: 1.9711 - acc: 0.4660 - val_loss: 2.6179 - val_acc: 0.4104\n",
      "Epoch 197/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 2.0047 - acc: 0.4580 - val_loss: 2.3578 - val_acc: 0.4234\n",
      "Epoch 198/500\n",
      "22708/22708 [==============================] - 15s 676us/step - loss: 1.9568 - acc: 0.4668 - val_loss: 2.5241 - val_acc: 0.4151\n",
      "Epoch 199/500\n",
      "22708/22708 [==============================] - 15s 673us/step - loss: 1.9605 - acc: 0.4649 - val_loss: 2.4646 - val_acc: 0.4123\n",
      "Epoch 200/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 1.9576 - acc: 0.4653 - val_loss: 2.5274 - val_acc: 0.3958\n",
      "Epoch 201/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 1.9569 - acc: 0.4663 - val_loss: 2.4489 - val_acc: 0.4157\n",
      "Epoch 202/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 1.9477 - acc: 0.4700 - val_loss: 2.4786 - val_acc: 0.3993\n",
      "Epoch 203/500\n",
      "22708/22708 [==============================] - 15s 672us/step - loss: 1.9526 - acc: 0.4666 - val_loss: 2.5128 - val_acc: 0.4171\n",
      "Epoch 204/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 1.9278 - acc: 0.4756 - val_loss: 2.4348 - val_acc: 0.4102\n",
      "Epoch 205/500\n",
      "22708/22708 [==============================] - 15s 678us/step - loss: 1.9458 - acc: 0.4720 - val_loss: 2.4888 - val_acc: 0.4109\n",
      "Epoch 206/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 1.9253 - acc: 0.4723 - val_loss: 2.5555 - val_acc: 0.4078\n",
      "Epoch 207/500\n",
      "22708/22708 [==============================] - 15s 672us/step - loss: 1.9210 - acc: 0.4740 - val_loss: 2.4021 - val_acc: 0.4153\n",
      "Epoch 208/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 1.9155 - acc: 0.4754 - val_loss: 2.4596 - val_acc: 0.4151\n",
      "Epoch 209/500\n",
      "22708/22708 [==============================] - 15s 673us/step - loss: 1.9585 - acc: 0.4702 - val_loss: 2.4155 - val_acc: 0.4171\n",
      "Epoch 210/500\n",
      "22708/22708 [==============================] - 15s 667us/step - loss: 1.9538 - acc: 0.4675 - val_loss: 2.4416 - val_acc: 0.4156\n",
      "Epoch 211/500\n",
      "22708/22708 [==============================] - 15s 672us/step - loss: 1.9017 - acc: 0.4726 - val_loss: 2.4332 - val_acc: 0.4093\n",
      "Epoch 212/500\n",
      "22708/22708 [==============================] - 15s 667us/step - loss: 1.9167 - acc: 0.4743 - val_loss: 2.3796 - val_acc: 0.4181\n",
      "Epoch 213/500\n",
      "22708/22708 [==============================] - 15s 668us/step - loss: 1.9106 - acc: 0.4734 - val_loss: 2.4092 - val_acc: 0.4138\n",
      "Epoch 214/500\n",
      "22708/22708 [==============================] - 15s 672us/step - loss: 1.8944 - acc: 0.4802 - val_loss: 2.4701 - val_acc: 0.4223\n",
      "Epoch 215/500\n",
      "22708/22708 [==============================] - 15s 670us/step - loss: 1.8903 - acc: 0.4787 - val_loss: 2.4550 - val_acc: 0.4180\n",
      "Epoch 216/500\n",
      "22708/22708 [==============================] - 15s 668us/step - loss: 1.9016 - acc: 0.4778 - val_loss: 2.5070 - val_acc: 0.4086\n",
      "Epoch 217/500\n",
      "22708/22708 [==============================] - 15s 670us/step - loss: 1.8855 - acc: 0.4786 - val_loss: 2.4318 - val_acc: 0.4223\n",
      "Epoch 218/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 1.8830 - acc: 0.4804 - val_loss: 2.4020 - val_acc: 0.4193\n",
      "Epoch 219/500\n",
      "22708/22708 [==============================] - 15s 674us/step - loss: 1.8658 - acc: 0.4858 - val_loss: 2.4967 - val_acc: 0.4246\n",
      "Epoch 220/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 1.8676 - acc: 0.4869 - val_loss: 2.5158 - val_acc: 0.4151\n",
      "Epoch 221/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 1.8814 - acc: 0.4824 - val_loss: 2.3617 - val_acc: 0.4259\n",
      "Epoch 222/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 1.8395 - acc: 0.4905 - val_loss: 2.5797 - val_acc: 0.4038\n",
      "Epoch 223/500\n",
      "22708/22708 [==============================] - 15s 670us/step - loss: 1.9212 - acc: 0.4752 - val_loss: 2.4674 - val_acc: 0.4161\n",
      "Epoch 224/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 1.8469 - acc: 0.4873 - val_loss: 2.5560 - val_acc: 0.4209\n",
      "Epoch 225/500\n",
      "22708/22708 [==============================] - 15s 676us/step - loss: 1.8629 - acc: 0.4853 - val_loss: 2.4086 - val_acc: 0.4221\n",
      "Epoch 226/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 1.8422 - acc: 0.4879 - val_loss: 2.4627 - val_acc: 0.4055\n",
      "Epoch 227/500\n",
      "22708/22708 [==============================] - 15s 672us/step - loss: 1.8299 - acc: 0.4920 - val_loss: 2.4193 - val_acc: 0.4185\n",
      "Epoch 228/500\n",
      "22708/22708 [==============================] - 15s 670us/step - loss: 1.8415 - acc: 0.4888 - val_loss: 2.4160 - val_acc: 0.4234\n",
      "Epoch 229/500\n",
      "22708/22708 [==============================] - 15s 672us/step - loss: 1.8200 - acc: 0.4930 - val_loss: 2.4487 - val_acc: 0.4190\n",
      "Epoch 230/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 1.8324 - acc: 0.4922 - val_loss: 2.4276 - val_acc: 0.4203\n",
      "Epoch 231/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 1.8240 - acc: 0.4936 - val_loss: 2.5052 - val_acc: 0.4098\n",
      "Epoch 232/500\n",
      "22708/22708 [==============================] - 15s 673us/step - loss: 2.3667 - acc: 0.4157 - val_loss: 3.0260 - val_acc: 0.4003\n",
      "Epoch 233/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 2.1791 - acc: 0.4298 - val_loss: 2.3943 - val_acc: 0.4115\n",
      "Epoch 234/500\n",
      "22708/22708 [==============================] - 15s 673us/step - loss: 1.9248 - acc: 0.4708 - val_loss: 2.3563 - val_acc: 0.4217\n",
      "Epoch 235/500\n",
      "22708/22708 [==============================] - 15s 675us/step - loss: 2.7707 - acc: 0.4055 - val_loss: 2.8062 - val_acc: 0.3804\n",
      "Epoch 236/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 2.5768 - acc: 0.4054 - val_loss: 2.7561 - val_acc: 0.4007\n",
      "Epoch 237/500\n",
      "22708/22708 [==============================] - 15s 672us/step - loss: 2.5085 - acc: 0.4085 - val_loss: 2.9041 - val_acc: 0.3189\n",
      "Epoch 238/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 2.4673 - acc: 0.4083 - val_loss: 2.9418 - val_acc: 0.4030\n",
      "Epoch 239/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 2.4530 - acc: 0.4112 - val_loss: 2.8657 - val_acc: 0.3851\n",
      "Epoch 240/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 2.4358 - acc: 0.4104 - val_loss: 2.7951 - val_acc: 0.4071\n",
      "Epoch 241/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 2.4285 - acc: 0.4143 - val_loss: 2.9601 - val_acc: 0.3404\n",
      "Epoch 242/500\n",
      "22708/22708 [==============================] - 15s 670us/step - loss: 2.4129 - acc: 0.4133 - val_loss: 2.7607 - val_acc: 0.4043\n",
      "Epoch 243/500\n",
      "22708/22708 [==============================] - 15s 670us/step - loss: 2.4065 - acc: 0.4139 - val_loss: 2.8888 - val_acc: 0.3359\n",
      "Epoch 244/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 2.3865 - acc: 0.4150 - val_loss: 2.8780 - val_acc: 0.3500\n",
      "Epoch 245/500\n",
      "22708/22708 [==============================] - 15s 667us/step - loss: 2.3773 - acc: 0.4158 - val_loss: 2.7990 - val_acc: 0.3924\n",
      "Epoch 246/500\n",
      "22708/22708 [==============================] - 15s 676us/step - loss: 2.3758 - acc: 0.4181 - val_loss: 2.7701 - val_acc: 0.3365\n",
      "Epoch 247/500\n",
      "22708/22708 [==============================] - 15s 670us/step - loss: 2.3636 - acc: 0.4182 - val_loss: 2.7930 - val_acc: 0.3427\n",
      "Epoch 248/500\n",
      "22708/22708 [==============================] - 15s 670us/step - loss: 2.3527 - acc: 0.4185 - val_loss: 2.9536 - val_acc: 0.3192\n",
      "Epoch 249/500\n",
      "22708/22708 [==============================] - 15s 670us/step - loss: 2.3413 - acc: 0.4193 - val_loss: 2.9506 - val_acc: 0.3541\n",
      "Epoch 250/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 2.3284 - acc: 0.4213 - val_loss: 2.9044 - val_acc: 0.3455\n",
      "Epoch 251/500\n",
      "22708/22708 [==============================] - 15s 674us/step - loss: 2.3188 - acc: 0.4209 - val_loss: 2.7705 - val_acc: 0.3341\n",
      "Epoch 252/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 2.3249 - acc: 0.4226 - val_loss: 2.8106 - val_acc: 0.4042\n",
      "Epoch 253/500\n",
      "22708/22708 [==============================] - 15s 668us/step - loss: 2.3034 - acc: 0.4224 - val_loss: 3.0300 - val_acc: 0.3386\n",
      "Epoch 254/500\n",
      "22708/22708 [==============================] - 15s 670us/step - loss: 2.3069 - acc: 0.4217 - val_loss: 2.7448 - val_acc: 0.4016\n",
      "Epoch 255/500\n",
      "22708/22708 [==============================] - 15s 674us/step - loss: 2.2894 - acc: 0.4216 - val_loss: 2.8445 - val_acc: 0.3976\n",
      "Epoch 256/500\n",
      "22708/22708 [==============================] - 15s 673us/step - loss: 2.2760 - acc: 0.4251 - val_loss: 2.7769 - val_acc: 0.3522\n",
      "Epoch 257/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 2.2714 - acc: 0.4250 - val_loss: 2.8604 - val_acc: 0.3565\n",
      "Epoch 258/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 2.2634 - acc: 0.4242 - val_loss: 2.8131 - val_acc: 0.3812\n",
      "Epoch 259/500\n",
      "22708/22708 [==============================] - 15s 667us/step - loss: 2.2632 - acc: 0.4248 - val_loss: 2.9509 - val_acc: 0.3543\n",
      "Epoch 260/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 2.2560 - acc: 0.4243 - val_loss: 2.7664 - val_acc: 0.3723\n",
      "Epoch 261/500\n",
      "22708/22708 [==============================] - 15s 672us/step - loss: 2.2439 - acc: 0.4289 - val_loss: 2.8867 - val_acc: 0.4057\n",
      "Epoch 262/500\n",
      "22708/22708 [==============================] - 15s 670us/step - loss: 2.2536 - acc: 0.4258 - val_loss: 2.8485 - val_acc: 0.4007\n",
      "Epoch 263/500\n",
      "22708/22708 [==============================] - 15s 668us/step - loss: 2.2369 - acc: 0.4278 - val_loss: 2.7147 - val_acc: 0.3909\n",
      "Epoch 264/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 2.2295 - acc: 0.4294 - val_loss: 2.9505 - val_acc: 0.3852\n",
      "Epoch 265/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 2.2362 - acc: 0.4297 - val_loss: 2.8438 - val_acc: 0.3497\n",
      "Epoch 266/500\n",
      "22708/22708 [==============================] - 15s 676us/step - loss: 2.2267 - acc: 0.4288 - val_loss: 2.8384 - val_acc: 0.4052\n",
      "Epoch 267/500\n",
      "22708/22708 [==============================] - 15s 673us/step - loss: 2.2154 - acc: 0.4294 - val_loss: 2.7547 - val_acc: 0.3874\n",
      "Epoch 268/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 2.2090 - acc: 0.4311 - val_loss: 2.9408 - val_acc: 0.3647\n",
      "Epoch 269/500\n",
      "22708/22708 [==============================] - 15s 672us/step - loss: 2.2094 - acc: 0.4303 - val_loss: 2.8149 - val_acc: 0.3986\n",
      "Epoch 270/500\n",
      "22708/22708 [==============================] - 15s 670us/step - loss: 2.2105 - acc: 0.4304 - val_loss: 2.8673 - val_acc: 0.3276\n",
      "Epoch 271/500\n",
      "22708/22708 [==============================] - 15s 670us/step - loss: 2.2071 - acc: 0.4310 - val_loss: 2.8276 - val_acc: 0.3796\n",
      "Epoch 272/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 2.1986 - acc: 0.4312 - val_loss: 3.0246 - val_acc: 0.3693\n",
      "Epoch 273/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 2.2072 - acc: 0.4314 - val_loss: 2.8540 - val_acc: 0.3431\n",
      "Epoch 274/500\n",
      "22708/22708 [==============================] - 15s 667us/step - loss: 2.1937 - acc: 0.4321 - val_loss: 2.8924 - val_acc: 0.3969\n",
      "Epoch 275/500\n",
      "22708/22708 [==============================] - 15s 674us/step - loss: 2.1875 - acc: 0.4336 - val_loss: 2.7954 - val_acc: 0.3764\n",
      "Epoch 276/500\n",
      "22708/22708 [==============================] - 15s 667us/step - loss: 2.1843 - acc: 0.4327 - val_loss: 2.8352 - val_acc: 0.3468\n",
      "Epoch 277/500\n",
      "22708/22708 [==============================] - 15s 672us/step - loss: 2.1924 - acc: 0.4336 - val_loss: 2.8422 - val_acc: 0.3866\n",
      "Epoch 278/500\n",
      "22708/22708 [==============================] - 15s 670us/step - loss: 2.1718 - acc: 0.4346 - val_loss: 2.8842 - val_acc: 0.3626\n",
      "Epoch 279/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 2.1740 - acc: 0.4338 - val_loss: 2.8315 - val_acc: 0.3504\n",
      "Epoch 280/500\n",
      "22708/22708 [==============================] - 15s 667us/step - loss: 2.1743 - acc: 0.4337 - val_loss: 2.8480 - val_acc: 0.3749\n",
      "Epoch 281/500\n",
      "22708/22708 [==============================] - 15s 667us/step - loss: 2.1642 - acc: 0.4338 - val_loss: 2.8646 - val_acc: 0.3639\n",
      "Epoch 282/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 2.1625 - acc: 0.4357 - val_loss: 2.8621 - val_acc: 0.3927\n",
      "Epoch 283/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 2.1492 - acc: 0.4386 - val_loss: 2.9065 - val_acc: 0.3771\n",
      "Epoch 284/500\n",
      "22708/22708 [==============================] - 15s 668us/step - loss: 2.1590 - acc: 0.4373 - val_loss: 2.8932 - val_acc: 0.4021\n",
      "Epoch 285/500\n",
      "22708/22708 [==============================] - 15s 668us/step - loss: 2.1454 - acc: 0.4394 - val_loss: 2.8800 - val_acc: 0.3929\n",
      "Epoch 286/500\n",
      "22708/22708 [==============================] - 15s 668us/step - loss: 2.1447 - acc: 0.4370 - val_loss: 2.8778 - val_acc: 0.3669\n",
      "Epoch 287/500\n",
      "22708/22708 [==============================] - 15s 672us/step - loss: 2.1335 - acc: 0.4405 - val_loss: 2.7870 - val_acc: 0.3940\n",
      "Epoch 288/500\n",
      "22708/22708 [==============================] - 15s 668us/step - loss: 2.1279 - acc: 0.4408 - val_loss: 2.8552 - val_acc: 0.3174\n",
      "Epoch 289/500\n",
      "22708/22708 [==============================] - 15s 666us/step - loss: 2.1468 - acc: 0.4361 - val_loss: 2.8849 - val_acc: 0.4034\n",
      "Epoch 290/500\n",
      "22708/22708 [==============================] - 15s 668us/step - loss: 2.1248 - acc: 0.4420 - val_loss: 2.7901 - val_acc: 0.4025\n",
      "Epoch 291/500\n",
      "22708/22708 [==============================] - 15s 666us/step - loss: 2.1216 - acc: 0.4410 - val_loss: 2.8134 - val_acc: 0.4011\n",
      "Epoch 292/500\n",
      "22708/22708 [==============================] - 15s 667us/step - loss: 2.1043 - acc: 0.4420 - val_loss: 2.8612 - val_acc: 0.3778\n",
      "Epoch 293/500\n",
      "22708/22708 [==============================] - 15s 668us/step - loss: 2.1027 - acc: 0.4431 - val_loss: 2.8871 - val_acc: 0.3845\n",
      "Epoch 294/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 2.0903 - acc: 0.4462 - val_loss: 2.8114 - val_acc: 0.3959\n",
      "Epoch 295/500\n",
      "22708/22708 [==============================] - 15s 668us/step - loss: 2.0920 - acc: 0.4455 - val_loss: 2.7886 - val_acc: 0.3713\n",
      "Epoch 296/500\n",
      "22708/22708 [==============================] - 15s 666us/step - loss: 2.0683 - acc: 0.4482 - val_loss: 2.7238 - val_acc: 0.3802\n",
      "Epoch 297/500\n",
      "22708/22708 [==============================] - 15s 666us/step - loss: 2.0691 - acc: 0.4484 - val_loss: 2.6875 - val_acc: 0.3652\n",
      "Epoch 298/500\n",
      "22708/22708 [==============================] - 15s 667us/step - loss: 2.0404 - acc: 0.4504 - val_loss: 2.6833 - val_acc: 0.3867\n",
      "Epoch 299/500\n",
      "22708/22708 [==============================] - 15s 667us/step - loss: 2.0154 - acc: 0.4520 - val_loss: 2.7491 - val_acc: 0.3901\n",
      "Epoch 300/500\n",
      "22708/22708 [==============================] - 15s 665us/step - loss: 1.9829 - acc: 0.4575 - val_loss: 2.7082 - val_acc: 0.3891\n",
      "Epoch 301/500\n",
      "22708/22708 [==============================] - 15s 665us/step - loss: 1.9660 - acc: 0.4638 - val_loss: 2.5319 - val_acc: 0.3771\n",
      "Epoch 302/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 1.9285 - acc: 0.4694 - val_loss: 2.4901 - val_acc: 0.4010\n",
      "Epoch 303/500\n",
      "22708/22708 [==============================] - 15s 664us/step - loss: 1.8941 - acc: 0.4740 - val_loss: 2.5483 - val_acc: 0.3992\n",
      "Epoch 304/500\n",
      "22708/22708 [==============================] - 15s 668us/step - loss: 1.8725 - acc: 0.4782 - val_loss: 2.4242 - val_acc: 0.4146\n",
      "Epoch 305/500\n",
      "22708/22708 [==============================] - 15s 663us/step - loss: 1.8561 - acc: 0.4861 - val_loss: 2.4583 - val_acc: 0.4127\n",
      "Epoch 306/500\n",
      "22708/22708 [==============================] - 15s 667us/step - loss: 1.8551 - acc: 0.4867 - val_loss: 2.4676 - val_acc: 0.4203\n",
      "Epoch 307/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 1.8349 - acc: 0.4899 - val_loss: 2.4236 - val_acc: 0.4254\n",
      "Epoch 308/500\n",
      "22708/22708 [==============================] - 15s 666us/step - loss: 1.8188 - acc: 0.4906 - val_loss: 2.4764 - val_acc: 0.4212\n",
      "Epoch 309/500\n",
      "22708/22708 [==============================] - 15s 668us/step - loss: 1.8181 - acc: 0.4935 - val_loss: 2.3462 - val_acc: 0.4306\n",
      "Epoch 310/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 1.7982 - acc: 0.5001 - val_loss: 2.4146 - val_acc: 0.4116\n",
      "Epoch 311/500\n",
      "22708/22708 [==============================] - 15s 664us/step - loss: 1.8233 - acc: 0.4912 - val_loss: 2.4678 - val_acc: 0.4071\n",
      "Epoch 312/500\n",
      "22708/22708 [==============================] - 15s 665us/step - loss: 1.7928 - acc: 0.4996 - val_loss: 2.5089 - val_acc: 0.4155\n",
      "Epoch 313/500\n",
      "22708/22708 [==============================] - 15s 665us/step - loss: 1.7984 - acc: 0.4983 - val_loss: 2.3171 - val_acc: 0.4311\n",
      "Epoch 314/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 2.4490 - acc: 0.4174 - val_loss: 2.9491 - val_acc: 0.3891\n",
      "Epoch 315/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 2.2773 - acc: 0.4273 - val_loss: 3.0102 - val_acc: 0.3725\n",
      "Epoch 316/500\n",
      "22708/22708 [==============================] - 15s 667us/step - loss: 2.2183 - acc: 0.4319 - val_loss: 2.9074 - val_acc: 0.3999\n",
      "Epoch 317/500\n",
      "22708/22708 [==============================] - 15s 673us/step - loss: 2.1845 - acc: 0.4364 - val_loss: 2.8885 - val_acc: 0.3715\n",
      "Epoch 318/500\n",
      "22708/22708 [==============================] - 15s 667us/step - loss: 2.1690 - acc: 0.4357 - val_loss: 2.8865 - val_acc: 0.3507\n",
      "Epoch 319/500\n",
      "22708/22708 [==============================] - 15s 668us/step - loss: 2.1172 - acc: 0.4393 - val_loss: 2.9029 - val_acc: 0.3919\n",
      "Epoch 320/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 2.0928 - acc: 0.4439 - val_loss: 2.7870 - val_acc: 0.3918\n",
      "Epoch 321/500\n",
      "22708/22708 [==============================] - 15s 664us/step - loss: 2.0472 - acc: 0.4512 - val_loss: 2.7198 - val_acc: 0.3893\n",
      "Epoch 322/500\n",
      "22708/22708 [==============================] - 15s 663us/step - loss: 1.9812 - acc: 0.4646 - val_loss: 2.4658 - val_acc: 0.4033\n",
      "Epoch 323/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 1.8601 - acc: 0.4818 - val_loss: 2.3796 - val_acc: 0.4295\n",
      "Epoch 324/500\n",
      "22708/22708 [==============================] - 15s 672us/step - loss: 1.8397 - acc: 0.4870 - val_loss: 2.3355 - val_acc: 0.4300\n",
      "Epoch 325/500\n",
      "22708/22708 [==============================] - 16s 683us/step - loss: 1.8078 - acc: 0.4944 - val_loss: 2.3831 - val_acc: 0.4216\n",
      "Epoch 326/500\n",
      "22708/22708 [==============================] - 16s 684us/step - loss: 1.8319 - acc: 0.4935 - val_loss: 2.5076 - val_acc: 0.4152\n",
      "Epoch 327/500\n",
      "22708/22708 [==============================] - 16s 686us/step - loss: 1.7898 - acc: 0.5000 - val_loss: 2.3895 - val_acc: 0.4209\n",
      "Epoch 328/500\n",
      "22708/22708 [==============================] - 15s 676us/step - loss: 1.7815 - acc: 0.5012 - val_loss: 2.4287 - val_acc: 0.4187\n",
      "Epoch 329/500\n",
      "22708/22708 [==============================] - 15s 670us/step - loss: 1.7714 - acc: 0.5033 - val_loss: 2.4308 - val_acc: 0.4225\n",
      "Epoch 330/500\n",
      "22708/22708 [==============================] - 15s 673us/step - loss: 1.9106 - acc: 0.4756 - val_loss: 2.3421 - val_acc: 0.4266\n",
      "Epoch 331/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 1.7550 - acc: 0.5058 - val_loss: 2.4045 - val_acc: 0.4310\n",
      "Epoch 332/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 1.7554 - acc: 0.5047 - val_loss: 2.3776 - val_acc: 0.4310\n",
      "Epoch 333/500\n",
      "22708/22708 [==============================] - 15s 668us/step - loss: 1.7493 - acc: 0.5096 - val_loss: 2.3471 - val_acc: 0.4219\n",
      "Epoch 334/500\n",
      "22708/22708 [==============================] - 15s 670us/step - loss: 1.7496 - acc: 0.5112 - val_loss: 2.3678 - val_acc: 0.4331\n",
      "Epoch 335/500\n",
      "22708/22708 [==============================] - 15s 675us/step - loss: 1.7265 - acc: 0.5140 - val_loss: 2.3919 - val_acc: 0.4270\n",
      "Epoch 336/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 1.7501 - acc: 0.5078 - val_loss: 2.5035 - val_acc: 0.4214\n",
      "Epoch 337/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 1.7682 - acc: 0.5026 - val_loss: 2.4075 - val_acc: 0.4111\n",
      "Epoch 338/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 1.7340 - acc: 0.5093 - val_loss: 2.4199 - val_acc: 0.4186\n",
      "Epoch 339/500\n",
      "22708/22708 [==============================] - 15s 668us/step - loss: 1.7067 - acc: 0.5166 - val_loss: 2.3955 - val_acc: 0.4281\n",
      "Epoch 340/500\n",
      "22708/22708 [==============================] - 15s 670us/step - loss: 1.7292 - acc: 0.5131 - val_loss: 2.3983 - val_acc: 0.4241\n",
      "Epoch 341/500\n",
      "22708/22708 [==============================] - 15s 670us/step - loss: 1.7200 - acc: 0.5118 - val_loss: 2.4065 - val_acc: 0.4227\n",
      "Epoch 342/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 1.7063 - acc: 0.5159 - val_loss: 2.3895 - val_acc: 0.4223\n",
      "Epoch 343/500\n",
      "22708/22708 [==============================] - 15s 673us/step - loss: 1.7140 - acc: 0.5145 - val_loss: 2.4965 - val_acc: 0.3882\n",
      "Epoch 344/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 1.7130 - acc: 0.5119 - val_loss: 2.4377 - val_acc: 0.4301\n",
      "Epoch 345/500\n",
      "22708/22708 [==============================] - 15s 672us/step - loss: 1.6769 - acc: 0.5235 - val_loss: 2.3674 - val_acc: 0.4255\n",
      "Epoch 346/500\n",
      "22708/22708 [==============================] - 15s 673us/step - loss: 1.7133 - acc: 0.5140 - val_loss: 2.4561 - val_acc: 0.4311\n",
      "Epoch 347/500\n",
      "22708/22708 [==============================] - 15s 673us/step - loss: 1.6846 - acc: 0.5223 - val_loss: 2.4756 - val_acc: 0.4169\n",
      "Epoch 348/500\n",
      "22708/22708 [==============================] - 15s 676us/step - loss: 1.6929 - acc: 0.5214 - val_loss: 2.4400 - val_acc: 0.4322\n",
      "Epoch 349/500\n",
      "22708/22708 [==============================] - 15s 672us/step - loss: 1.6743 - acc: 0.5248 - val_loss: 2.5085 - val_acc: 0.4201\n",
      "Epoch 350/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 1.6824 - acc: 0.5225 - val_loss: 2.4381 - val_acc: 0.4386\n",
      "Epoch 351/500\n",
      "22708/22708 [==============================] - 15s 676us/step - loss: 1.6822 - acc: 0.5222 - val_loss: 2.4152 - val_acc: 0.4316\n",
      "Epoch 352/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 1.7494 - acc: 0.5112 - val_loss: 2.8382 - val_acc: 0.3819\n",
      "Epoch 353/500\n",
      "22708/22708 [==============================] - 15s 673us/step - loss: 2.1070 - acc: 0.4436 - val_loss: 2.6557 - val_acc: 0.4063\n",
      "Epoch 354/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 1.7776 - acc: 0.5024 - val_loss: 2.4740 - val_acc: 0.4135\n",
      "Epoch 355/500\n",
      "22708/22708 [==============================] - 15s 678us/step - loss: 1.6873 - acc: 0.5223 - val_loss: 2.4022 - val_acc: 0.4245\n",
      "Epoch 356/500\n",
      "22708/22708 [==============================] - 15s 673us/step - loss: 1.6502 - acc: 0.5301 - val_loss: 2.4246 - val_acc: 0.4220\n",
      "Epoch 357/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 1.6745 - acc: 0.5232 - val_loss: 2.4204 - val_acc: 0.4090\n",
      "Epoch 358/500\n",
      "22708/22708 [==============================] - 15s 670us/step - loss: 1.7771 - acc: 0.5158 - val_loss: 2.7369 - val_acc: 0.3983\n",
      "Epoch 359/500\n",
      "22708/22708 [==============================] - 15s 670us/step - loss: 2.1103 - acc: 0.4455 - val_loss: 2.7007 - val_acc: 0.4044\n",
      "Epoch 360/500\n",
      "22708/22708 [==============================] - 15s 668us/step - loss: 1.9184 - acc: 0.4748 - val_loss: 2.3761 - val_acc: 0.4255\n",
      "Epoch 361/500\n",
      "22708/22708 [==============================] - 15s 668us/step - loss: 1.6814 - acc: 0.5213 - val_loss: 2.4339 - val_acc: 0.4144\n",
      "Epoch 362/500\n",
      "22708/22708 [==============================] - 15s 672us/step - loss: 1.7475 - acc: 0.5091 - val_loss: 2.4010 - val_acc: 0.4274\n",
      "Epoch 363/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 1.6398 - acc: 0.5322 - val_loss: 2.4180 - val_acc: 0.4124\n",
      "Epoch 364/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 1.6622 - acc: 0.5288 - val_loss: 2.4109 - val_acc: 0.4292\n",
      "Epoch 365/500\n",
      "22708/22708 [==============================] - 15s 668us/step - loss: 1.6303 - acc: 0.5338 - val_loss: 2.4475 - val_acc: 0.4299\n",
      "Epoch 366/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 1.6178 - acc: 0.5411 - val_loss: 2.3656 - val_acc: 0.4342\n",
      "Epoch 367/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 1.6405 - acc: 0.5331 - val_loss: 2.4465 - val_acc: 0.4128\n",
      "Epoch 368/500\n",
      "22708/22708 [==============================] - 15s 678us/step - loss: 1.6110 - acc: 0.5404 - val_loss: 2.4267 - val_acc: 0.4255\n",
      "Epoch 369/500\n",
      "22708/22708 [==============================] - 15s 668us/step - loss: 1.6529 - acc: 0.5275 - val_loss: 2.4070 - val_acc: 0.4324\n",
      "Epoch 370/500\n",
      "22708/22708 [==============================] - 15s 668us/step - loss: 1.6495 - acc: 0.5309 - val_loss: 2.4084 - val_acc: 0.4340\n",
      "Epoch 371/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 1.6108 - acc: 0.5400 - val_loss: 2.4078 - val_acc: 0.4316\n",
      "Epoch 372/500\n",
      "22708/22708 [==============================] - 15s 672us/step - loss: 1.6632 - acc: 0.5254 - val_loss: 2.4056 - val_acc: 0.4317\n",
      "Epoch 373/500\n",
      "22708/22708 [==============================] - 15s 670us/step - loss: 1.5925 - acc: 0.5450 - val_loss: 2.4104 - val_acc: 0.4197\n",
      "Epoch 374/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 1.9911 - acc: 0.4626 - val_loss: 2.7322 - val_acc: 0.4162\n",
      "Epoch 375/500\n",
      "22708/22708 [==============================] - 15s 672us/step - loss: 1.9949 - acc: 0.4688 - val_loss: 3.2271 - val_acc: 0.3357\n",
      "Epoch 376/500\n",
      "22708/22708 [==============================] - 15s 673us/step - loss: 2.2349 - acc: 0.4271 - val_loss: 2.7330 - val_acc: 0.3981\n",
      "Epoch 377/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 1.9090 - acc: 0.4779 - val_loss: 2.7779 - val_acc: 0.4093\n",
      "Epoch 378/500\n",
      "22708/22708 [==============================] - 15s 672us/step - loss: 2.4174 - acc: 0.4135 - val_loss: 2.8054 - val_acc: 0.3417\n",
      "Epoch 379/500\n",
      "22708/22708 [==============================] - 15s 670us/step - loss: 2.2257 - acc: 0.4334 - val_loss: 2.9593 - val_acc: 0.3932\n",
      "Epoch 380/500\n",
      "22708/22708 [==============================] - 15s 672us/step - loss: 2.1947 - acc: 0.4316 - val_loss: 3.0059 - val_acc: 0.3957\n",
      "Epoch 381/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 2.1525 - acc: 0.4391 - val_loss: 2.8092 - val_acc: 0.3860\n",
      "Epoch 382/500\n",
      "22708/22708 [==============================] - 15s 673us/step - loss: 2.1311 - acc: 0.4427 - val_loss: 2.8725 - val_acc: 0.3918\n",
      "Epoch 383/500\n",
      "22708/22708 [==============================] - 15s 676us/step - loss: 2.1014 - acc: 0.4442 - val_loss: 2.8146 - val_acc: 0.3927\n",
      "Epoch 384/500\n",
      "22708/22708 [==============================] - 15s 673us/step - loss: 2.0861 - acc: 0.4447 - val_loss: 2.9257 - val_acc: 0.3822\n",
      "Epoch 385/500\n",
      "22708/22708 [==============================] - 15s 672us/step - loss: 2.0995 - acc: 0.4416 - val_loss: 2.7354 - val_acc: 0.3946\n",
      "Epoch 386/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 2.0364 - acc: 0.4528 - val_loss: 2.6535 - val_acc: 0.3863\n",
      "Epoch 387/500\n",
      "22708/22708 [==============================] - 15s 674us/step - loss: 1.9656 - acc: 0.4641 - val_loss: 2.3841 - val_acc: 0.4292\n",
      "Epoch 388/500\n",
      "22708/22708 [==============================] - 15s 680us/step - loss: 1.8041 - acc: 0.4952 - val_loss: 2.4261 - val_acc: 0.4208\n",
      "Epoch 389/500\n",
      "22708/22708 [==============================] - 16s 685us/step - loss: 1.6932 - acc: 0.5204 - val_loss: 2.4363 - val_acc: 0.4219\n",
      "Epoch 390/500\n",
      "22708/22708 [==============================] - 15s 674us/step - loss: 1.6294 - acc: 0.5369 - val_loss: 2.4709 - val_acc: 0.4210\n",
      "Epoch 391/500\n",
      "22708/22708 [==============================] - 15s 674us/step - loss: 1.5980 - acc: 0.5428 - val_loss: 2.4321 - val_acc: 0.4353\n",
      "Epoch 392/500\n",
      "22708/22708 [==============================] - 15s 674us/step - loss: 1.6226 - acc: 0.5368 - val_loss: 2.5407 - val_acc: 0.4258\n",
      "Epoch 393/500\n",
      "22708/22708 [==============================] - 15s 676us/step - loss: 1.5978 - acc: 0.5439 - val_loss: 2.4518 - val_acc: 0.4269\n",
      "Epoch 394/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 1.6183 - acc: 0.5384 - val_loss: 2.4787 - val_acc: 0.4295\n",
      "Epoch 395/500\n",
      "22708/22708 [==============================] - 15s 676us/step - loss: 1.5767 - acc: 0.5506 - val_loss: 2.4342 - val_acc: 0.4301\n",
      "Epoch 396/500\n",
      "22708/22708 [==============================] - 15s 676us/step - loss: 1.6846 - acc: 0.5258 - val_loss: 2.4816 - val_acc: 0.4171\n",
      "Epoch 397/500\n",
      "22708/22708 [==============================] - 15s 676us/step - loss: 1.5893 - acc: 0.5434 - val_loss: 2.5217 - val_acc: 0.4275\n",
      "Epoch 398/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 1.5938 - acc: 0.5450 - val_loss: 2.4588 - val_acc: 0.4123\n",
      "Epoch 399/500\n",
      "22708/22708 [==============================] - 15s 675us/step - loss: 1.5570 - acc: 0.5541 - val_loss: 2.4513 - val_acc: 0.4282\n",
      "Epoch 400/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 1.5760 - acc: 0.5485 - val_loss: 2.5047 - val_acc: 0.4142\n",
      "Epoch 401/500\n",
      "22708/22708 [==============================] - 15s 673us/step - loss: 1.5768 - acc: 0.5471 - val_loss: 2.4212 - val_acc: 0.4215\n",
      "Epoch 402/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 1.5360 - acc: 0.5597 - val_loss: 2.5342 - val_acc: 0.4295\n",
      "Epoch 403/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 1.6606 - acc: 0.5289 - val_loss: 2.4469 - val_acc: 0.4293\n",
      "Epoch 404/500\n",
      "22708/22708 [==============================] - 15s 674us/step - loss: 1.5575 - acc: 0.5551 - val_loss: 2.5224 - val_acc: 0.4269\n",
      "Epoch 405/500\n",
      "22708/22708 [==============================] - 15s 673us/step - loss: 1.5549 - acc: 0.5545 - val_loss: 2.4571 - val_acc: 0.4260\n",
      "Epoch 406/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 1.5444 - acc: 0.5594 - val_loss: 2.5066 - val_acc: 0.4300\n",
      "Epoch 407/500\n",
      "22708/22708 [==============================] - 15s 668us/step - loss: 1.5314 - acc: 0.5637 - val_loss: 2.6138 - val_acc: 0.4245\n",
      "Epoch 408/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 1.5233 - acc: 0.5628 - val_loss: 2.5227 - val_acc: 0.4234\n",
      "Epoch 409/500\n",
      "22708/22708 [==============================] - 15s 674us/step - loss: 1.5357 - acc: 0.5591 - val_loss: 2.4369 - val_acc: 0.4276\n",
      "Epoch 410/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 1.5295 - acc: 0.5598 - val_loss: 2.4736 - val_acc: 0.4297\n",
      "Epoch 411/500\n",
      "22708/22708 [==============================] - 15s 670us/step - loss: 1.5197 - acc: 0.5655 - val_loss: 2.5033 - val_acc: 0.4279\n",
      "Epoch 412/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 1.5656 - acc: 0.5521 - val_loss: 2.4668 - val_acc: 0.4202\n",
      "Epoch 413/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 1.5082 - acc: 0.5665 - val_loss: 3.0093 - val_acc: 0.3724\n",
      "Epoch 414/500\n",
      "22708/22708 [==============================] - 15s 672us/step - loss: 1.6128 - acc: 0.5428 - val_loss: 2.5591 - val_acc: 0.4187\n",
      "Epoch 415/500\n",
      "22708/22708 [==============================] - 15s 673us/step - loss: 1.5519 - acc: 0.5547 - val_loss: 2.5687 - val_acc: 0.4151\n",
      "Epoch 416/500\n",
      "22708/22708 [==============================] - 15s 670us/step - loss: 1.8677 - acc: 0.4908 - val_loss: 2.3605 - val_acc: 0.4291\n",
      "Epoch 417/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 1.5757 - acc: 0.5479 - val_loss: 2.4528 - val_acc: 0.4293\n",
      "Epoch 418/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 1.4858 - acc: 0.5693 - val_loss: 2.4752 - val_acc: 0.4333\n",
      "Epoch 419/500\n",
      "22708/22708 [==============================] - 15s 670us/step - loss: 1.5371 - acc: 0.5573 - val_loss: 2.4941 - val_acc: 0.4273\n",
      "Epoch 420/500\n",
      "22708/22708 [==============================] - 15s 672us/step - loss: 1.4842 - acc: 0.5720 - val_loss: 2.5320 - val_acc: 0.4205\n",
      "Epoch 421/500\n",
      "22708/22708 [==============================] - 15s 668us/step - loss: 1.5270 - acc: 0.5613 - val_loss: 2.4864 - val_acc: 0.4331\n",
      "Epoch 422/500\n",
      "22708/22708 [==============================] - 15s 670us/step - loss: 1.4743 - acc: 0.5778 - val_loss: 2.4927 - val_acc: 0.4216\n",
      "Epoch 423/500\n",
      "22708/22708 [==============================] - 15s 668us/step - loss: 1.4695 - acc: 0.5754 - val_loss: 2.5498 - val_acc: 0.4306\n",
      "Epoch 424/500\n",
      "22708/22708 [==============================] - 15s 668us/step - loss: 1.5000 - acc: 0.5659 - val_loss: 2.5620 - val_acc: 0.4235\n",
      "Epoch 425/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 1.4540 - acc: 0.5826 - val_loss: 2.5743 - val_acc: 0.4102\n",
      "Epoch 426/500\n",
      "22708/22708 [==============================] - 15s 670us/step - loss: 1.4792 - acc: 0.5747 - val_loss: 2.5777 - val_acc: 0.4193\n",
      "Epoch 427/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 1.5986 - acc: 0.5425 - val_loss: 2.9352 - val_acc: 0.3846\n",
      "Epoch 428/500\n",
      "22708/22708 [==============================] - 15s 670us/step - loss: 1.9443 - acc: 0.4806 - val_loss: 2.9477 - val_acc: 0.3701\n",
      "Epoch 429/500\n",
      "22708/22708 [==============================] - 15s 673us/step - loss: 2.2044 - acc: 0.4325 - val_loss: 3.0303 - val_acc: 0.3634\n",
      "Epoch 430/500\n",
      "22708/22708 [==============================] - 15s 675us/step - loss: 2.0979 - acc: 0.4460 - val_loss: 2.7262 - val_acc: 0.4002\n",
      "Epoch 431/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 1.9587 - acc: 0.4637 - val_loss: 2.6525 - val_acc: 0.4008\n",
      "Epoch 432/500\n",
      "22708/22708 [==============================] - 15s 667us/step - loss: 1.8722 - acc: 0.4863 - val_loss: 2.3726 - val_acc: 0.4350\n",
      "Epoch 433/500\n",
      "22708/22708 [==============================] - 15s 666us/step - loss: 1.5632 - acc: 0.5493 - val_loss: 2.4031 - val_acc: 0.4345\n",
      "Epoch 434/500\n",
      "22708/22708 [==============================] - 15s 670us/step - loss: 1.4881 - acc: 0.5712 - val_loss: 2.4238 - val_acc: 0.4284\n",
      "Epoch 435/500\n",
      "22708/22708 [==============================] - 15s 675us/step - loss: 1.4455 - acc: 0.5820 - val_loss: 2.6927 - val_acc: 0.4008\n",
      "Epoch 436/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 1.6338 - acc: 0.5370 - val_loss: 2.4870 - val_acc: 0.4295\n",
      "Epoch 437/500\n",
      "22708/22708 [==============================] - 15s 665us/step - loss: 1.4494 - acc: 0.5819 - val_loss: 2.5751 - val_acc: 0.3954\n",
      "Epoch 438/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 1.4635 - acc: 0.5763 - val_loss: 2.5131 - val_acc: 0.4311\n",
      "Epoch 439/500\n",
      "22708/22708 [==============================] - 15s 668us/step - loss: 1.4373 - acc: 0.5848 - val_loss: 2.4823 - val_acc: 0.4295\n",
      "Epoch 440/500\n",
      "22708/22708 [==============================] - 15s 670us/step - loss: 1.4745 - acc: 0.5742 - val_loss: 2.6102 - val_acc: 0.4237\n",
      "Epoch 441/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 1.4802 - acc: 0.5732 - val_loss: 2.5231 - val_acc: 0.4081\n",
      "Epoch 442/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 1.4746 - acc: 0.5763 - val_loss: 2.5034 - val_acc: 0.4355\n",
      "Epoch 443/500\n",
      "22708/22708 [==============================] - 15s 670us/step - loss: 1.3936 - acc: 0.5939 - val_loss: 2.5258 - val_acc: 0.4302\n",
      "Epoch 444/500\n",
      "22708/22708 [==============================] - 15s 670us/step - loss: 1.3905 - acc: 0.5943 - val_loss: 2.5050 - val_acc: 0.4234\n",
      "Epoch 445/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 1.4696 - acc: 0.5768 - val_loss: 2.4933 - val_acc: 0.4181\n",
      "Epoch 446/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 1.4058 - acc: 0.5897 - val_loss: 2.5373 - val_acc: 0.4296\n",
      "Epoch 447/500\n",
      "22708/22708 [==============================] - 15s 670us/step - loss: 1.4381 - acc: 0.5872 - val_loss: 2.5656 - val_acc: 0.4286\n",
      "Epoch 448/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 1.4147 - acc: 0.5876 - val_loss: 2.5074 - val_acc: 0.4213\n",
      "Epoch 449/500\n",
      "22708/22708 [==============================] - 15s 672us/step - loss: 1.3816 - acc: 0.5969 - val_loss: 2.5975 - val_acc: 0.4244\n",
      "Epoch 450/500\n",
      "22708/22708 [==============================] - 15s 675us/step - loss: 1.4033 - acc: 0.5913 - val_loss: 2.5104 - val_acc: 0.4227\n",
      "Epoch 451/500\n",
      "22708/22708 [==============================] - 15s 674us/step - loss: 1.3569 - acc: 0.6056 - val_loss: 2.4957 - val_acc: 0.4147\n",
      "Epoch 452/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 1.5579 - acc: 0.5565 - val_loss: 2.6826 - val_acc: 0.4049\n",
      "Epoch 453/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 1.9365 - acc: 0.4775 - val_loss: 2.8815 - val_acc: 0.3797\n",
      "Epoch 454/500\n",
      "22708/22708 [==============================] - 15s 670us/step - loss: 2.4792 - acc: 0.4170 - val_loss: 2.9637 - val_acc: 0.3344\n",
      "Epoch 455/500\n",
      "22708/22708 [==============================] - 15s 675us/step - loss: 2.2839 - acc: 0.4279 - val_loss: 2.8280 - val_acc: 0.3747\n",
      "Epoch 456/500\n",
      "22708/22708 [==============================] - 15s 668us/step - loss: 2.2024 - acc: 0.4358 - val_loss: 2.9931 - val_acc: 0.3974\n",
      "Epoch 457/500\n",
      "22708/22708 [==============================] - 15s 673us/step - loss: 2.1719 - acc: 0.4380 - val_loss: 2.9120 - val_acc: 0.4016\n",
      "Epoch 458/500\n",
      "22708/22708 [==============================] - 15s 668us/step - loss: 2.1149 - acc: 0.4424 - val_loss: 2.8602 - val_acc: 0.3915\n",
      "Epoch 459/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 2.0826 - acc: 0.4471 - val_loss: 3.1625 - val_acc: 0.3382\n",
      "Epoch 460/500\n",
      "22708/22708 [==============================] - 15s 667us/step - loss: 2.0001 - acc: 0.4602 - val_loss: 2.6928 - val_acc: 0.3901\n",
      "Epoch 461/500\n",
      "22708/22708 [==============================] - 15s 666us/step - loss: 1.7396 - acc: 0.5119 - val_loss: 2.6551 - val_acc: 0.4068\n",
      "Epoch 462/500\n",
      "22708/22708 [==============================] - 15s 672us/step - loss: 2.4763 - acc: 0.4186 - val_loss: 2.8149 - val_acc: 0.3953\n",
      "Epoch 463/500\n",
      "22708/22708 [==============================] - 15s 673us/step - loss: 2.2461 - acc: 0.4329 - val_loss: 2.8229 - val_acc: 0.4010\n",
      "Epoch 464/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 2.1840 - acc: 0.4369 - val_loss: 2.9534 - val_acc: 0.3745\n",
      "Epoch 465/500\n",
      "22708/22708 [==============================] - 15s 667us/step - loss: 2.1359 - acc: 0.4410 - val_loss: 2.9887 - val_acc: 0.3799\n",
      "Epoch 466/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 2.0882 - acc: 0.4472 - val_loss: 2.8814 - val_acc: 0.3892\n",
      "Epoch 467/500\n",
      "22708/22708 [==============================] - 15s 672us/step - loss: 2.0474 - acc: 0.4513 - val_loss: 2.8069 - val_acc: 0.3974\n",
      "Epoch 468/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 1.9834 - acc: 0.4592 - val_loss: 2.8434 - val_acc: 0.3983\n",
      "Epoch 469/500\n",
      "22708/22708 [==============================] - 15s 668us/step - loss: 1.7913 - acc: 0.4984 - val_loss: 2.4075 - val_acc: 0.4357\n",
      "Epoch 470/500\n",
      "22708/22708 [==============================] - 15s 673us/step - loss: 1.5931 - acc: 0.5463 - val_loss: 2.4990 - val_acc: 0.4311\n",
      "Epoch 471/500\n",
      "22708/22708 [==============================] - 15s 668us/step - loss: 1.4242 - acc: 0.5880 - val_loss: 2.5095 - val_acc: 0.4205\n",
      "Epoch 472/500\n",
      "22708/22708 [==============================] - 15s 667us/step - loss: 1.4052 - acc: 0.5914 - val_loss: 2.5344 - val_acc: 0.4217\n",
      "Epoch 473/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 1.4236 - acc: 0.5902 - val_loss: 2.5027 - val_acc: 0.4317\n",
      "Epoch 474/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 1.4836 - acc: 0.5725 - val_loss: 2.5441 - val_acc: 0.4315\n",
      "Epoch 475/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 1.3896 - acc: 0.5983 - val_loss: 2.5528 - val_acc: 0.4156\n",
      "Epoch 476/500\n",
      "22708/22708 [==============================] - 15s 667us/step - loss: 1.3733 - acc: 0.6041 - val_loss: 2.4863 - val_acc: 0.4306\n",
      "Epoch 477/500\n",
      "22708/22708 [==============================] - 15s 668us/step - loss: 1.3726 - acc: 0.6010 - val_loss: 2.5647 - val_acc: 0.4280\n",
      "Epoch 478/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 1.3940 - acc: 0.5944 - val_loss: 2.5262 - val_acc: 0.4254\n",
      "Epoch 479/500\n",
      "22708/22708 [==============================] - 15s 668us/step - loss: 1.3419 - acc: 0.6086 - val_loss: 2.5837 - val_acc: 0.4242\n",
      "Epoch 480/500\n",
      "22708/22708 [==============================] - 15s 668us/step - loss: 1.3561 - acc: 0.6065 - val_loss: 2.5249 - val_acc: 0.4174\n",
      "Epoch 481/500\n",
      "22708/22708 [==============================] - 15s 666us/step - loss: 1.3488 - acc: 0.6113 - val_loss: 2.5792 - val_acc: 0.4166\n",
      "Epoch 482/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 1.4104 - acc: 0.5915 - val_loss: 2.5804 - val_acc: 0.4272\n",
      "Epoch 483/500\n",
      "22708/22708 [==============================] - 15s 670us/step - loss: 1.3802 - acc: 0.6003 - val_loss: 2.6175 - val_acc: 0.4281\n",
      "Epoch 484/500\n",
      "22708/22708 [==============================] - 15s 666us/step - loss: 1.4207 - acc: 0.5892 - val_loss: 2.6420 - val_acc: 0.4194\n",
      "Epoch 485/500\n",
      "22708/22708 [==============================] - 15s 665us/step - loss: 1.3810 - acc: 0.5961 - val_loss: 2.6493 - val_acc: 0.4120\n",
      "Epoch 486/500\n",
      "22708/22708 [==============================] - 15s 667us/step - loss: 1.3716 - acc: 0.6023 - val_loss: 2.6084 - val_acc: 0.4124\n",
      "Epoch 487/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 1.3329 - acc: 0.6115 - val_loss: 2.6205 - val_acc: 0.4089\n",
      "Epoch 488/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 2.1214 - acc: 0.4451 - val_loss: 2.8775 - val_acc: 0.3898\n",
      "Epoch 489/500\n",
      "22708/22708 [==============================] - 15s 667us/step - loss: 1.9616 - acc: 0.4637 - val_loss: 2.5883 - val_acc: 0.4041\n",
      "Epoch 490/500\n",
      "22708/22708 [==============================] - 15s 670us/step - loss: 1.9685 - acc: 0.4712 - val_loss: 2.7194 - val_acc: 0.3915\n",
      "Epoch 491/500\n",
      "22708/22708 [==============================] - 15s 672us/step - loss: 1.9239 - acc: 0.4956 - val_loss: 3.0180 - val_acc: 0.3175\n",
      "Epoch 492/500\n",
      "22708/22708 [==============================] - 15s 670us/step - loss: 2.3521 - acc: 0.4251 - val_loss: 2.8327 - val_acc: 0.4109\n",
      "Epoch 493/500\n",
      "22708/22708 [==============================] - 15s 669us/step - loss: 2.1697 - acc: 0.4389 - val_loss: 2.9049 - val_acc: 0.3909\n",
      "Epoch 494/500\n",
      "22708/22708 [==============================] - 15s 670us/step - loss: 2.0760 - acc: 0.4465 - val_loss: 2.9879 - val_acc: 0.4101\n",
      "Epoch 495/500\n",
      "22708/22708 [==============================] - 15s 670us/step - loss: 2.0306 - acc: 0.4543 - val_loss: 2.8506 - val_acc: 0.4032\n",
      "Epoch 496/500\n",
      "22708/22708 [==============================] - 15s 668us/step - loss: 1.8709 - acc: 0.4842 - val_loss: 2.4594 - val_acc: 0.4259\n",
      "Epoch 497/500\n",
      "22708/22708 [==============================] - 15s 670us/step - loss: 1.7016 - acc: 0.5275 - val_loss: 2.7285 - val_acc: 0.3753\n",
      "Epoch 498/500\n",
      "22708/22708 [==============================] - 15s 670us/step - loss: 1.6269 - acc: 0.5358 - val_loss: 2.5628 - val_acc: 0.4165\n",
      "Epoch 499/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 2.3977 - acc: 0.4225 - val_loss: 2.9483 - val_acc: 0.3847\n",
      "Epoch 500/500\n",
      "22708/22708 [==============================] - 15s 671us/step - loss: 2.2531 - acc: 0.4322 - val_loss: 2.9728 - val_acc: 0.3284\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ffae5805588>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 13
    }
   ]
  }
 ]
}