{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "256-BatchNormalization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "accelerator": "GPU",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "kHwmHF3W-ycN",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import os\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.layers import Dense, Dropout, Activation, LSTM, Bidirectional, Input, Reshape, Permute, Lambda, TimeDistributed, SpatialDropout1D, BatchNormalization, GlobalAveragePooling1D\n",
    "from keras.layers.merge import Add, Multiply, multiply\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fTHrv35G-3Vt",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def load_seq_lst(lst_name, num_seq, ovr_num=None, num_joints=25, num_class=52):\n",
    "    assert (os.path.isfile(lst_name + '.txt') and os.path.isfile(lst_name + '.h5'))\n",
    "    if ovr_num is None:\n",
    "        ovr_num = int(num_seq / 2)\n",
    "    keyname_lst = [item.strip().split() for item in open(lst_name + '.txt', 'r').readlines()]\n",
    "    X = []\n",
    "    Y = []\n",
    "    vid_list = []\n",
    "    start_list = []\n",
    "    with h5py.File(lst_name + '.h5', 'r') as hf:\n",
    "        for item in keyname_lst:\n",
    "            skeleton = np.asarray(hf.get(item[0]))\n",
    "            skeleton1 = skeleton[:, 0:75].reshape((skeleton.shape[0], num_joints, 3))\n",
    "            skeleton2 = skeleton[:, 75:].reshape((skeleton.shape[0], num_joints, 3))\n",
    "            skeleton = np.concatenate((skeleton1, skeleton2), axis=-1)\n",
    "\n",
    "            labels = np.asarray(hf.get(item[1]), dtype=int)\n",
    "            labels_pertime = np.zeros((skeleton.shape[0]), dtype=np.int32)\n",
    "            for clip_idx in range(len(labels)):\n",
    "                # notice: for detection, labels start from 1, as there are empty clips for the input stream\n",
    "                labels_pertime[labels[clip_idx][1]:labels[clip_idx][2]] = labels[clip_idx][0] + 1\n",
    "            labels_pertime = labels_pertime.astype(np.int32)\n",
    "            labels_pertime = np_utils.to_categorical(labels_pertime, num_class)\n",
    "\n",
    "            if skeleton.shape[0] > num_seq:\n",
    "                start = 0\n",
    "                while start + num_seq < skeleton.shape[0]:\n",
    "                    X.append(skeleton[start:start + num_seq])\n",
    "                    Y.append(labels_pertime[start + num_seq])\n",
    "                    vid_list.append(item[0])\n",
    "                    start_list.append(start)\n",
    "                    start = start + ovr_num\n",
    "                X.append(skeleton[-num_seq:])\n",
    "                Y.append(labels_pertime[-1])\n",
    "                vid_list.append(item[0])\n",
    "                start_list.append(skeleton.shape[0] - num_seq)\n",
    "            else:\n",
    "                skeleton = np.concatenate((np.zeros((num_seq - skeleton.shape[0], skeleton.shape[1], skeleton.shape[2])), skeleton), axis=0)\n",
    "                labels_pertime = np.concatenate((np.zeros((num_seq - labels_pertime.shape[0], labels_pertime.shape[1])), labels_pertime), axis=0)\n",
    "                X.append(skeleton)\n",
    "                Y.append(labels_pertime[-1])\n",
    "                vid_list.append(item[0])\n",
    "                start_list.append(0)\n",
    "\n",
    "    X = np.asarray(X).astype(np.float32)\n",
    "    Y = np.asarray(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y, vid_list, start_list"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rZOuSHXx-5cT",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def multi_pertime(param, sub_mean=False):\n",
    "    skt_input = Input(shape=(param['num_seq'], param['num_joints'], param['dim_point']))\n",
    "    data = skt_input\n",
    "\n",
    "    if sub_mean:\n",
    "        data = Permute((1, 3, 2))(data)\n",
    "        data2 = TimeDistributed(TimeDistributed(Dense(1, trainable=False)))(data)\n",
    "        data2 = Lambda(lambda x: K.repeat_elements(x, param['num_joints'], axis=-1), output_shape=lambda s: (s[0], s[1], s[2], s[3] * param['num_joints']))(data2)\n",
    "        data = Add()([data, data2])\n",
    "\n",
    "    data = Reshape((param['num_seq'], param['num_joints'] * param['dim_point']))(data)\n",
    "\n",
    "    # squeeze = GlobalAveragePooling1D()(data)\n",
    "    # excitation = Dense(units=256 // 4)(squeeze)\n",
    "    # excitation = Activation('relu')(excitation)\n",
    "    # excitation = Dense(units=256)(excitation)\n",
    "    # excitation = Activation('sigmoid')(excitation)\n",
    "    # excitation = Reshape((1,256))(excitation)\n",
    "\n",
    "    # scale = multiply([data,excitation])\n",
    "\n",
    "    hid_size = 256\n",
    "    # data = SpatialDropout1D(0.05)(scale)\n",
    "    out = Bidirectional(LSTM(hid_size, return_sequences=True))(data)\n",
    "    # out = SpatialDropout1D(0.05)(out)\n",
    "    out = Bidirectional(LSTM(hid_size, return_sequences=True))(out)\n",
    "    # out = SpatialDropout1D(0.05)(out)\n",
    "    out = Bidirectional(LSTM(hid_size, return_sequences=True))(out)\n",
    "\n",
    "    out = BatchNormalization()(out)\n",
    "    #out = Dropout(0.5)(out)\n",
    "    out = GlobalAveragePooling1D()(out)\n",
    "    out = Activation('relu')(Dropout(0.5)(out))\n",
    "    prob = Dense(param['num_class'], activation='softmax')(out)\n",
    "\n",
    "    opt = SGD(lr=param['base_learn_rate'], decay=param['weight_regular'], momentum=0.9, nesterov=True)\n",
    "    model = Model(skt_input, prob)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oOpHGRYd_FEA",
    "colab_type": "code",
    "outputId": "a34697ce-7ea0-442b-d4be-7926f66b12f4",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5afvV-DdtUcL",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def schedule(epoch):\n",
    "    lr = K.get_value(model.optimizer.lr)\n",
    "    if epoch % param['step_inter'] == 0 and epoch > 0:\n",
    "        lr = lr * param['lr_gamma']\n",
    "    return np.float(lr)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PFe5IAPg-7gU",
    "colab_type": "code",
    "outputId": "e5b117b0-8d72-43c4-f653-65b48e2f4efa",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    }
   },
   "source": [
    "param = {'max_iter': 500, 'step_inter': 80, 'base_learn_rate': 0.01, 'lr_gamma': 0.5, 'weight_regular': 0, 'num_seq': 50, 'batchsize': 1024, 'write_file': True,\n",
    "         'num_joints': 25, 'num_class': 51 + 1, 'dim_point': 3 * 2, 'save_model': True}\n",
    "param['trn_file'] = F\"/content/drive/My Drive/crs_view_trn_skt\"\n",
    "param['val_file'] = F\"/content/drive/My Drive/crs_view_val_skt\"\n",
    "param['save_path'] = F\"/content/drive/My Drive/save_param_temp/gcn/weights2.hdf5\"\n",
    "param['write_file_name'] = F\"/content/drive/My Drive/save_param_temp/gcn.txt\"\n",
    "# no batch size, as one batch only\n",
    "\n",
    "trainX, trainY, train_vid_list, train_start_list = load_seq_lst(param['trn_file'], param['num_seq'])\n",
    "valX, valY, val_vid_list, val_start_list = load_seq_lst(param['val_file'], param['num_seq'])"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "(22708, 50, 25, 6) (22708, 52)\n",
      "(12767, 50, 25, 6) (12767, 52)\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rJ_AM7qAAgEg",
    "colab_type": "code",
    "outputId": "aa3a65c2-0349-410d-d781-357f13f8c1d0",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "source": [
    "model = multi_pertime(param, sub_mean=True)\n",
    "reduce_lr = LearningRateScheduler(schedule)\n",
    "checkpoint = ModelCheckpoint(param['save_path'], verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=5)\n",
    "\n",
    "model.fit(trainX, trainY, batch_size=param['batchsize'], epochs=param['max_iter'], shuffle=True, validation_data=(valX, valY),callbacks=[checkpoint], verbose=1)"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 50, 25, 6)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "permute_11 (Permute)            (None, 50, 6, 25)    0           input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_22 (TimeDistri (None, 50, 6, 1)     26          permute_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 50, 6, 25)    0           time_distributed_22[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 50, 6, 25)    0           permute_11[0][0]                 \n",
      "                                                                 lambda_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_15 (Reshape)            (None, 50, 150)      0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_15 (Bidirectional (None, 50, 512)      833536      reshape_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_16 (Bidirectional (None, 50, 512)      1574912     bidirectional_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_17 (Bidirectional (None, 50, 512)      1574912     bidirectional_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 50, 512)      2048        bidirectional_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_8 (Glo (None, 512)          0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 512)          0           global_average_pooling1d_8[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 512)          0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 52)           26676       activation_10[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 4,012,110\n",
      "Trainable params: 4,011,086\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n",
      "Train on 22708 samples, validate on 12767 samples\n",
      "Epoch 1/500\n",
      "22708/22708 [==============================] - 26s 1ms/step - loss: 3.1279 - acc: 0.3338 - val_loss: 8.2491 - val_acc: 0.4072\n",
      "Epoch 2/500\n",
      "22708/22708 [==============================] - 21s 941us/step - loss: 2.6605 - acc: 0.3989 - val_loss: 4.2274 - val_acc: 0.4072\n",
      "Epoch 3/500\n",
      "22708/22708 [==============================] - 22s 953us/step - loss: 2.5448 - acc: 0.4031 - val_loss: 4.0599 - val_acc: 0.4072\n",
      "Epoch 4/500\n",
      "22708/22708 [==============================] - 22s 964us/step - loss: 2.4787 - acc: 0.4084 - val_loss: 3.5914 - val_acc: 0.4072\n",
      "Epoch 5/500\n",
      "22708/22708 [==============================] - 22s 969us/step - loss: 2.4141 - acc: 0.4160 - val_loss: 3.0555 - val_acc: 0.4079\n",
      "Epoch 6/500\n",
      "22708/22708 [==============================] - 22s 970us/step - loss: 2.3824 - acc: 0.4190 - val_loss: 3.0477 - val_acc: 0.4070\n",
      "Epoch 7/500\n",
      "22708/22708 [==============================] - 22s 966us/step - loss: 2.3215 - acc: 0.4219 - val_loss: 3.2447 - val_acc: 0.4097\n",
      "Epoch 8/500\n",
      "22708/22708 [==============================] - 22s 966us/step - loss: 2.2863 - acc: 0.4253 - val_loss: 3.0803 - val_acc: 0.4072\n",
      "Epoch 9/500\n",
      "22708/22708 [==============================] - 22s 965us/step - loss: 2.2594 - acc: 0.4289 - val_loss: 3.0829 - val_acc: 0.4095\n",
      "Epoch 10/500\n",
      "22708/22708 [==============================] - 22s 967us/step - loss: 2.2151 - acc: 0.4369 - val_loss: 2.8183 - val_acc: 0.4096\n",
      "Epoch 11/500\n",
      "22708/22708 [==============================] - 22s 965us/step - loss: 2.1776 - acc: 0.4403 - val_loss: 2.9797 - val_acc: 0.4081\n",
      "Epoch 12/500\n",
      "22708/22708 [==============================] - 22s 969us/step - loss: 2.1596 - acc: 0.4431 - val_loss: 2.8403 - val_acc: 0.4102\n",
      "Epoch 13/500\n",
      "22708/22708 [==============================] - 22s 967us/step - loss: 2.1430 - acc: 0.4437 - val_loss: 2.9653 - val_acc: 0.4127\n",
      "Epoch 14/500\n",
      "22708/22708 [==============================] - 22s 969us/step - loss: 2.1250 - acc: 0.4487 - val_loss: 3.1258 - val_acc: 0.4076\n",
      "Epoch 15/500\n",
      "22708/22708 [==============================] - 22s 967us/step - loss: 2.1015 - acc: 0.4523 - val_loss: 2.7796 - val_acc: 0.3778\n",
      "Epoch 16/500\n",
      "22708/22708 [==============================] - 22s 970us/step - loss: 2.0690 - acc: 0.4540 - val_loss: 2.9398 - val_acc: 0.4078\n",
      "Epoch 17/500\n",
      "22708/22708 [==============================] - 22s 965us/step - loss: 2.0516 - acc: 0.4618 - val_loss: 2.7242 - val_acc: 0.4170\n",
      "Epoch 18/500\n",
      "22708/22708 [==============================] - 22s 970us/step - loss: 2.0198 - acc: 0.4617 - val_loss: 2.8588 - val_acc: 0.4113\n",
      "Epoch 19/500\n",
      "22708/22708 [==============================] - 22s 967us/step - loss: 2.0147 - acc: 0.4663 - val_loss: 2.8461 - val_acc: 0.4163\n",
      "Epoch 20/500\n",
      "22708/22708 [==============================] - 22s 964us/step - loss: 1.9724 - acc: 0.4735 - val_loss: 3.1375 - val_acc: 0.4047\n",
      "Epoch 21/500\n",
      "22708/22708 [==============================] - 22s 962us/step - loss: 1.9751 - acc: 0.4727 - val_loss: 2.8717 - val_acc: 0.4103\n",
      "Epoch 22/500\n",
      "22708/22708 [==============================] - 22s 966us/step - loss: 2.1752 - acc: 0.4402 - val_loss: 3.8226 - val_acc: 0.4078\n",
      "Epoch 23/500\n",
      "22708/22708 [==============================] - 22s 966us/step - loss: 1.9996 - acc: 0.4652 - val_loss: 2.9349 - val_acc: 0.4105\n",
      "Epoch 24/500\n",
      "22708/22708 [==============================] - 22s 964us/step - loss: 1.9715 - acc: 0.4723 - val_loss: 3.3491 - val_acc: 0.3923\n",
      "Epoch 25/500\n",
      "22708/22708 [==============================] - 22s 964us/step - loss: 1.9323 - acc: 0.4782 - val_loss: 2.7147 - val_acc: 0.4003\n",
      "Epoch 26/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.9057 - acc: 0.4861 - val_loss: 3.1411 - val_acc: 0.4118\n",
      "Epoch 27/500\n",
      "22708/22708 [==============================] - 22s 959us/step - loss: 1.8993 - acc: 0.4891 - val_loss: 3.1676 - val_acc: 0.3772\n",
      "Epoch 28/500\n",
      "22708/22708 [==============================] - 22s 963us/step - loss: 1.9241 - acc: 0.4819 - val_loss: 2.8225 - val_acc: 0.4147\n",
      "Epoch 29/500\n",
      "22708/22708 [==============================] - 22s 962us/step - loss: 1.8745 - acc: 0.4919 - val_loss: 2.8690 - val_acc: 0.4056\n",
      "Epoch 30/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.8536 - acc: 0.4985 - val_loss: 3.1327 - val_acc: 0.4084\n",
      "Epoch 31/500\n",
      "22708/22708 [==============================] - 22s 964us/step - loss: 2.1689 - acc: 0.4380 - val_loss: 4.1630 - val_acc: 0.3948\n",
      "Epoch 32/500\n",
      "22708/22708 [==============================] - 22s 962us/step - loss: 2.0243 - acc: 0.4658 - val_loss: 3.1176 - val_acc: 0.4088\n",
      "Epoch 33/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.8934 - acc: 0.4882 - val_loss: 2.7110 - val_acc: 0.4194\n",
      "Epoch 34/500\n",
      "22708/22708 [==============================] - 22s 963us/step - loss: 1.8452 - acc: 0.4966 - val_loss: 2.9394 - val_acc: 0.4154\n",
      "Epoch 35/500\n",
      "22708/22708 [==============================] - 22s 962us/step - loss: 1.8256 - acc: 0.5007 - val_loss: 2.8569 - val_acc: 0.4103\n",
      "Epoch 36/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 1.8112 - acc: 0.5042 - val_loss: 2.7018 - val_acc: 0.3973\n",
      "Epoch 37/500\n",
      "22708/22708 [==============================] - 22s 959us/step - loss: 1.8164 - acc: 0.5041 - val_loss: 2.6736 - val_acc: 0.4035\n",
      "Epoch 38/500\n",
      "22708/22708 [==============================] - 22s 959us/step - loss: 1.7771 - acc: 0.5090 - val_loss: 2.9004 - val_acc: 0.3943\n",
      "Epoch 39/500\n",
      "22708/22708 [==============================] - 22s 958us/step - loss: 1.7680 - acc: 0.5115 - val_loss: 3.2133 - val_acc: 0.3994\n",
      "Epoch 40/500\n",
      "22708/22708 [==============================] - 22s 959us/step - loss: 1.7614 - acc: 0.5184 - val_loss: 3.0950 - val_acc: 0.4074\n",
      "Epoch 41/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 1.7662 - acc: 0.5125 - val_loss: 2.8693 - val_acc: 0.4107\n",
      "Epoch 42/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.7390 - acc: 0.5167 - val_loss: 2.8399 - val_acc: 0.3923\n",
      "Epoch 43/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 1.7222 - acc: 0.5199 - val_loss: 3.3535 - val_acc: 0.3358\n",
      "Epoch 44/500\n",
      "22708/22708 [==============================] - 22s 958us/step - loss: 1.7101 - acc: 0.5259 - val_loss: 2.7204 - val_acc: 0.4225\n",
      "Epoch 45/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.7070 - acc: 0.5252 - val_loss: 3.4690 - val_acc: 0.3572\n",
      "Epoch 46/500\n",
      "22708/22708 [==============================] - 22s 959us/step - loss: 1.6954 - acc: 0.5231 - val_loss: 3.0323 - val_acc: 0.4253\n",
      "Epoch 47/500\n",
      "22708/22708 [==============================] - 22s 962us/step - loss: 1.6925 - acc: 0.5279 - val_loss: 3.1294 - val_acc: 0.4046\n",
      "Epoch 48/500\n",
      "22708/22708 [==============================] - 22s 957us/step - loss: 1.6663 - acc: 0.5370 - val_loss: 3.0597 - val_acc: 0.3990\n",
      "Epoch 49/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 1.6503 - acc: 0.5354 - val_loss: 3.1311 - val_acc: 0.3775\n",
      "Epoch 50/500\n",
      "22708/22708 [==============================] - 22s 958us/step - loss: 2.0821 - acc: 0.4573 - val_loss: 3.1547 - val_acc: 0.4087\n",
      "Epoch 51/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.8043 - acc: 0.5047 - val_loss: 3.0452 - val_acc: 0.4162\n",
      "Epoch 52/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.7138 - acc: 0.5217 - val_loss: 2.8303 - val_acc: 0.4220\n",
      "Epoch 53/500\n",
      "22708/22708 [==============================] - 22s 966us/step - loss: 1.7113 - acc: 0.5217 - val_loss: 3.0596 - val_acc: 0.4066\n",
      "Epoch 54/500\n",
      "22708/22708 [==============================] - 22s 959us/step - loss: 1.6705 - acc: 0.5327 - val_loss: 2.9579 - val_acc: 0.4134\n",
      "Epoch 55/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.6302 - acc: 0.5411 - val_loss: 2.8913 - val_acc: 0.4066\n",
      "Epoch 56/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 1.6197 - acc: 0.5439 - val_loss: 2.8187 - val_acc: 0.3946\n",
      "Epoch 57/500\n",
      "22708/22708 [==============================] - 22s 959us/step - loss: 1.6105 - acc: 0.5449 - val_loss: 3.0666 - val_acc: 0.4183\n",
      "Epoch 58/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 1.6132 - acc: 0.5449 - val_loss: 3.0707 - val_acc: 0.4171\n",
      "Epoch 59/500\n",
      "22708/22708 [==============================] - 22s 959us/step - loss: 1.5702 - acc: 0.5556 - val_loss: 2.7932 - val_acc: 0.4059\n",
      "Epoch 60/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.9258 - acc: 0.4804 - val_loss: 5.0664 - val_acc: 0.4070\n",
      "Epoch 61/500\n",
      "22708/22708 [==============================] - 22s 962us/step - loss: 1.7711 - acc: 0.5083 - val_loss: 3.1237 - val_acc: 0.3927\n",
      "Epoch 62/500\n",
      "22708/22708 [==============================] - 22s 963us/step - loss: 1.6184 - acc: 0.5421 - val_loss: 2.7705 - val_acc: 0.4049\n",
      "Epoch 63/500\n",
      "22708/22708 [==============================] - 22s 958us/step - loss: 1.5695 - acc: 0.5526 - val_loss: 2.9867 - val_acc: 0.4106\n",
      "Epoch 64/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 1.5599 - acc: 0.5594 - val_loss: 2.9190 - val_acc: 0.3970\n",
      "Epoch 65/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.5479 - acc: 0.5583 - val_loss: 3.2875 - val_acc: 0.3482\n",
      "Epoch 66/500\n",
      "22708/22708 [==============================] - 22s 964us/step - loss: 1.5203 - acc: 0.5665 - val_loss: 3.2941 - val_acc: 0.4129\n",
      "Epoch 67/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.5830 - acc: 0.5508 - val_loss: 2.9041 - val_acc: 0.4162\n",
      "Epoch 68/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 1.5552 - acc: 0.5560 - val_loss: 2.8049 - val_acc: 0.4223\n",
      "Epoch 69/500\n",
      "22708/22708 [==============================] - 22s 964us/step - loss: 1.4895 - acc: 0.5757 - val_loss: 2.8562 - val_acc: 0.4129\n",
      "Epoch 70/500\n",
      "22708/22708 [==============================] - 22s 957us/step - loss: 1.5125 - acc: 0.5643 - val_loss: 2.8346 - val_acc: 0.4097\n",
      "Epoch 71/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.4591 - acc: 0.5768 - val_loss: 2.7435 - val_acc: 0.4093\n",
      "Epoch 72/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 1.4619 - acc: 0.5805 - val_loss: 2.9080 - val_acc: 0.3934\n",
      "Epoch 73/500\n",
      "22708/22708 [==============================] - 22s 959us/step - loss: 1.4628 - acc: 0.5768 - val_loss: 2.9236 - val_acc: 0.3969\n",
      "Epoch 74/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.6142 - acc: 0.5436 - val_loss: 2.8767 - val_acc: 0.4234\n",
      "Epoch 75/500\n",
      "22708/22708 [==============================] - 22s 958us/step - loss: 1.5045 - acc: 0.5710 - val_loss: 2.8513 - val_acc: 0.3956\n",
      "Epoch 76/500\n",
      "22708/22708 [==============================] - 22s 962us/step - loss: 1.4203 - acc: 0.5879 - val_loss: 2.8501 - val_acc: 0.4198\n",
      "Epoch 77/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.4066 - acc: 0.5923 - val_loss: 2.9618 - val_acc: 0.3840\n",
      "Epoch 78/500\n",
      "22708/22708 [==============================] - 22s 957us/step - loss: 1.4350 - acc: 0.5844 - val_loss: 2.8617 - val_acc: 0.4177\n",
      "Epoch 79/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 1.4164 - acc: 0.5879 - val_loss: 2.7835 - val_acc: 0.4195\n",
      "Epoch 80/500\n",
      "22708/22708 [==============================] - 22s 962us/step - loss: 1.3825 - acc: 0.5980 - val_loss: 3.3854 - val_acc: 0.3524\n",
      "Epoch 81/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 1.3853 - acc: 0.5979 - val_loss: 3.0963 - val_acc: 0.4193\n",
      "Epoch 82/500\n",
      "22708/22708 [==============================] - 22s 959us/step - loss: 1.7901 - acc: 0.5160 - val_loss: 3.9050 - val_acc: 0.4112\n",
      "Epoch 83/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 1.7063 - acc: 0.5265 - val_loss: 2.8698 - val_acc: 0.4136\n",
      "Epoch 84/500\n",
      "22708/22708 [==============================] - 22s 964us/step - loss: 1.4396 - acc: 0.5833 - val_loss: 3.0502 - val_acc: 0.4170\n",
      "Epoch 85/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.3822 - acc: 0.5955 - val_loss: 3.2024 - val_acc: 0.4247\n",
      "Epoch 86/500\n",
      "22708/22708 [==============================] - 22s 959us/step - loss: 1.3441 - acc: 0.6088 - val_loss: 3.0702 - val_acc: 0.3772\n",
      "Epoch 87/500\n",
      "22708/22708 [==============================] - 22s 959us/step - loss: 1.3494 - acc: 0.6066 - val_loss: 3.1173 - val_acc: 0.4042\n",
      "Epoch 88/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 1.3462 - acc: 0.6020 - val_loss: 2.9083 - val_acc: 0.4072\n",
      "Epoch 89/500\n",
      "22708/22708 [==============================] - 22s 958us/step - loss: 1.3706 - acc: 0.6012 - val_loss: 2.9342 - val_acc: 0.4118\n",
      "Epoch 90/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.3010 - acc: 0.6182 - val_loss: 3.2664 - val_acc: 0.4186\n",
      "Epoch 91/500\n",
      "22708/22708 [==============================] - 22s 965us/step - loss: 1.3170 - acc: 0.6123 - val_loss: 2.8913 - val_acc: 0.4081\n",
      "Epoch 92/500\n",
      "22708/22708 [==============================] - 22s 959us/step - loss: 1.5797 - acc: 0.5548 - val_loss: 2.7995 - val_acc: 0.4251\n",
      "Epoch 93/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.3138 - acc: 0.6135 - val_loss: 2.8720 - val_acc: 0.4126\n",
      "Epoch 94/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 1.2501 - acc: 0.6293 - val_loss: 3.0279 - val_acc: 0.3705\n",
      "Epoch 95/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 1.2445 - acc: 0.6343 - val_loss: 3.2538 - val_acc: 0.3562\n",
      "Epoch 96/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.2525 - acc: 0.6326 - val_loss: 3.2680 - val_acc: 0.3988\n",
      "Epoch 97/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.2319 - acc: 0.6380 - val_loss: 3.0944 - val_acc: 0.4112\n",
      "Epoch 98/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.2005 - acc: 0.6443 - val_loss: 3.0297 - val_acc: 0.3840\n",
      "Epoch 99/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 1.3791 - acc: 0.5973 - val_loss: 3.0725 - val_acc: 0.4175\n",
      "Epoch 100/500\n",
      "22708/22708 [==============================] - 22s 958us/step - loss: 1.3832 - acc: 0.5976 - val_loss: 2.7705 - val_acc: 0.4110\n",
      "Epoch 101/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 1.5814 - acc: 0.5557 - val_loss: 3.3526 - val_acc: 0.4206\n",
      "Epoch 102/500\n",
      "22708/22708 [==============================] - 22s 958us/step - loss: 1.3242 - acc: 0.6059 - val_loss: 2.9382 - val_acc: 0.4233\n",
      "Epoch 103/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 1.2025 - acc: 0.6410 - val_loss: 3.1088 - val_acc: 0.4060\n",
      "Epoch 104/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.2870 - acc: 0.6211 - val_loss: 3.0482 - val_acc: 0.4167\n",
      "Epoch 105/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.2299 - acc: 0.6357 - val_loss: 3.1475 - val_acc: 0.4098\n",
      "Epoch 106/500\n",
      "22708/22708 [==============================] - 22s 963us/step - loss: 1.1509 - acc: 0.6568 - val_loss: 3.4086 - val_acc: 0.3747\n",
      "Epoch 107/500\n",
      "22708/22708 [==============================] - 22s 959us/step - loss: 1.1197 - acc: 0.6641 - val_loss: 3.2701 - val_acc: 0.3366\n",
      "Epoch 108/500\n",
      "22708/22708 [==============================] - 22s 962us/step - loss: 1.1502 - acc: 0.6575 - val_loss: 3.1090 - val_acc: 0.4107\n",
      "Epoch 109/500\n",
      "22708/22708 [==============================] - 22s 963us/step - loss: 1.3237 - acc: 0.6120 - val_loss: 3.0102 - val_acc: 0.3961\n",
      "Epoch 110/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.1497 - acc: 0.6571 - val_loss: 3.2555 - val_acc: 0.4158\n",
      "Epoch 111/500\n",
      "22708/22708 [==============================] - 22s 962us/step - loss: 1.0751 - acc: 0.6774 - val_loss: 3.0736 - val_acc: 0.4039\n",
      "Epoch 112/500\n",
      "22708/22708 [==============================] - 22s 965us/step - loss: 1.0678 - acc: 0.6768 - val_loss: 3.1885 - val_acc: 0.4003\n",
      "Epoch 113/500\n",
      "22708/22708 [==============================] - 22s 963us/step - loss: 1.1913 - acc: 0.6432 - val_loss: 3.0776 - val_acc: 0.4010\n",
      "Epoch 114/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.2315 - acc: 0.6342 - val_loss: 3.2975 - val_acc: 0.4039\n",
      "Epoch 115/500\n",
      "22708/22708 [==============================] - 22s 967us/step - loss: 1.1876 - acc: 0.6469 - val_loss: 3.3397 - val_acc: 0.4101\n",
      "Epoch 116/500\n",
      "22708/22708 [==============================] - 22s 966us/step - loss: 1.1830 - acc: 0.6453 - val_loss: 3.1021 - val_acc: 0.4071\n",
      "Epoch 117/500\n",
      "22708/22708 [==============================] - 22s 965us/step - loss: 1.0127 - acc: 0.6922 - val_loss: 3.1041 - val_acc: 0.3964\n",
      "Epoch 118/500\n",
      "22708/22708 [==============================] - 22s 964us/step - loss: 0.9954 - acc: 0.7005 - val_loss: 3.3162 - val_acc: 0.4147\n",
      "Epoch 119/500\n",
      "22708/22708 [==============================] - 22s 968us/step - loss: 1.0534 - acc: 0.6815 - val_loss: 3.3455 - val_acc: 0.4100\n",
      "Epoch 120/500\n",
      "22708/22708 [==============================] - 22s 964us/step - loss: 1.1349 - acc: 0.6603 - val_loss: 3.2287 - val_acc: 0.3769\n",
      "Epoch 121/500\n",
      "22708/22708 [==============================] - 22s 964us/step - loss: 1.1676 - acc: 0.6495 - val_loss: 3.2560 - val_acc: 0.3925\n",
      "Epoch 122/500\n",
      "22708/22708 [==============================] - 22s 965us/step - loss: 1.0343 - acc: 0.6883 - val_loss: 3.0607 - val_acc: 0.4104\n",
      "Epoch 123/500\n",
      "22708/22708 [==============================] - 22s 966us/step - loss: 1.4051 - acc: 0.6026 - val_loss: 3.1765 - val_acc: 0.3984\n",
      "Epoch 124/500\n",
      "22708/22708 [==============================] - 22s 963us/step - loss: 1.0317 - acc: 0.6867 - val_loss: 3.1755 - val_acc: 0.3936\n",
      "Epoch 125/500\n",
      "22708/22708 [==============================] - 22s 966us/step - loss: 0.9388 - acc: 0.7154 - val_loss: 3.1193 - val_acc: 0.4026\n",
      "Epoch 126/500\n",
      "22708/22708 [==============================] - 22s 965us/step - loss: 0.9056 - acc: 0.7234 - val_loss: 3.3584 - val_acc: 0.4009\n",
      "Epoch 127/500\n",
      "22708/22708 [==============================] - 22s 962us/step - loss: 1.0053 - acc: 0.6971 - val_loss: 3.3908 - val_acc: 0.4082\n",
      "Epoch 128/500\n",
      "22708/22708 [==============================] - 22s 964us/step - loss: 0.9801 - acc: 0.7024 - val_loss: 3.3896 - val_acc: 0.3710\n",
      "Epoch 129/500\n",
      "22708/22708 [==============================] - 22s 962us/step - loss: 1.2181 - acc: 0.6401 - val_loss: 3.1511 - val_acc: 0.4001\n",
      "Epoch 130/500\n",
      "22708/22708 [==============================] - 22s 964us/step - loss: 0.9533 - acc: 0.7108 - val_loss: 3.3698 - val_acc: 0.3897\n",
      "Epoch 131/500\n",
      "22708/22708 [==============================] - 22s 966us/step - loss: 0.8800 - acc: 0.7312 - val_loss: 3.1519 - val_acc: 0.3954\n",
      "Epoch 132/500\n",
      "22708/22708 [==============================] - 22s 965us/step - loss: 0.8464 - acc: 0.7408 - val_loss: 3.6028 - val_acc: 0.4058\n",
      "Epoch 133/500\n",
      "22708/22708 [==============================] - 22s 968us/step - loss: 1.3373 - acc: 0.6186 - val_loss: 3.2354 - val_acc: 0.4065\n",
      "Epoch 134/500\n",
      "22708/22708 [==============================] - 22s 967us/step - loss: 0.9353 - acc: 0.7129 - val_loss: 3.5622 - val_acc: 0.4019\n",
      "Epoch 135/500\n",
      "22708/22708 [==============================] - 22s 962us/step - loss: 0.8710 - acc: 0.7328 - val_loss: 3.1520 - val_acc: 0.3957\n",
      "Epoch 136/500\n",
      "22708/22708 [==============================] - 22s 964us/step - loss: 1.5175 - acc: 0.5859 - val_loss: 3.2547 - val_acc: 0.4117\n",
      "Epoch 137/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.2937 - acc: 0.6227 - val_loss: 3.2106 - val_acc: 0.3875\n",
      "Epoch 138/500\n",
      "22708/22708 [==============================] - 22s 959us/step - loss: 0.9600 - acc: 0.7094 - val_loss: 3.2297 - val_acc: 0.4028\n",
      "Epoch 139/500\n",
      "22708/22708 [==============================] - 22s 962us/step - loss: 0.8028 - acc: 0.7549 - val_loss: 3.4345 - val_acc: 0.3945\n",
      "Epoch 140/500\n",
      "22708/22708 [==============================] - 22s 962us/step - loss: 0.7772 - acc: 0.7626 - val_loss: 3.4035 - val_acc: 0.3796\n",
      "Epoch 141/500\n",
      "22708/22708 [==============================] - 22s 963us/step - loss: 0.7661 - acc: 0.7649 - val_loss: 3.3380 - val_acc: 0.3817\n",
      "Epoch 142/500\n",
      "22708/22708 [==============================] - 22s 962us/step - loss: 1.3122 - acc: 0.6353 - val_loss: 3.6853 - val_acc: 0.3981\n",
      "Epoch 143/500\n",
      "22708/22708 [==============================] - 22s 962us/step - loss: 1.6076 - acc: 0.5542 - val_loss: 3.5111 - val_acc: 0.3597\n",
      "Epoch 144/500\n",
      "22708/22708 [==============================] - 22s 959us/step - loss: 1.0497 - acc: 0.6879 - val_loss: 3.2831 - val_acc: 0.4023\n",
      "Epoch 145/500\n",
      "22708/22708 [==============================] - 22s 962us/step - loss: 0.7756 - acc: 0.7648 - val_loss: 3.4364 - val_acc: 0.3991\n",
      "Epoch 146/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 0.7206 - acc: 0.7807 - val_loss: 3.3720 - val_acc: 0.3746\n",
      "Epoch 147/500\n",
      "22708/22708 [==============================] - 22s 964us/step - loss: 1.2570 - acc: 0.6443 - val_loss: 3.3463 - val_acc: 0.3770\n",
      "Epoch 148/500\n",
      "22708/22708 [==============================] - 22s 964us/step - loss: 0.9725 - acc: 0.7064 - val_loss: 3.2412 - val_acc: 0.3836\n",
      "Epoch 149/500\n",
      "22708/22708 [==============================] - 22s 958us/step - loss: 0.7259 - acc: 0.7791 - val_loss: 3.4712 - val_acc: 0.3980\n",
      "Epoch 150/500\n",
      "22708/22708 [==============================] - 22s 965us/step - loss: 0.6632 - acc: 0.8002 - val_loss: 3.4972 - val_acc: 0.3746\n",
      "Epoch 151/500\n",
      "22708/22708 [==============================] - 22s 965us/step - loss: 0.7160 - acc: 0.7836 - val_loss: 3.7390 - val_acc: 0.3957\n",
      "Epoch 152/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.1828 - acc: 0.6544 - val_loss: 3.4570 - val_acc: 0.3721\n",
      "Epoch 153/500\n",
      "22708/22708 [==============================] - 22s 964us/step - loss: 0.8993 - acc: 0.7234 - val_loss: 3.5291 - val_acc: 0.3849\n",
      "Epoch 154/500\n",
      "22708/22708 [==============================] - 22s 963us/step - loss: 1.0885 - acc: 0.6792 - val_loss: 3.4134 - val_acc: 0.4190\n",
      "Epoch 155/500\n",
      "22708/22708 [==============================] - 22s 962us/step - loss: 0.7092 - acc: 0.7841 - val_loss: 3.4046 - val_acc: 0.4082\n",
      "Epoch 156/500\n",
      "22708/22708 [==============================] - 22s 962us/step - loss: 0.6262 - acc: 0.8094 - val_loss: 3.6345 - val_acc: 0.4046\n",
      "Epoch 157/500\n",
      "22708/22708 [==============================] - 22s 962us/step - loss: 0.5969 - acc: 0.8209 - val_loss: 3.6100 - val_acc: 0.4057\n",
      "Epoch 158/500\n",
      "22708/22708 [==============================] - 22s 965us/step - loss: 0.5765 - acc: 0.8246 - val_loss: 3.5644 - val_acc: 0.3444\n",
      "Epoch 159/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 0.5809 - acc: 0.8230 - val_loss: 3.9580 - val_acc: 0.4037\n",
      "Epoch 160/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 1.1382 - acc: 0.6709 - val_loss: 4.4391 - val_acc: 0.3654\n",
      "Epoch 161/500\n",
      "22708/22708 [==============================] - 22s 963us/step - loss: 1.2004 - acc: 0.6521 - val_loss: 3.6449 - val_acc: 0.3573\n",
      "Epoch 162/500\n",
      "22708/22708 [==============================] - 22s 962us/step - loss: 1.2879 - acc: 0.6327 - val_loss: 3.3977 - val_acc: 0.4033\n",
      "Epoch 163/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 0.8294 - acc: 0.7472 - val_loss: 3.5161 - val_acc: 0.4071\n",
      "Epoch 164/500\n",
      "22708/22708 [==============================] - 22s 968us/step - loss: 0.5824 - acc: 0.8265 - val_loss: 3.5582 - val_acc: 0.3959\n",
      "Epoch 165/500\n",
      "22708/22708 [==============================] - 22s 965us/step - loss: 0.5297 - acc: 0.8426 - val_loss: 3.7115 - val_acc: 0.3948\n",
      "Epoch 166/500\n",
      "22708/22708 [==============================] - 22s 962us/step - loss: 0.4947 - acc: 0.8553 - val_loss: 3.6711 - val_acc: 0.3620\n",
      "Epoch 167/500\n",
      "22708/22708 [==============================] - 22s 963us/step - loss: 0.4937 - acc: 0.8529 - val_loss: 4.0593 - val_acc: 0.4003\n",
      "Epoch 168/500\n",
      "22708/22708 [==============================] - 22s 966us/step - loss: 0.4818 - acc: 0.8550 - val_loss: 3.8895 - val_acc: 0.4017\n",
      "Epoch 169/500\n",
      "22708/22708 [==============================] - 22s 962us/step - loss: 0.4468 - acc: 0.8660 - val_loss: 3.9738 - val_acc: 0.4093\n",
      "Epoch 170/500\n",
      "22708/22708 [==============================] - 22s 963us/step - loss: 0.4372 - acc: 0.8705 - val_loss: 3.8361 - val_acc: 0.3901\n",
      "Epoch 171/500\n",
      "22708/22708 [==============================] - 22s 963us/step - loss: 0.4457 - acc: 0.8663 - val_loss: 3.8663 - val_acc: 0.3583\n",
      "Epoch 172/500\n",
      "22708/22708 [==============================] - 22s 962us/step - loss: 1.8280 - acc: 0.5577 - val_loss: 8.0068 - val_acc: 0.4060\n",
      "Epoch 173/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 2.2215 - acc: 0.4365 - val_loss: 6.3923 - val_acc: 0.3933\n",
      "Epoch 174/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 2.2159 - acc: 0.4329 - val_loss: 6.5718 - val_acc: 0.4068\n",
      "Epoch 175/500\n",
      "22708/22708 [==============================] - 22s 962us/step - loss: 2.1190 - acc: 0.4463 - val_loss: 4.3977 - val_acc: 0.4183\n",
      "Epoch 176/500\n",
      "22708/22708 [==============================] - 22s 963us/step - loss: 1.6786 - acc: 0.5300 - val_loss: 3.7465 - val_acc: 0.4182\n",
      "Epoch 177/500\n",
      "22708/22708 [==============================] - 22s 962us/step - loss: 1.2664 - acc: 0.6252 - val_loss: 3.1732 - val_acc: 0.3916\n",
      "Epoch 178/500\n",
      "22708/22708 [==============================] - 22s 963us/step - loss: 0.7868 - acc: 0.7554 - val_loss: 3.5038 - val_acc: 0.3875\n",
      "Epoch 179/500\n",
      "22708/22708 [==============================] - 22s 958us/step - loss: 0.9198 - acc: 0.7216 - val_loss: 3.4431 - val_acc: 0.3970\n",
      "Epoch 180/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 0.6063 - acc: 0.8155 - val_loss: 3.5965 - val_acc: 0.3400\n",
      "Epoch 181/500\n",
      "22708/22708 [==============================] - 22s 958us/step - loss: 0.4862 - acc: 0.8572 - val_loss: 3.7363 - val_acc: 0.4074\n",
      "Epoch 182/500\n",
      "22708/22708 [==============================] - 22s 962us/step - loss: 0.4400 - acc: 0.8721 - val_loss: 3.6581 - val_acc: 0.3652\n",
      "Epoch 183/500\n",
      "22708/22708 [==============================] - 22s 963us/step - loss: 0.4100 - acc: 0.8837 - val_loss: 3.8733 - val_acc: 0.3833\n",
      "Epoch 184/500\n",
      "22708/22708 [==============================] - 22s 959us/step - loss: 0.4145 - acc: 0.8812 - val_loss: 3.7318 - val_acc: 0.3716\n",
      "Epoch 185/500\n",
      "22708/22708 [==============================] - 22s 963us/step - loss: 0.3923 - acc: 0.8837 - val_loss: 3.8977 - val_acc: 0.3886\n",
      "Epoch 186/500\n",
      "22708/22708 [==============================] - 22s 964us/step - loss: 0.3666 - acc: 0.8955 - val_loss: 3.9765 - val_acc: 0.3683\n",
      "Epoch 187/500\n",
      "22708/22708 [==============================] - 22s 966us/step - loss: 0.3566 - acc: 0.8979 - val_loss: 4.0656 - val_acc: 0.3662\n",
      "Epoch 188/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.2941 - acc: 0.6913 - val_loss: 6.6730 - val_acc: 0.1556\n",
      "Epoch 189/500\n",
      "22708/22708 [==============================] - 22s 963us/step - loss: 2.4535 - acc: 0.4036 - val_loss: 7.5186 - val_acc: 0.1471\n",
      "Epoch 190/500\n",
      "22708/22708 [==============================] - 22s 964us/step - loss: 2.0808 - acc: 0.4534 - val_loss: 5.5839 - val_acc: 0.2144\n",
      "Epoch 191/500\n",
      "22708/22708 [==============================] - 22s 959us/step - loss: 2.6992 - acc: 0.3985 - val_loss: 8.0249 - val_acc: 0.4065\n",
      "Epoch 192/500\n",
      "22708/22708 [==============================] - 22s 962us/step - loss: 2.6050 - acc: 0.4026 - val_loss: 6.0938 - val_acc: 0.4066\n",
      "Epoch 193/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 2.5700 - acc: 0.4025 - val_loss: 5.0849 - val_acc: 0.4066\n",
      "Epoch 194/500\n",
      "22708/22708 [==============================] - 22s 959us/step - loss: 2.5713 - acc: 0.4029 - val_loss: 6.9024 - val_acc: 0.4072\n",
      "Epoch 195/500\n",
      "22708/22708 [==============================] - 22s 957us/step - loss: 2.5647 - acc: 0.4036 - val_loss: 5.4026 - val_acc: 0.4075\n",
      "Epoch 196/500\n",
      "22708/22708 [==============================] - 22s 958us/step - loss: 2.5273 - acc: 0.4046 - val_loss: 3.7728 - val_acc: 0.4068\n",
      "Epoch 197/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 2.5128 - acc: 0.4035 - val_loss: 3.6056 - val_acc: 0.4079\n",
      "Epoch 198/500\n",
      "22708/22708 [==============================] - 22s 954us/step - loss: 2.4951 - acc: 0.4055 - val_loss: 3.2651 - val_acc: 0.4069\n",
      "Epoch 199/500\n",
      "22708/22708 [==============================] - 22s 955us/step - loss: 2.4727 - acc: 0.4056 - val_loss: 3.5325 - val_acc: 0.4071\n",
      "Epoch 200/500\n",
      "22708/22708 [==============================] - 22s 956us/step - loss: 2.4605 - acc: 0.4043 - val_loss: 5.9255 - val_acc: 0.4057\n",
      "Epoch 201/500\n",
      "22708/22708 [==============================] - 22s 956us/step - loss: 2.4429 - acc: 0.4048 - val_loss: 3.0629 - val_acc: 0.4068\n",
      "Epoch 202/500\n",
      "22708/22708 [==============================] - 22s 955us/step - loss: 2.4451 - acc: 0.4069 - val_loss: 3.2987 - val_acc: 0.3962\n",
      "Epoch 203/500\n",
      "22708/22708 [==============================] - 22s 952us/step - loss: 2.4591 - acc: 0.4073 - val_loss: 4.2410 - val_acc: 0.2101\n",
      "Epoch 204/500\n",
      "22708/22708 [==============================] - 22s 959us/step - loss: 2.4316 - acc: 0.4060 - val_loss: 3.5276 - val_acc: 0.4066\n",
      "Epoch 205/500\n",
      "22708/22708 [==============================] - 22s 953us/step - loss: 2.3938 - acc: 0.4053 - val_loss: 3.5041 - val_acc: 0.4042\n",
      "Epoch 206/500\n",
      "22708/22708 [==============================] - 22s 959us/step - loss: 2.3805 - acc: 0.4080 - val_loss: 7.4692 - val_acc: 0.4066\n",
      "Epoch 207/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 2.3590 - acc: 0.4098 - val_loss: 3.3064 - val_acc: 0.4035\n",
      "Epoch 208/500\n",
      "22708/22708 [==============================] - 22s 957us/step - loss: 2.3313 - acc: 0.4105 - val_loss: 3.9246 - val_acc: 0.4066\n",
      "Epoch 209/500\n",
      "22708/22708 [==============================] - 22s 967us/step - loss: 2.3112 - acc: 0.4137 - val_loss: 3.5314 - val_acc: 0.3725\n",
      "Epoch 210/500\n",
      "22708/22708 [==============================] - 22s 963us/step - loss: 2.3006 - acc: 0.4130 - val_loss: 6.5788 - val_acc: 0.4075\n",
      "Epoch 211/500\n",
      "22708/22708 [==============================] - 22s 959us/step - loss: 2.2661 - acc: 0.4195 - val_loss: 4.0579 - val_acc: 0.4017\n",
      "Epoch 212/500\n",
      "22708/22708 [==============================] - 22s 959us/step - loss: 2.2840 - acc: 0.4148 - val_loss: 7.7244 - val_acc: 0.4071\n",
      "Epoch 213/500\n",
      "22708/22708 [==============================] - 22s 957us/step - loss: 2.2503 - acc: 0.4202 - val_loss: 4.2823 - val_acc: 0.4078\n",
      "Epoch 214/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 2.2236 - acc: 0.4234 - val_loss: 5.6551 - val_acc: 0.4066\n",
      "Epoch 215/500\n",
      "22708/22708 [==============================] - 22s 955us/step - loss: 2.2197 - acc: 0.4216 - val_loss: 4.7033 - val_acc: 0.1193\n",
      "Epoch 216/500\n",
      "22708/22708 [==============================] - 22s 956us/step - loss: 2.2219 - acc: 0.4225 - val_loss: 6.3127 - val_acc: 0.4065\n",
      "Epoch 217/500\n",
      "22708/22708 [==============================] - 22s 956us/step - loss: 2.1866 - acc: 0.4284 - val_loss: 5.6832 - val_acc: 0.4064\n",
      "Epoch 218/500\n",
      "22708/22708 [==============================] - 22s 958us/step - loss: 2.1668 - acc: 0.4284 - val_loss: 4.8268 - val_acc: 0.4063\n",
      "Epoch 219/500\n",
      "22708/22708 [==============================] - 22s 958us/step - loss: 2.1858 - acc: 0.4293 - val_loss: 4.9095 - val_acc: 0.4010\n",
      "Epoch 220/500\n",
      "22708/22708 [==============================] - 22s 962us/step - loss: 2.1494 - acc: 0.4350 - val_loss: 7.2279 - val_acc: 0.4054\n",
      "Epoch 221/500\n",
      "22708/22708 [==============================] - 22s 959us/step - loss: 2.1483 - acc: 0.4336 - val_loss: 3.7293 - val_acc: 0.3962\n",
      "Epoch 222/500\n",
      "22708/22708 [==============================] - 22s 959us/step - loss: 2.1280 - acc: 0.4371 - val_loss: 4.0413 - val_acc: 0.2033\n",
      "Epoch 223/500\n",
      "22708/22708 [==============================] - 22s 958us/step - loss: 2.1152 - acc: 0.4391 - val_loss: 4.7456 - val_acc: 0.3943\n",
      "Epoch 224/500\n",
      "22708/22708 [==============================] - 22s 955us/step - loss: 2.1041 - acc: 0.4388 - val_loss: 4.7893 - val_acc: 0.2109\n",
      "Epoch 225/500\n",
      "22708/22708 [==============================] - 22s 958us/step - loss: 2.1006 - acc: 0.4424 - val_loss: 3.8567 - val_acc: 0.3422\n",
      "Epoch 226/500\n",
      "22708/22708 [==============================] - 22s 952us/step - loss: 2.0842 - acc: 0.4443 - val_loss: 4.1224 - val_acc: 0.3052\n",
      "Epoch 227/500\n",
      "22708/22708 [==============================] - 22s 958us/step - loss: 2.0799 - acc: 0.4451 - val_loss: 5.3081 - val_acc: 0.3893\n",
      "Epoch 228/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 2.0713 - acc: 0.4457 - val_loss: 3.9717 - val_acc: 0.4065\n",
      "Epoch 229/500\n",
      "22708/22708 [==============================] - 22s 959us/step - loss: 2.0665 - acc: 0.4477 - val_loss: 3.7849 - val_acc: 0.1928\n",
      "Epoch 230/500\n",
      "22708/22708 [==============================] - 22s 957us/step - loss: 2.0521 - acc: 0.4492 - val_loss: 3.4587 - val_acc: 0.3832\n",
      "Epoch 231/500\n",
      "22708/22708 [==============================] - 22s 954us/step - loss: 2.0408 - acc: 0.4535 - val_loss: 5.0376 - val_acc: 0.4031\n",
      "Epoch 232/500\n",
      "22708/22708 [==============================] - 22s 957us/step - loss: 2.0250 - acc: 0.4506 - val_loss: 5.5634 - val_acc: 0.4045\n",
      "Epoch 233/500\n",
      "22708/22708 [==============================] - 22s 958us/step - loss: 2.0192 - acc: 0.4554 - val_loss: 6.6015 - val_acc: 0.4052\n",
      "Epoch 234/500\n",
      "22708/22708 [==============================] - 22s 957us/step - loss: 2.0182 - acc: 0.4570 - val_loss: 4.4866 - val_acc: 0.3745\n",
      "Epoch 235/500\n",
      "22708/22708 [==============================] - 22s 955us/step - loss: 2.0173 - acc: 0.4545 - val_loss: 4.3782 - val_acc: 0.3883\n",
      "Epoch 236/500\n",
      "22708/22708 [==============================] - 22s 959us/step - loss: 1.9975 - acc: 0.4597 - val_loss: 4.7208 - val_acc: 0.4026\n",
      "Epoch 237/500\n",
      "22708/22708 [==============================] - 22s 955us/step - loss: 1.9978 - acc: 0.4595 - val_loss: 4.3507 - val_acc: 0.4027\n",
      "Epoch 238/500\n",
      "22708/22708 [==============================] - 22s 958us/step - loss: 1.9752 - acc: 0.4609 - val_loss: 4.2052 - val_acc: 0.3916\n",
      "Epoch 239/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 1.9691 - acc: 0.4652 - val_loss: 4.9281 - val_acc: 0.3817\n",
      "Epoch 240/500\n",
      "22708/22708 [==============================] - 22s 959us/step - loss: 1.9656 - acc: 0.4634 - val_loss: 4.3977 - val_acc: 0.3608\n",
      "Epoch 241/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.9561 - acc: 0.4648 - val_loss: 5.0492 - val_acc: 0.3852\n",
      "Epoch 242/500\n",
      "22708/22708 [==============================] - 22s 959us/step - loss: 1.9488 - acc: 0.4656 - val_loss: 3.6700 - val_acc: 0.3824\n",
      "Epoch 243/500\n",
      "22708/22708 [==============================] - 22s 963us/step - loss: 1.9392 - acc: 0.4682 - val_loss: 3.6636 - val_acc: 0.3820\n",
      "Epoch 244/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.9334 - acc: 0.4715 - val_loss: 3.8805 - val_acc: 0.3482\n",
      "Epoch 245/500\n",
      "22708/22708 [==============================] - 22s 962us/step - loss: 1.9344 - acc: 0.4678 - val_loss: 5.4575 - val_acc: 0.4034\n",
      "Epoch 246/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.9288 - acc: 0.4711 - val_loss: 3.6406 - val_acc: 0.3790\n",
      "Epoch 247/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.9040 - acc: 0.4751 - val_loss: 4.0815 - val_acc: 0.3952\n",
      "Epoch 248/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.9068 - acc: 0.4749 - val_loss: 5.1242 - val_acc: 0.1791\n",
      "Epoch 249/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.9094 - acc: 0.4752 - val_loss: 3.5770 - val_acc: 0.3928\n",
      "Epoch 250/500\n",
      "22708/22708 [==============================] - 22s 958us/step - loss: 1.8880 - acc: 0.4768 - val_loss: 4.4516 - val_acc: 0.3854\n",
      "Epoch 251/500\n",
      "22708/22708 [==============================] - 22s 963us/step - loss: 1.8772 - acc: 0.4816 - val_loss: 5.0959 - val_acc: 0.4012\n",
      "Epoch 252/500\n",
      "22708/22708 [==============================] - 22s 965us/step - loss: 1.8996 - acc: 0.4792 - val_loss: 5.5935 - val_acc: 0.3376\n",
      "Epoch 253/500\n",
      "22708/22708 [==============================] - 22s 957us/step - loss: 1.9504 - acc: 0.4659 - val_loss: 5.5964 - val_acc: 0.3769\n",
      "Epoch 254/500\n",
      "22708/22708 [==============================] - 22s 957us/step - loss: 1.9039 - acc: 0.4775 - val_loss: 3.5743 - val_acc: 0.3854\n",
      "Epoch 255/500\n",
      "22708/22708 [==============================] - 22s 955us/step - loss: 1.8717 - acc: 0.4814 - val_loss: 3.8147 - val_acc: 0.4057\n",
      "Epoch 256/500\n",
      "22708/22708 [==============================] - 22s 958us/step - loss: 1.8649 - acc: 0.4832 - val_loss: 3.5877 - val_acc: 0.3851\n",
      "Epoch 257/500\n",
      "22708/22708 [==============================] - 22s 958us/step - loss: 1.8576 - acc: 0.4877 - val_loss: 4.6757 - val_acc: 0.3688\n",
      "Epoch 258/500\n",
      "22708/22708 [==============================] - 22s 957us/step - loss: 1.8467 - acc: 0.4882 - val_loss: 3.9757 - val_acc: 0.1796\n",
      "Epoch 259/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.8446 - acc: 0.4862 - val_loss: 3.6290 - val_acc: 0.3839\n",
      "Epoch 260/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 1.8355 - acc: 0.4913 - val_loss: 3.7449 - val_acc: 0.3959\n",
      "Epoch 261/500\n",
      "22708/22708 [==============================] - 22s 963us/step - loss: 1.8288 - acc: 0.4910 - val_loss: 4.4770 - val_acc: 0.3738\n",
      "Epoch 262/500\n",
      "22708/22708 [==============================] - 22s 962us/step - loss: 1.8148 - acc: 0.4940 - val_loss: 4.6733 - val_acc: 0.3128\n",
      "Epoch 263/500\n",
      "22708/22708 [==============================] - 22s 955us/step - loss: 1.8134 - acc: 0.4983 - val_loss: 4.2186 - val_acc: 0.3453\n",
      "Epoch 264/500\n",
      "22708/22708 [==============================] - 22s 956us/step - loss: 1.8441 - acc: 0.4848 - val_loss: 3.9150 - val_acc: 0.2868\n",
      "Epoch 265/500\n",
      "22708/22708 [==============================] - 22s 957us/step - loss: 1.7981 - acc: 0.4978 - val_loss: 4.3805 - val_acc: 0.3256\n",
      "Epoch 266/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 1.8083 - acc: 0.4925 - val_loss: 3.5341 - val_acc: 0.3287\n",
      "Epoch 267/500\n",
      "22708/22708 [==============================] - 22s 956us/step - loss: 1.7873 - acc: 0.4959 - val_loss: 4.3588 - val_acc: 0.3648\n",
      "Epoch 268/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.7734 - acc: 0.5015 - val_loss: 3.9647 - val_acc: 0.3854\n",
      "Epoch 269/500\n",
      "22708/22708 [==============================] - 22s 955us/step - loss: 1.7644 - acc: 0.5017 - val_loss: 4.2040 - val_acc: 0.2523\n",
      "Epoch 270/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 1.7898 - acc: 0.5012 - val_loss: 3.9001 - val_acc: 0.3732\n",
      "Epoch 271/500\n",
      "22708/22708 [==============================] - 22s 957us/step - loss: 1.7569 - acc: 0.5072 - val_loss: 3.7799 - val_acc: 0.3708\n",
      "Epoch 272/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 1.7508 - acc: 0.5058 - val_loss: 4.4061 - val_acc: 0.3337\n",
      "Epoch 273/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 1.7359 - acc: 0.5137 - val_loss: 3.7443 - val_acc: 0.3795\n",
      "Epoch 274/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.7522 - acc: 0.5079 - val_loss: 3.7294 - val_acc: 0.3690\n",
      "Epoch 275/500\n",
      "22708/22708 [==============================] - 22s 962us/step - loss: 1.7225 - acc: 0.5131 - val_loss: 4.5817 - val_acc: 0.3540\n",
      "Epoch 276/500\n",
      "22708/22708 [==============================] - 22s 962us/step - loss: 1.7396 - acc: 0.5105 - val_loss: 3.7631 - val_acc: 0.3670\n",
      "Epoch 277/500\n",
      "22708/22708 [==============================] - 22s 958us/step - loss: 1.7101 - acc: 0.5145 - val_loss: 3.6004 - val_acc: 0.3830\n",
      "Epoch 278/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.7180 - acc: 0.5148 - val_loss: 5.4053 - val_acc: 0.3463\n",
      "Epoch 279/500\n",
      "22708/22708 [==============================] - 22s 956us/step - loss: 1.7305 - acc: 0.5092 - val_loss: 5.7340 - val_acc: 0.2425\n",
      "Epoch 280/500\n",
      "22708/22708 [==============================] - 22s 962us/step - loss: 1.7024 - acc: 0.5170 - val_loss: 3.9771 - val_acc: 0.3322\n",
      "Epoch 281/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.6816 - acc: 0.5214 - val_loss: 4.2810 - val_acc: 0.3902\n",
      "Epoch 282/500\n",
      "22708/22708 [==============================] - 22s 956us/step - loss: 1.6913 - acc: 0.5213 - val_loss: 3.6384 - val_acc: 0.2926\n",
      "Epoch 283/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 1.6798 - acc: 0.5221 - val_loss: 3.8943 - val_acc: 0.2318\n",
      "Epoch 284/500\n",
      "22708/22708 [==============================] - 22s 957us/step - loss: 1.6838 - acc: 0.5197 - val_loss: 3.8204 - val_acc: 0.3728\n",
      "Epoch 285/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 1.6647 - acc: 0.5246 - val_loss: 4.0417 - val_acc: 0.2469\n",
      "Epoch 286/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 1.6532 - acc: 0.5284 - val_loss: 4.4479 - val_acc: 0.2237\n",
      "Epoch 287/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 1.6599 - acc: 0.5246 - val_loss: 3.2152 - val_acc: 0.3725\n",
      "Epoch 288/500\n",
      "22708/22708 [==============================] - 22s 962us/step - loss: 1.6408 - acc: 0.5295 - val_loss: 4.0371 - val_acc: 0.3630\n",
      "Epoch 289/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 1.6502 - acc: 0.5286 - val_loss: 3.1597 - val_acc: 0.4028\n",
      "Epoch 290/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 1.6248 - acc: 0.5343 - val_loss: 4.3913 - val_acc: 0.4063\n",
      "Epoch 291/500\n",
      "22708/22708 [==============================] - 22s 962us/step - loss: 1.6191 - acc: 0.5330 - val_loss: 4.0521 - val_acc: 0.3898\n",
      "Epoch 292/500\n",
      "22708/22708 [==============================] - 22s 962us/step - loss: 1.6231 - acc: 0.5351 - val_loss: 3.8395 - val_acc: 0.3665\n",
      "Epoch 293/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 1.6093 - acc: 0.5360 - val_loss: 4.6018 - val_acc: 0.3862\n",
      "Epoch 294/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.6062 - acc: 0.5367 - val_loss: 4.2956 - val_acc: 0.3988\n",
      "Epoch 295/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.5846 - acc: 0.5428 - val_loss: 3.8388 - val_acc: 0.3772\n",
      "Epoch 296/500\n",
      "22708/22708 [==============================] - 22s 963us/step - loss: 1.5845 - acc: 0.5417 - val_loss: 5.2751 - val_acc: 0.2216\n",
      "Epoch 297/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 1.5950 - acc: 0.5437 - val_loss: 3.7368 - val_acc: 0.3548\n",
      "Epoch 298/500\n",
      "22708/22708 [==============================] - 22s 959us/step - loss: 1.5721 - acc: 0.5466 - val_loss: 3.5169 - val_acc: 0.4071\n",
      "Epoch 299/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.5732 - acc: 0.5490 - val_loss: 4.1660 - val_acc: 0.2862\n",
      "Epoch 300/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.5464 - acc: 0.5498 - val_loss: 4.6846 - val_acc: 0.2066\n",
      "Epoch 301/500\n",
      "22708/22708 [==============================] - 22s 958us/step - loss: 1.5369 - acc: 0.5548 - val_loss: 5.8360 - val_acc: 0.3426\n",
      "Epoch 302/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 1.5684 - acc: 0.5469 - val_loss: 4.2399 - val_acc: 0.3312\n",
      "Epoch 303/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.5219 - acc: 0.5546 - val_loss: 3.3116 - val_acc: 0.3869\n",
      "Epoch 304/500\n",
      "22708/22708 [==============================] - 22s 966us/step - loss: 1.5204 - acc: 0.5555 - val_loss: 3.8415 - val_acc: 0.3914\n",
      "Epoch 305/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 1.4912 - acc: 0.5636 - val_loss: 4.6123 - val_acc: 0.2830\n",
      "Epoch 306/500\n",
      "22708/22708 [==============================] - 22s 962us/step - loss: 1.5242 - acc: 0.5561 - val_loss: 4.5141 - val_acc: 0.4063\n",
      "Epoch 307/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.5048 - acc: 0.5619 - val_loss: 3.7806 - val_acc: 0.3329\n",
      "Epoch 308/500\n",
      "22708/22708 [==============================] - 22s 962us/step - loss: 1.4820 - acc: 0.5688 - val_loss: 3.9674 - val_acc: 0.3572\n",
      "Epoch 309/500\n",
      "22708/22708 [==============================] - 22s 962us/step - loss: 1.4944 - acc: 0.5634 - val_loss: 3.5666 - val_acc: 0.3856\n",
      "Epoch 310/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 1.4764 - acc: 0.5659 - val_loss: 3.6854 - val_acc: 0.4016\n",
      "Epoch 311/500\n",
      "22708/22708 [==============================] - 22s 962us/step - loss: 1.4502 - acc: 0.5727 - val_loss: 3.2944 - val_acc: 0.3782\n",
      "Epoch 312/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.4426 - acc: 0.5732 - val_loss: 4.0223 - val_acc: 0.4047\n",
      "Epoch 313/500\n",
      "22708/22708 [==============================] - 22s 963us/step - loss: 1.4222 - acc: 0.5827 - val_loss: 3.9421 - val_acc: 0.3803\n",
      "Epoch 314/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.4343 - acc: 0.5791 - val_loss: 4.1495 - val_acc: 0.2855\n",
      "Epoch 315/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 1.4299 - acc: 0.5791 - val_loss: 3.8908 - val_acc: 0.3528\n",
      "Epoch 316/500\n",
      "22708/22708 [==============================] - 22s 965us/step - loss: 1.4187 - acc: 0.5814 - val_loss: 3.6920 - val_acc: 0.4029\n",
      "Epoch 317/500\n",
      "22708/22708 [==============================] - 22s 962us/step - loss: 1.4471 - acc: 0.5727 - val_loss: 3.6868 - val_acc: 0.3501\n",
      "Epoch 318/500\n",
      "22708/22708 [==============================] - 22s 962us/step - loss: 1.3982 - acc: 0.5879 - val_loss: 4.4791 - val_acc: 0.3432\n",
      "Epoch 319/500\n",
      "22708/22708 [==============================] - 22s 963us/step - loss: 1.3847 - acc: 0.5902 - val_loss: 3.6109 - val_acc: 0.3981\n",
      "Epoch 320/500\n",
      "22708/22708 [==============================] - 22s 963us/step - loss: 1.3774 - acc: 0.5910 - val_loss: 4.5100 - val_acc: 0.3695\n",
      "Epoch 321/500\n",
      "22708/22708 [==============================] - 22s 965us/step - loss: 1.3930 - acc: 0.5896 - val_loss: 4.3144 - val_acc: 0.3855\n",
      "Epoch 322/500\n",
      "22708/22708 [==============================] - 22s 968us/step - loss: 1.3391 - acc: 0.6046 - val_loss: 4.2356 - val_acc: 0.3622\n",
      "Epoch 323/500\n",
      "22708/22708 [==============================] - 22s 966us/step - loss: 1.3337 - acc: 0.6012 - val_loss: 3.3986 - val_acc: 0.3923\n",
      "Epoch 324/500\n",
      "22708/22708 [==============================] - 22s 966us/step - loss: 1.3459 - acc: 0.5985 - val_loss: 4.3160 - val_acc: 0.3880\n",
      "Epoch 325/500\n",
      "22708/22708 [==============================] - 22s 965us/step - loss: 1.3160 - acc: 0.6104 - val_loss: 3.7718 - val_acc: 0.3894\n",
      "Epoch 326/500\n",
      "22708/22708 [==============================] - 22s 964us/step - loss: 1.2929 - acc: 0.6145 - val_loss: 4.0642 - val_acc: 0.3158\n",
      "Epoch 327/500\n",
      "22708/22708 [==============================] - 22s 965us/step - loss: 1.3511 - acc: 0.5975 - val_loss: 3.3874 - val_acc: 0.3955\n",
      "Epoch 328/500\n",
      "22708/22708 [==============================] - 22s 964us/step - loss: 1.2541 - acc: 0.6228 - val_loss: 3.6706 - val_acc: 0.3568\n",
      "Epoch 329/500\n",
      "22708/22708 [==============================] - 22s 964us/step - loss: 1.2594 - acc: 0.6198 - val_loss: 3.5741 - val_acc: 0.3916\n",
      "Epoch 330/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.3387 - acc: 0.6058 - val_loss: 3.4216 - val_acc: 0.3940\n",
      "Epoch 331/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.2259 - acc: 0.6316 - val_loss: 3.9455 - val_acc: 0.3648\n",
      "Epoch 332/500\n",
      "22708/22708 [==============================] - 22s 964us/step - loss: 1.3026 - acc: 0.6126 - val_loss: 3.7866 - val_acc: 0.3901\n",
      "Epoch 333/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.1945 - acc: 0.6388 - val_loss: 3.5036 - val_acc: 0.3690\n",
      "Epoch 334/500\n",
      "22708/22708 [==============================] - 22s 963us/step - loss: 1.2485 - acc: 0.6239 - val_loss: 3.7946 - val_acc: 0.3221\n",
      "Epoch 335/500\n",
      "22708/22708 [==============================] - 22s 962us/step - loss: 1.1810 - acc: 0.6411 - val_loss: 3.3035 - val_acc: 0.3973\n",
      "Epoch 336/500\n",
      "22708/22708 [==============================] - 22s 958us/step - loss: 1.3185 - acc: 0.6077 - val_loss: 4.0982 - val_acc: 0.3943\n",
      "Epoch 337/500\n",
      "22708/22708 [==============================] - 22s 963us/step - loss: 1.1487 - acc: 0.6489 - val_loss: 3.9723 - val_acc: 0.3260\n",
      "Epoch 338/500\n",
      "22708/22708 [==============================] - 22s 964us/step - loss: 1.1735 - acc: 0.6423 - val_loss: 4.0899 - val_acc: 0.3975\n",
      "Epoch 339/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.1617 - acc: 0.6482 - val_loss: 3.7271 - val_acc: 0.3775\n",
      "Epoch 340/500\n",
      "22708/22708 [==============================] - 22s 963us/step - loss: 1.1882 - acc: 0.6376 - val_loss: 3.7018 - val_acc: 0.4165\n",
      "Epoch 341/500\n",
      "22708/22708 [==============================] - 22s 957us/step - loss: 2.2045 - acc: 0.4748 - val_loss: 7.2184 - val_acc: 0.3850\n",
      "Epoch 342/500\n",
      "22708/22708 [==============================] - 22s 953us/step - loss: 2.5963 - acc: 0.4036 - val_loss: 6.5153 - val_acc: 0.4057\n",
      "Epoch 343/500\n",
      "22708/22708 [==============================] - 22s 956us/step - loss: 2.5698 - acc: 0.4043 - val_loss: 6.1872 - val_acc: 0.4059\n",
      "Epoch 344/500\n",
      "22708/22708 [==============================] - 22s 955us/step - loss: 2.5328 - acc: 0.4049 - val_loss: 7.0936 - val_acc: 0.4071\n",
      "Epoch 345/500\n",
      "22708/22708 [==============================] - 22s 957us/step - loss: 2.5246 - acc: 0.4048 - val_loss: 4.6554 - val_acc: 0.4057\n",
      "Epoch 346/500\n",
      "22708/22708 [==============================] - 22s 958us/step - loss: 2.4948 - acc: 0.4065 - val_loss: 4.0553 - val_acc: 0.3941\n",
      "Epoch 347/500\n",
      "22708/22708 [==============================] - 22s 953us/step - loss: 2.4823 - acc: 0.4071 - val_loss: 3.3963 - val_acc: 0.4074\n",
      "Epoch 348/500\n",
      "22708/22708 [==============================] - 22s 956us/step - loss: 2.4697 - acc: 0.4070 - val_loss: 3.3228 - val_acc: 0.4044\n",
      "Epoch 349/500\n",
      "22708/22708 [==============================] - 22s 954us/step - loss: 2.4518 - acc: 0.4077 - val_loss: 3.6982 - val_acc: 0.4064\n",
      "Epoch 350/500\n",
      "22708/22708 [==============================] - 22s 957us/step - loss: 2.4528 - acc: 0.4074 - val_loss: 4.3828 - val_acc: 0.4064\n",
      "Epoch 351/500\n",
      "22708/22708 [==============================] - 22s 952us/step - loss: 2.4544 - acc: 0.4078 - val_loss: 3.6347 - val_acc: 0.3826\n",
      "Epoch 352/500\n",
      "22708/22708 [==============================] - 22s 955us/step - loss: 2.4284 - acc: 0.4086 - val_loss: 3.3509 - val_acc: 0.4078\n",
      "Epoch 353/500\n",
      "22708/22708 [==============================] - 22s 954us/step - loss: 2.4294 - acc: 0.4093 - val_loss: 3.7018 - val_acc: 0.3956\n",
      "Epoch 354/500\n",
      "22708/22708 [==============================] - 22s 955us/step - loss: 2.4197 - acc: 0.4092 - val_loss: 3.8328 - val_acc: 0.4073\n",
      "Epoch 355/500\n",
      "22708/22708 [==============================] - 22s 953us/step - loss: 2.4000 - acc: 0.4094 - val_loss: 3.8461 - val_acc: 0.4076\n",
      "Epoch 356/500\n",
      "22708/22708 [==============================] - 22s 954us/step - loss: 2.3908 - acc: 0.4101 - val_loss: 4.2060 - val_acc: 0.4067\n",
      "Epoch 357/500\n",
      "22708/22708 [==============================] - 22s 952us/step - loss: 2.3694 - acc: 0.4120 - val_loss: 3.3320 - val_acc: 0.4030\n",
      "Epoch 358/500\n",
      "22708/22708 [==============================] - 22s 955us/step - loss: 2.3687 - acc: 0.4110 - val_loss: 3.1036 - val_acc: 0.4059\n",
      "Epoch 359/500\n",
      "22708/22708 [==============================] - 22s 955us/step - loss: 2.3445 - acc: 0.4125 - val_loss: 2.9750 - val_acc: 0.4082\n",
      "Epoch 360/500\n",
      "22708/22708 [==============================] - 22s 951us/step - loss: 2.3226 - acc: 0.4147 - val_loss: 3.5151 - val_acc: 0.4053\n",
      "Epoch 361/500\n",
      "22708/22708 [==============================] - 22s 959us/step - loss: 2.3148 - acc: 0.4148 - val_loss: 4.2918 - val_acc: 0.4013\n",
      "Epoch 362/500\n",
      "22708/22708 [==============================] - 22s 954us/step - loss: 2.3289 - acc: 0.4129 - val_loss: 6.7409 - val_acc: 0.4080\n",
      "Epoch 363/500\n",
      "22708/22708 [==============================] - 22s 949us/step - loss: 2.3210 - acc: 0.4122 - val_loss: 4.3317 - val_acc: 0.4057\n",
      "Epoch 364/500\n",
      "22708/22708 [==============================] - 22s 951us/step - loss: 2.2746 - acc: 0.4151 - val_loss: 4.6062 - val_acc: 0.4057\n",
      "Epoch 365/500\n",
      "22708/22708 [==============================] - 22s 953us/step - loss: 2.2545 - acc: 0.4161 - val_loss: 3.2766 - val_acc: 0.4053\n",
      "Epoch 366/500\n",
      "22708/22708 [==============================] - 22s 956us/step - loss: 2.2320 - acc: 0.4191 - val_loss: 3.6458 - val_acc: 0.2634\n",
      "Epoch 367/500\n",
      "22708/22708 [==============================] - 22s 955us/step - loss: 2.2259 - acc: 0.4189 - val_loss: 4.2879 - val_acc: 0.2510\n",
      "Epoch 368/500\n",
      "22708/22708 [==============================] - 22s 954us/step - loss: 2.2080 - acc: 0.4220 - val_loss: 3.6036 - val_acc: 0.3747\n",
      "Epoch 369/500\n",
      "22708/22708 [==============================] - 22s 950us/step - loss: 2.1985 - acc: 0.4231 - val_loss: 3.9981 - val_acc: 0.3709\n",
      "Epoch 370/500\n",
      "22708/22708 [==============================] - 22s 953us/step - loss: 2.1871 - acc: 0.4249 - val_loss: 3.7744 - val_acc: 0.3893\n",
      "Epoch 371/500\n",
      "22708/22708 [==============================] - 22s 956us/step - loss: 2.1651 - acc: 0.4269 - val_loss: 3.3778 - val_acc: 0.3885\n",
      "Epoch 372/500\n",
      "22708/22708 [==============================] - 22s 953us/step - loss: 2.1572 - acc: 0.4276 - val_loss: 5.2136 - val_acc: 0.2998\n",
      "Epoch 373/500\n",
      "22708/22708 [==============================] - 22s 954us/step - loss: 2.1387 - acc: 0.4326 - val_loss: 3.3427 - val_acc: 0.3841\n",
      "Epoch 374/500\n",
      "22708/22708 [==============================] - 22s 953us/step - loss: 2.1016 - acc: 0.4395 - val_loss: 3.6294 - val_acc: 0.3908\n",
      "Epoch 375/500\n",
      "22708/22708 [==============================] - 22s 957us/step - loss: 2.0578 - acc: 0.4446 - val_loss: 4.5030 - val_acc: 0.3753\n",
      "Epoch 376/500\n",
      "22708/22708 [==============================] - 22s 952us/step - loss: 2.0251 - acc: 0.4520 - val_loss: 4.2123 - val_acc: 0.4020\n",
      "Epoch 377/500\n",
      "22708/22708 [==============================] - 22s 954us/step - loss: 1.9883 - acc: 0.4577 - val_loss: 5.0389 - val_acc: 0.2033\n",
      "Epoch 378/500\n",
      "22708/22708 [==============================] - 22s 950us/step - loss: 1.9659 - acc: 0.4642 - val_loss: 4.2562 - val_acc: 0.4037\n",
      "Epoch 379/500\n",
      "22708/22708 [==============================] - 22s 954us/step - loss: 1.9390 - acc: 0.4669 - val_loss: 4.1868 - val_acc: 0.3565\n",
      "Epoch 380/500\n",
      "22708/22708 [==============================] - 22s 951us/step - loss: 1.9021 - acc: 0.4706 - val_loss: 3.8201 - val_acc: 0.3510\n",
      "Epoch 381/500\n",
      "22708/22708 [==============================] - 22s 954us/step - loss: 1.8831 - acc: 0.4782 - val_loss: 3.9534 - val_acc: 0.3427\n",
      "Epoch 382/500\n",
      "22708/22708 [==============================] - 22s 955us/step - loss: 1.8554 - acc: 0.4816 - val_loss: 4.1357 - val_acc: 0.3828\n",
      "Epoch 383/500\n",
      "22708/22708 [==============================] - 22s 954us/step - loss: 1.8384 - acc: 0.4852 - val_loss: 4.2765 - val_acc: 0.3721\n",
      "Epoch 384/500\n",
      "22708/22708 [==============================] - 22s 955us/step - loss: 1.7894 - acc: 0.4984 - val_loss: 4.1167 - val_acc: 0.3802\n",
      "Epoch 385/500\n",
      "22708/22708 [==============================] - 22s 953us/step - loss: 1.7592 - acc: 0.5009 - val_loss: 4.1770 - val_acc: 0.3910\n",
      "Epoch 386/500\n",
      "22708/22708 [==============================] - 22s 956us/step - loss: 1.7150 - acc: 0.5149 - val_loss: 4.1329 - val_acc: 0.3934\n",
      "Epoch 387/500\n",
      "22708/22708 [==============================] - 22s 956us/step - loss: 1.6735 - acc: 0.5220 - val_loss: 3.9377 - val_acc: 0.3790\n",
      "Epoch 388/500\n",
      "22708/22708 [==============================] - 22s 956us/step - loss: 1.5873 - acc: 0.5374 - val_loss: 4.1759 - val_acc: 0.3818\n",
      "Epoch 389/500\n",
      "22708/22708 [==============================] - 22s 963us/step - loss: 1.5040 - acc: 0.5569 - val_loss: 4.2006 - val_acc: 0.3690\n",
      "Epoch 390/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 1.4367 - acc: 0.5768 - val_loss: 4.0269 - val_acc: 0.3566\n",
      "Epoch 391/500\n",
      "22708/22708 [==============================] - 22s 959us/step - loss: 1.5064 - acc: 0.5602 - val_loss: 3.6480 - val_acc: 0.4011\n",
      "Epoch 392/500\n",
      "22708/22708 [==============================] - 22s 957us/step - loss: 1.3278 - acc: 0.6031 - val_loss: 3.6333 - val_acc: 0.3793\n",
      "Epoch 393/500\n",
      "22708/22708 [==============================] - 22s 959us/step - loss: 1.3897 - acc: 0.5864 - val_loss: 3.7660 - val_acc: 0.4097\n",
      "Epoch 394/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 1.2992 - acc: 0.6131 - val_loss: 3.5376 - val_acc: 0.3663\n",
      "Epoch 395/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 1.2295 - acc: 0.6281 - val_loss: 3.4070 - val_acc: 0.3956\n",
      "Epoch 396/500\n",
      "22708/22708 [==============================] - 22s 956us/step - loss: 1.1808 - acc: 0.6400 - val_loss: 3.3629 - val_acc: 0.4028\n",
      "Epoch 397/500\n",
      "22708/22708 [==============================] - 22s 962us/step - loss: 1.1540 - acc: 0.6465 - val_loss: 3.5960 - val_acc: 0.3623\n",
      "Epoch 398/500\n",
      "22708/22708 [==============================] - 22s 952us/step - loss: 2.2330 - acc: 0.4526 - val_loss: 6.2943 - val_acc: 0.4057\n",
      "Epoch 399/500\n",
      "22708/22708 [==============================] - 22s 952us/step - loss: 2.4681 - acc: 0.4069 - val_loss: 5.1094 - val_acc: 0.4062\n",
      "Epoch 400/500\n",
      "22708/22708 [==============================] - 22s 949us/step - loss: 2.4644 - acc: 0.4089 - val_loss: 5.6226 - val_acc: 0.4065\n",
      "Epoch 401/500\n",
      "22708/22708 [==============================] - 22s 953us/step - loss: 2.4005 - acc: 0.4099 - val_loss: 4.3801 - val_acc: 0.4044\n",
      "Epoch 402/500\n",
      "22708/22708 [==============================] - 22s 951us/step - loss: 2.3510 - acc: 0.4126 - val_loss: 5.8865 - val_acc: 0.4055\n",
      "Epoch 403/500\n",
      "22708/22708 [==============================] - 22s 950us/step - loss: 2.3104 - acc: 0.4130 - val_loss: 3.3861 - val_acc: 0.4010\n",
      "Epoch 404/500\n",
      "22708/22708 [==============================] - 22s 955us/step - loss: 2.2757 - acc: 0.4132 - val_loss: 4.7954 - val_acc: 0.1093\n",
      "Epoch 405/500\n",
      "22708/22708 [==============================] - 22s 952us/step - loss: 2.2340 - acc: 0.4193 - val_loss: 4.5114 - val_acc: 0.4046\n",
      "Epoch 406/500\n",
      "22708/22708 [==============================] - 22s 952us/step - loss: 2.2045 - acc: 0.4246 - val_loss: 4.0454 - val_acc: 0.3488\n",
      "Epoch 407/500\n",
      "22708/22708 [==============================] - 22s 954us/step - loss: 2.1859 - acc: 0.4271 - val_loss: 3.9227 - val_acc: 0.3327\n",
      "Epoch 408/500\n",
      "22708/22708 [==============================] - 22s 951us/step - loss: 2.0919 - acc: 0.4364 - val_loss: 4.0062 - val_acc: 0.3811\n",
      "Epoch 409/500\n",
      "22708/22708 [==============================] - 22s 952us/step - loss: 2.0293 - acc: 0.4510 - val_loss: 3.2992 - val_acc: 0.3215\n",
      "Epoch 410/500\n",
      "22708/22708 [==============================] - 22s 956us/step - loss: 1.9423 - acc: 0.4649 - val_loss: 3.8340 - val_acc: 0.3465\n",
      "Epoch 411/500\n",
      "22708/22708 [==============================] - 22s 962us/step - loss: 1.8589 - acc: 0.4803 - val_loss: 4.3349 - val_acc: 0.4030\n",
      "Epoch 412/500\n",
      "22708/22708 [==============================] - 22s 959us/step - loss: 1.7128 - acc: 0.5136 - val_loss: 4.7679 - val_acc: 0.3774\n",
      "Epoch 413/500\n",
      "22708/22708 [==============================] - 22s 957us/step - loss: 1.9351 - acc: 0.4712 - val_loss: 4.6571 - val_acc: 0.4042\n",
      "Epoch 414/500\n",
      "22708/22708 [==============================] - 22s 957us/step - loss: 1.8232 - acc: 0.4930 - val_loss: 3.4392 - val_acc: 0.3911\n",
      "Epoch 415/500\n",
      "22708/22708 [==============================] - 22s 957us/step - loss: 2.0479 - acc: 0.4458 - val_loss: 3.8707 - val_acc: 0.3863\n",
      "Epoch 416/500\n",
      "22708/22708 [==============================] - 22s 956us/step - loss: 1.8958 - acc: 0.4767 - val_loss: 3.3353 - val_acc: 0.3436\n",
      "Epoch 417/500\n",
      "22708/22708 [==============================] - 22s 962us/step - loss: 1.7892 - acc: 0.4988 - val_loss: 3.8178 - val_acc: 0.3731\n",
      "Epoch 418/500\n",
      "22708/22708 [==============================] - 22s 958us/step - loss: 2.3548 - acc: 0.4207 - val_loss: 5.2109 - val_acc: 0.4045\n",
      "Epoch 419/500\n",
      "22708/22708 [==============================] - 22s 957us/step - loss: 2.4639 - acc: 0.4077 - val_loss: 6.0738 - val_acc: 0.4074\n",
      "Epoch 420/500\n",
      "22708/22708 [==============================] - 22s 959us/step - loss: 2.4169 - acc: 0.4107 - val_loss: 3.6946 - val_acc: 0.4063\n",
      "Epoch 421/500\n",
      "22708/22708 [==============================] - 22s 957us/step - loss: 2.3850 - acc: 0.4108 - val_loss: 3.1613 - val_acc: 0.2448\n",
      "Epoch 422/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 2.3605 - acc: 0.4126 - val_loss: 6.2428 - val_acc: 0.4066\n",
      "Epoch 423/500\n",
      "22708/22708 [==============================] - 22s 958us/step - loss: 2.3306 - acc: 0.4133 - val_loss: 3.2739 - val_acc: 0.3892\n",
      "Epoch 424/500\n",
      "22708/22708 [==============================] - 22s 955us/step - loss: 2.2967 - acc: 0.4135 - val_loss: 4.4521 - val_acc: 0.4033\n",
      "Epoch 425/500\n",
      "22708/22708 [==============================] - 22s 956us/step - loss: 2.2583 - acc: 0.4172 - val_loss: 4.1091 - val_acc: 0.1942\n",
      "Epoch 426/500\n",
      "22708/22708 [==============================] - 22s 963us/step - loss: 2.2426 - acc: 0.4181 - val_loss: 4.0654 - val_acc: 0.4039\n",
      "Epoch 427/500\n",
      "22708/22708 [==============================] - 22s 957us/step - loss: 2.1837 - acc: 0.4224 - val_loss: 5.6371 - val_acc: 0.4060\n",
      "Epoch 428/500\n",
      "22708/22708 [==============================] - 22s 957us/step - loss: 2.1266 - acc: 0.4321 - val_loss: 3.4968 - val_acc: 0.4032\n",
      "Epoch 429/500\n",
      "22708/22708 [==============================] - 22s 964us/step - loss: 2.0687 - acc: 0.4413 - val_loss: 4.4158 - val_acc: 0.2473\n",
      "Epoch 430/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 2.0121 - acc: 0.4527 - val_loss: 4.0177 - val_acc: 0.3841\n",
      "Epoch 431/500\n",
      "22708/22708 [==============================] - 22s 963us/step - loss: 1.9670 - acc: 0.4590 - val_loss: 3.5827 - val_acc: 0.3928\n",
      "Epoch 432/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 1.8966 - acc: 0.4753 - val_loss: 3.8268 - val_acc: 0.3883\n",
      "Epoch 433/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 1.7862 - acc: 0.4978 - val_loss: 3.8642 - val_acc: 0.3892\n",
      "Epoch 434/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 1.6359 - acc: 0.5301 - val_loss: 4.0479 - val_acc: 0.3742\n",
      "Epoch 435/500\n",
      "22708/22708 [==============================] - 22s 958us/step - loss: 1.9778 - acc: 0.4705 - val_loss: 5.9333 - val_acc: 0.1468\n",
      "Epoch 436/500\n",
      "22708/22708 [==============================] - 22s 952us/step - loss: 2.4894 - acc: 0.4044 - val_loss: 7.5785 - val_acc: 0.4071\n",
      "Epoch 437/500\n",
      "22708/22708 [==============================] - 22s 957us/step - loss: 2.4497 - acc: 0.4055 - val_loss: 4.4205 - val_acc: 0.4064\n",
      "Epoch 438/500\n",
      "22708/22708 [==============================] - 22s 954us/step - loss: 2.4137 - acc: 0.4095 - val_loss: 4.5597 - val_acc: 0.4075\n",
      "Epoch 439/500\n",
      "22708/22708 [==============================] - 22s 953us/step - loss: 2.3819 - acc: 0.4116 - val_loss: 3.7094 - val_acc: 0.2826\n",
      "Epoch 440/500\n",
      "22708/22708 [==============================] - 22s 957us/step - loss: 2.3332 - acc: 0.4135 - val_loss: 3.8625 - val_acc: 0.3298\n",
      "Epoch 441/500\n",
      "22708/22708 [==============================] - 22s 955us/step - loss: 2.2877 - acc: 0.4140 - val_loss: 4.1526 - val_acc: 0.4064\n",
      "Epoch 442/500\n",
      "22708/22708 [==============================] - 22s 956us/step - loss: 2.2082 - acc: 0.4221 - val_loss: 3.2693 - val_acc: 0.3714\n",
      "Epoch 443/500\n",
      "22708/22708 [==============================] - 22s 957us/step - loss: 2.1524 - acc: 0.4278 - val_loss: 4.2180 - val_acc: 0.3850\n",
      "Epoch 444/500\n",
      "22708/22708 [==============================] - 22s 957us/step - loss: 2.1066 - acc: 0.4335 - val_loss: 5.5810 - val_acc: 0.1133\n",
      "Epoch 445/500\n",
      "22708/22708 [==============================] - 22s 955us/step - loss: 2.0319 - acc: 0.4489 - val_loss: 3.4687 - val_acc: 0.3757\n",
      "Epoch 446/500\n",
      "22708/22708 [==============================] - 22s 957us/step - loss: 1.9382 - acc: 0.4673 - val_loss: 4.9943 - val_acc: 0.3411\n",
      "Epoch 447/500\n",
      "22708/22708 [==============================] - 22s 959us/step - loss: 1.8414 - acc: 0.4862 - val_loss: 4.1482 - val_acc: 0.3226\n",
      "Epoch 448/500\n",
      "22708/22708 [==============================] - 22s 959us/step - loss: 1.5639 - acc: 0.5476 - val_loss: 4.6944 - val_acc: 0.3305\n",
      "Epoch 449/500\n",
      "22708/22708 [==============================] - 22s 955us/step - loss: 1.9782 - acc: 0.4593 - val_loss: 4.6514 - val_acc: 0.3689\n",
      "Epoch 450/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.6630 - acc: 0.5284 - val_loss: 3.3570 - val_acc: 0.3877\n",
      "Epoch 451/500\n",
      "22708/22708 [==============================] - 22s 963us/step - loss: 1.6414 - acc: 0.5330 - val_loss: 3.5199 - val_acc: 0.3755\n",
      "Epoch 452/500\n",
      "22708/22708 [==============================] - 22s 958us/step - loss: 1.8868 - acc: 0.4754 - val_loss: 3.7597 - val_acc: 0.3646\n",
      "Epoch 453/500\n",
      "22708/22708 [==============================] - 22s 957us/step - loss: 2.4067 - acc: 0.4205 - val_loss: 4.9046 - val_acc: 0.4042\n",
      "Epoch 454/500\n",
      "22708/22708 [==============================] - 22s 953us/step - loss: 2.4835 - acc: 0.4079 - val_loss: 4.7506 - val_acc: 0.2759\n",
      "Epoch 455/500\n",
      "22708/22708 [==============================] - 22s 953us/step - loss: 2.4693 - acc: 0.4079 - val_loss: 3.4797 - val_acc: 0.4017\n",
      "Epoch 456/500\n",
      "22708/22708 [==============================] - 22s 952us/step - loss: 2.4415 - acc: 0.4094 - val_loss: 3.6960 - val_acc: 0.4050\n",
      "Epoch 457/500\n",
      "22708/22708 [==============================] - 22s 956us/step - loss: 2.4145 - acc: 0.4100 - val_loss: 3.5189 - val_acc: 0.3978\n",
      "Epoch 458/500\n",
      "22708/22708 [==============================] - 22s 953us/step - loss: 2.4042 - acc: 0.4097 - val_loss: 3.6686 - val_acc: 0.3829\n",
      "Epoch 459/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 2.4042 - acc: 0.4112 - val_loss: 3.3039 - val_acc: 0.3020\n",
      "Epoch 460/500\n",
      "22708/22708 [==============================] - 22s 951us/step - loss: 2.3922 - acc: 0.4109 - val_loss: 3.5669 - val_acc: 0.4075\n",
      "Epoch 461/500\n",
      "22708/22708 [==============================] - 22s 957us/step - loss: 2.3701 - acc: 0.4118 - val_loss: 3.8398 - val_acc: 0.2600\n",
      "Epoch 462/500\n",
      "22708/22708 [==============================] - 22s 956us/step - loss: 2.3575 - acc: 0.4124 - val_loss: 3.8771 - val_acc: 0.4055\n",
      "Epoch 463/500\n",
      "22708/22708 [==============================] - 22s 954us/step - loss: 2.3472 - acc: 0.4129 - val_loss: 3.7222 - val_acc: 0.2392\n",
      "Epoch 464/500\n",
      "22708/22708 [==============================] - 22s 953us/step - loss: 2.3300 - acc: 0.4157 - val_loss: 4.6466 - val_acc: 0.4082\n",
      "Epoch 465/500\n",
      "22708/22708 [==============================] - 22s 951us/step - loss: 2.3245 - acc: 0.4131 - val_loss: 5.7933 - val_acc: 0.4067\n",
      "Epoch 466/500\n",
      "22708/22708 [==============================] - 22s 955us/step - loss: 2.3226 - acc: 0.4134 - val_loss: 3.3569 - val_acc: 0.2610\n",
      "Epoch 467/500\n",
      "22708/22708 [==============================] - 22s 956us/step - loss: 2.2977 - acc: 0.4150 - val_loss: 3.6010 - val_acc: 0.2456\n",
      "Epoch 468/500\n",
      "22708/22708 [==============================] - 22s 955us/step - loss: 2.2760 - acc: 0.4150 - val_loss: 4.9458 - val_acc: 0.2188\n",
      "Epoch 469/500\n",
      "22708/22708 [==============================] - 22s 953us/step - loss: 2.2545 - acc: 0.4171 - val_loss: 3.9194 - val_acc: 0.2479\n",
      "Epoch 470/500\n",
      "22708/22708 [==============================] - 22s 957us/step - loss: 2.2189 - acc: 0.4210 - val_loss: 3.2252 - val_acc: 0.4057\n",
      "Epoch 471/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 2.1866 - acc: 0.4229 - val_loss: 4.7083 - val_acc: 0.4027\n",
      "Epoch 472/500\n",
      "22708/22708 [==============================] - 22s 955us/step - loss: 2.1529 - acc: 0.4280 - val_loss: 4.0366 - val_acc: 0.2608\n",
      "Epoch 473/500\n",
      "22708/22708 [==============================] - 22s 957us/step - loss: 2.1077 - acc: 0.4366 - val_loss: 3.4503 - val_acc: 0.3863\n",
      "Epoch 474/500\n",
      "22708/22708 [==============================] - 22s 956us/step - loss: 2.0271 - acc: 0.4492 - val_loss: 3.5269 - val_acc: 0.3815\n",
      "Epoch 475/500\n",
      "22708/22708 [==============================] - 22s 955us/step - loss: 1.9724 - acc: 0.4577 - val_loss: 4.6104 - val_acc: 0.3267\n",
      "Epoch 476/500\n",
      "22708/22708 [==============================] - 22s 958us/step - loss: 1.9084 - acc: 0.4704 - val_loss: 3.5236 - val_acc: 0.3951\n",
      "Epoch 477/500\n",
      "22708/22708 [==============================] - 22s 958us/step - loss: 1.8004 - acc: 0.4917 - val_loss: 3.6229 - val_acc: 0.3692\n",
      "Epoch 478/500\n",
      "22708/22708 [==============================] - 22s 958us/step - loss: 1.5860 - acc: 0.5424 - val_loss: 3.6828 - val_acc: 0.3697\n",
      "Epoch 479/500\n",
      "22708/22708 [==============================] - 22s 956us/step - loss: 2.3773 - acc: 0.4162 - val_loss: 6.3915 - val_acc: 0.1203\n",
      "Epoch 480/500\n",
      "22708/22708 [==============================] - 22s 955us/step - loss: 2.4445 - acc: 0.4058 - val_loss: 5.1576 - val_acc: 0.4060\n",
      "Epoch 481/500\n",
      "22708/22708 [==============================] - 22s 953us/step - loss: 2.3800 - acc: 0.4101 - val_loss: 4.4254 - val_acc: 0.1665\n",
      "Epoch 482/500\n",
      "22708/22708 [==============================] - 22s 952us/step - loss: 2.3313 - acc: 0.4138 - val_loss: 3.5071 - val_acc: 0.1897\n",
      "Epoch 483/500\n",
      "22708/22708 [==============================] - 22s 951us/step - loss: 2.3259 - acc: 0.4150 - val_loss: 4.5867 - val_acc: 0.1540\n",
      "Epoch 484/500\n",
      "22708/22708 [==============================] - 22s 954us/step - loss: 2.3072 - acc: 0.4126 - val_loss: 4.3927 - val_acc: 0.1522\n",
      "Epoch 485/500\n",
      "22708/22708 [==============================] - 22s 956us/step - loss: 2.1930 - acc: 0.4235 - val_loss: 6.2285 - val_acc: 0.4068\n",
      "Epoch 486/500\n",
      "22708/22708 [==============================] - 22s 955us/step - loss: 2.0730 - acc: 0.4400 - val_loss: 4.2052 - val_acc: 0.4071\n",
      "Epoch 487/500\n",
      "22708/22708 [==============================] - 22s 959us/step - loss: 1.9412 - acc: 0.4642 - val_loss: 3.3590 - val_acc: 0.3399\n",
      "Epoch 488/500\n",
      "22708/22708 [==============================] - 22s 958us/step - loss: 1.8236 - acc: 0.4875 - val_loss: 3.5086 - val_acc: 0.4029\n",
      "Epoch 489/500\n",
      "22708/22708 [==============================] - 22s 962us/step - loss: 1.6531 - acc: 0.5258 - val_loss: 3.2961 - val_acc: 0.3901\n",
      "Epoch 490/500\n",
      "22708/22708 [==============================] - 22s 963us/step - loss: 1.4102 - acc: 0.5809 - val_loss: 3.3282 - val_acc: 0.3951\n",
      "Epoch 491/500\n",
      "22708/22708 [==============================] - 22s 963us/step - loss: 1.2811 - acc: 0.6148 - val_loss: 3.5069 - val_acc: 0.4050\n",
      "Epoch 492/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 1.2477 - acc: 0.6219 - val_loss: 3.1193 - val_acc: 0.4054\n",
      "Epoch 493/500\n",
      "22708/22708 [==============================] - 22s 959us/step - loss: 1.4242 - acc: 0.5838 - val_loss: 3.4684 - val_acc: 0.3984\n",
      "Epoch 494/500\n",
      "22708/22708 [==============================] - 22s 957us/step - loss: 1.6340 - acc: 0.5306 - val_loss: 3.2828 - val_acc: 0.3851\n",
      "Epoch 495/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 1.2476 - acc: 0.6278 - val_loss: 3.3986 - val_acc: 0.4011\n",
      "Epoch 496/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.7061 - acc: 0.5152 - val_loss: 3.4170 - val_acc: 0.4006\n",
      "Epoch 497/500\n",
      "22708/22708 [==============================] - 22s 957us/step - loss: 1.4169 - acc: 0.5775 - val_loss: 3.2243 - val_acc: 0.3861\n",
      "Epoch 498/500\n",
      "22708/22708 [==============================] - 22s 961us/step - loss: 1.1054 - acc: 0.6621 - val_loss: 3.2494 - val_acc: 0.3936\n",
      "Epoch 499/500\n",
      "22708/22708 [==============================] - 22s 958us/step - loss: 1.1116 - acc: 0.6596 - val_loss: 3.2130 - val_acc: 0.3916\n",
      "Epoch 500/500\n",
      "22708/22708 [==============================] - 22s 960us/step - loss: 1.0819 - acc: 0.6688 - val_loss: 3.4822 - val_acc: 0.4120\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff2a2a77f28>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 32
    }
   ]
  }
 ]
}