{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "512.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "accelerator": "GPU",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "kHwmHF3W-ycN",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import os\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.layers import Dense, Dropout, Activation, LSTM, Bidirectional, Input, Reshape, Permute, Lambda, TimeDistributed, SpatialDropout1D, BatchNormalization\n",
    "from keras.layers.merge import Add\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fTHrv35G-3Vt",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def load_seq_lst(lst_name, num_seq, ovr_num=None, num_joints=25, num_class=52):\n",
    "    assert (os.path.isfile(lst_name + '.txt') and os.path.isfile(lst_name + '.h5'))\n",
    "    if ovr_num is None:\n",
    "        ovr_num = int(num_seq / 2)\n",
    "    keyname_lst = [item.strip().split() for item in open(lst_name + '.txt', 'r').readlines()]\n",
    "    X = []\n",
    "    Y = []\n",
    "    vid_list = []\n",
    "    start_list = []\n",
    "    with h5py.File(lst_name + '.h5', 'r') as hf:\n",
    "        for item in keyname_lst:\n",
    "            skeleton = np.asarray(hf.get(item[0]))\n",
    "            # skeleton = skeleton.reshape((skeleton.shape[0], self._num_joints, self._dim_point ))\n",
    "            skeleton1 = skeleton[:, 0:75].reshape((skeleton.shape[0], num_joints, 3))\n",
    "            skeleton2 = skeleton[:, 75:].reshape((skeleton.shape[0], num_joints, 3))\n",
    "            skeleton = np.concatenate((skeleton1, skeleton2), axis=-1)\n",
    "\n",
    "            labels = np.asarray(hf.get(item[1]), dtype=int)\n",
    "            labels_pertime = np.zeros((skeleton.shape[0]), dtype=np.int32)\n",
    "            for clip_idx in range(len(labels)):\n",
    "                labels_pertime[labels[clip_idx][1]:labels[clip_idx][2]] = labels[clip_idx][0] + 1\n",
    "            labels_pertime = labels_pertime.astype(np.int32)\n",
    "            labels_pertime = np_utils.to_categorical(labels_pertime, num_class)\n",
    "\n",
    "            if skeleton.shape[0] > num_seq:\n",
    "                start = 0\n",
    "                while start + num_seq < skeleton.shape[0]:\n",
    "                    X.append(skeleton[start:start + num_seq])\n",
    "                    Y.append(labels_pertime[start + num_seq])\n",
    "                    vid_list.append(item[0])\n",
    "                    start_list.append(start)\n",
    "                    start = start + ovr_num\n",
    "                X.append(skeleton[-num_seq:])\n",
    "                Y.append(labels_pertime[-1])\n",
    "                vid_list.append(item[0])\n",
    "                start_list.append(skeleton.shape[0] - num_seq)\n",
    "            else:\n",
    "                skeleton = np.concatenate((np.zeros((num_seq - skeleton.shape[0], skeleton.shape[1], skeleton.shape[2])), skeleton), axis=0)\n",
    "                labels_pertime = np.concatenate((np.zeros((num_seq - labels_pertime.shape[0], labels_pertime.shape[1])), labels_pertime), axis=0)\n",
    "                X.append(skeleton)\n",
    "                Y.append(labels_pertime[-1])\n",
    "                vid_list.append(item[0])\n",
    "                start_list.append(0)\n",
    "\n",
    "    X = np.asarray(X).astype(np.float32)\n",
    "    Y = np.asarray(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y, vid_list, start_list"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rZOuSHXx-5cT",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def multi_pertime(param, sub_mean=False):\n",
    "    skt_input = Input(shape=(param['num_seq'], param['num_joints'], param['dim_point']))\n",
    "    data = skt_input\n",
    "\n",
    "    if sub_mean:\n",
    "        data = Permute((1, 3, 2))(data)\n",
    "        data2 = TimeDistributed(TimeDistributed(Dense(1, trainable=False)))(data)\n",
    "        data2 = Lambda(lambda x: K.repeat_elements(x, param['num_joints'], axis=-1), output_shape=lambda s: (s[0], s[1], s[2], s[3] * param['num_joints']))(data2)\n",
    "        data = Add()([data, data2])\n",
    "\n",
    "    data = Reshape((param['num_seq'], param['num_joints'] * param['dim_point']))(data)\n",
    "\n",
    "    hid_size = 512\n",
    "    #data = SpatialDropout1D(0.05)(data)\n",
    "    out = Bidirectional(LSTM(hid_size, return_sequences=True))(data)\n",
    "    #out = SpatialDropout1D(0.05)(out)\n",
    "    out = Bidirectional(LSTM(hid_size, return_sequences=True))(out)\n",
    "    #out = SpatialDropout1D(0.05)(out)\n",
    "    out = Bidirectional(LSTM(hid_size, return_sequences=False))(out)\n",
    "\n",
    "    out = BatchNormalization()(out)\n",
    "    out = Activation('relu')(Dropout(0.5)(out))\n",
    "    prob = Dense(param['num_class'], activation='softmax')(out)\n",
    "\n",
    "    opt = SGD(lr=param['base_learn_rate'], decay=param['weight_regular'], momentum=0.9, nesterov=True)\n",
    "    model = Model(skt_input, prob)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oOpHGRYd_FEA",
    "colab_type": "code",
    "outputId": "dc596175-8960-46f1-e050-3aa6f5d7d967",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    }
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5afvV-DdtUcL",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def schedule(epoch):\n",
    "    lr = K.get_value(model.optimizer.lr)\n",
    "    if epoch % param['step_inter'] == 0 and epoch > 0:\n",
    "        lr = lr * param['lr_gamma']\n",
    "    return np.float(lr)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PFe5IAPg-7gU",
    "colab_type": "code",
    "outputId": "9dbb472b-064c-4346-a9a5-e4298bf9dbd4",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    }
   },
   "source": [
    "param = {'max_iter': 500, 'step_inter': 80, 'base_learn_rate': 0.01, 'lr_gamma': 0.5, 'weight_regular': 0, 'num_seq': 50, 'batchsize': 1024, 'write_file': True,\n",
    "         'num_joints': 25, 'num_class': 51 + 1, 'dim_point': 3 * 2, 'save_model': True}\n",
    "param['trn_file'] = F\"/content/drive/My Drive/crs_view_trn_skt\"\n",
    "param['val_file'] = F\"/content/drive/My Drive/crs_view_val_skt\"\n",
    "param['save_path'] = F\"/content/drive/My Drive/save_param_temp/gcn/weights1.hdf5\"\n",
    "param['write_file_name'] = F\"/content/drive/My Drive/save_param_temp/gcn.txt\"\n",
    "# no batch size, as one batch only\n",
    "\n",
    "trainX, trainY, train_vid_list, train_start_list = load_seq_lst(param['trn_file'], param['num_seq'])\n",
    "valX, valY, val_vid_list, val_start_list = load_seq_lst(param['val_file'], param['num_seq'])"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "(22708, 50, 25, 6) (22708, 52)\n",
      "(12767, 50, 25, 6) (12767, 52)\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rJ_AM7qAAgEg",
    "colab_type": "code",
    "outputId": "ed5b58c4-c6de-4b2b-84c9-4e242a4e86d4",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "source": [
    "model = multi_pertime(param, sub_mean=True)\n",
    "reduce_lr = LearningRateScheduler(schedule)\n",
    "checkpoint = ModelCheckpoint(param['save_path'], verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=5)\n",
    "\n",
    "model.fit(trainX, trainY, batch_size=param['batchsize'], epochs=param['max_iter'], shuffle=True, validation_data=(valX, valY),callbacks=[checkpoint], verbose=1)"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 50, 25, 6)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "permute_5 (Permute)             (None, 50, 6, 25)    0           input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_10 (TimeDistri (None, 50, 6, 1)     26          permute_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 50, 6, 25)    0           time_distributed_10[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 50, 6, 25)    0           permute_5[0][0]                  \n",
      "                                                                 lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 50, 150)      0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_13 (Bidirectional (None, 50, 1024)     2715648     reshape_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_14 (Bidirectional (None, 50, 1024)     6295552     bidirectional_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_15 (Bidirectional (None, 1024)         6295552     bidirectional_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 1024)         4096        bidirectional_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 1024)         0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 1024)         0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 52)           53300       activation_4[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 15,364,174\n",
      "Trainable params: 15,362,126\n",
      "Non-trainable params: 2,048\n",
      "__________________________________________________________________________________________________\n",
      "Train on 22708 samples, validate on 12767 samples\n",
      "Epoch 1/500\n",
      "22708/22708 [==============================] - 40s 2ms/step - loss: 2.9131 - acc: 0.3612 - val_loss: 4.0482 - val_acc: 0.4060\n",
      "Epoch 2/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 2.4543 - acc: 0.4092 - val_loss: 3.3089 - val_acc: 0.4075\n",
      "Epoch 3/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 2.3279 - acc: 0.4237 - val_loss: 3.0929 - val_acc: 0.4084\n",
      "Epoch 4/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 2.2414 - acc: 0.4319 - val_loss: 2.7180 - val_acc: 0.4105\n",
      "Epoch 5/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 2.1889 - acc: 0.4381 - val_loss: 3.0033 - val_acc: 0.4086\n",
      "Epoch 6/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 2.1276 - acc: 0.4509 - val_loss: 3.0259 - val_acc: 0.4083\n",
      "Epoch 7/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 2.0840 - acc: 0.4541 - val_loss: 2.7178 - val_acc: 0.4109\n",
      "Epoch 8/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 2.0523 - acc: 0.4624 - val_loss: 2.8473 - val_acc: 0.4101\n",
      "Epoch 9/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 2.0257 - acc: 0.4634 - val_loss: 2.8819 - val_acc: 0.4104\n",
      "Epoch 10/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.9913 - acc: 0.4714 - val_loss: 2.8005 - val_acc: 0.4101\n",
      "Epoch 11/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.9639 - acc: 0.4758 - val_loss: 2.7631 - val_acc: 0.4093\n",
      "Epoch 12/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.9459 - acc: 0.4790 - val_loss: 2.7065 - val_acc: 0.4098\n",
      "Epoch 13/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.9198 - acc: 0.4806 - val_loss: 2.8289 - val_acc: 0.4107\n",
      "Epoch 14/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.9033 - acc: 0.4879 - val_loss: 2.7660 - val_acc: 0.4105\n",
      "Epoch 15/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.8698 - acc: 0.4934 - val_loss: 2.8076 - val_acc: 0.4139\n",
      "Epoch 16/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.8551 - acc: 0.4974 - val_loss: 2.7504 - val_acc: 0.4118\n",
      "Epoch 17/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.8371 - acc: 0.5002 - val_loss: 2.8837 - val_acc: 0.4089\n",
      "Epoch 18/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.8203 - acc: 0.5039 - val_loss: 3.0542 - val_acc: 0.4139\n",
      "Epoch 19/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.7992 - acc: 0.5095 - val_loss: 2.6999 - val_acc: 0.4078\n",
      "Epoch 20/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.7852 - acc: 0.5135 - val_loss: 3.0023 - val_acc: 0.3669\n",
      "Epoch 21/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.7703 - acc: 0.5167 - val_loss: 2.7856 - val_acc: 0.4039\n",
      "Epoch 22/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.7535 - acc: 0.5177 - val_loss: 2.7917 - val_acc: 0.4094\n",
      "Epoch 23/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.7422 - acc: 0.5199 - val_loss: 2.9728 - val_acc: 0.3963\n",
      "Epoch 24/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.7250 - acc: 0.5226 - val_loss: 2.9403 - val_acc: 0.3777\n",
      "Epoch 25/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.7184 - acc: 0.5271 - val_loss: 2.7593 - val_acc: 0.4091\n",
      "Epoch 26/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.7000 - acc: 0.5290 - val_loss: 3.0007 - val_acc: 0.4065\n",
      "Epoch 27/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.6896 - acc: 0.5288 - val_loss: 3.1791 - val_acc: 0.4014\n",
      "Epoch 28/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.6685 - acc: 0.5347 - val_loss: 3.0634 - val_acc: 0.3697\n",
      "Epoch 29/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.6616 - acc: 0.5355 - val_loss: 2.8606 - val_acc: 0.3963\n",
      "Epoch 30/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.6552 - acc: 0.5384 - val_loss: 2.9601 - val_acc: 0.4053\n",
      "Epoch 31/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.6389 - acc: 0.5436 - val_loss: 2.8401 - val_acc: 0.4136\n",
      "Epoch 32/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.6144 - acc: 0.5471 - val_loss: 2.7655 - val_acc: 0.3977\n",
      "Epoch 33/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.6218 - acc: 0.5451 - val_loss: 2.7666 - val_acc: 0.4106\n",
      "Epoch 34/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.5956 - acc: 0.5505 - val_loss: 2.9822 - val_acc: 0.4078\n",
      "Epoch 35/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.5874 - acc: 0.5517 - val_loss: 2.7795 - val_acc: 0.4068\n",
      "Epoch 36/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.5792 - acc: 0.5563 - val_loss: 2.8861 - val_acc: 0.3892\n",
      "Epoch 37/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.5772 - acc: 0.5567 - val_loss: 3.0619 - val_acc: 0.4165\n",
      "Epoch 38/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.5377 - acc: 0.5617 - val_loss: 3.1499 - val_acc: 0.4006\n",
      "Epoch 39/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.5502 - acc: 0.5593 - val_loss: 2.8887 - val_acc: 0.4014\n",
      "Epoch 40/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.5310 - acc: 0.5644 - val_loss: 2.9150 - val_acc: 0.4162\n",
      "Epoch 41/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.5285 - acc: 0.5673 - val_loss: 3.3201 - val_acc: 0.4138\n",
      "Epoch 42/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.5186 - acc: 0.5658 - val_loss: 3.0012 - val_acc: 0.4051\n",
      "Epoch 43/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.4975 - acc: 0.5733 - val_loss: 2.9568 - val_acc: 0.3621\n",
      "Epoch 44/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.4829 - acc: 0.5753 - val_loss: 3.1374 - val_acc: 0.3664\n",
      "Epoch 45/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.4845 - acc: 0.5770 - val_loss: 2.9528 - val_acc: 0.4035\n",
      "Epoch 46/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.4746 - acc: 0.5787 - val_loss: 3.1711 - val_acc: 0.3623\n",
      "Epoch 47/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.4699 - acc: 0.5813 - val_loss: 2.8550 - val_acc: 0.4188\n",
      "Epoch 48/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.4529 - acc: 0.5823 - val_loss: 2.9939 - val_acc: 0.3436\n",
      "Epoch 49/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.4589 - acc: 0.5798 - val_loss: 3.2398 - val_acc: 0.4031\n",
      "Epoch 50/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.4412 - acc: 0.5846 - val_loss: 2.9555 - val_acc: 0.3566\n",
      "Epoch 51/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.4209 - acc: 0.5926 - val_loss: 3.0623 - val_acc: 0.4122\n",
      "Epoch 52/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.4171 - acc: 0.5898 - val_loss: 3.0053 - val_acc: 0.3959\n",
      "Epoch 53/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.4218 - acc: 0.5871 - val_loss: 2.8484 - val_acc: 0.4234\n",
      "Epoch 54/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.3845 - acc: 0.5984 - val_loss: 2.9274 - val_acc: 0.4097\n",
      "Epoch 55/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.3725 - acc: 0.6021 - val_loss: 2.7831 - val_acc: 0.4026\n",
      "Epoch 56/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.7102 - acc: 0.5296 - val_loss: 6.2183 - val_acc: 0.3897\n",
      "Epoch 57/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.5542 - acc: 0.5584 - val_loss: 3.5534 - val_acc: 0.4136\n",
      "Epoch 58/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.4356 - acc: 0.5857 - val_loss: 3.6092 - val_acc: 0.4114\n",
      "Epoch 59/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.4139 - acc: 0.5925 - val_loss: 3.1045 - val_acc: 0.3939\n",
      "Epoch 60/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.3630 - acc: 0.6040 - val_loss: 3.0565 - val_acc: 0.4068\n",
      "Epoch 61/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.3502 - acc: 0.6083 - val_loss: 2.8783 - val_acc: 0.4153\n",
      "Epoch 62/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.3367 - acc: 0.6130 - val_loss: 3.4350 - val_acc: 0.3641\n",
      "Epoch 63/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.3219 - acc: 0.6151 - val_loss: 2.9564 - val_acc: 0.4063\n",
      "Epoch 64/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.4042 - acc: 0.5945 - val_loss: 3.1891 - val_acc: 0.4109\n",
      "Epoch 65/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.2982 - acc: 0.6184 - val_loss: 3.1038 - val_acc: 0.4171\n",
      "Epoch 66/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.2762 - acc: 0.6225 - val_loss: 3.2919 - val_acc: 0.3869\n",
      "Epoch 67/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.2682 - acc: 0.6274 - val_loss: 3.2107 - val_acc: 0.3912\n",
      "Epoch 68/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.2454 - acc: 0.6306 - val_loss: 3.1420 - val_acc: 0.4124\n",
      "Epoch 69/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.3022 - acc: 0.6189 - val_loss: 2.9972 - val_acc: 0.4050\n",
      "Epoch 70/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.2462 - acc: 0.6329 - val_loss: 2.9652 - val_acc: 0.3974\n",
      "Epoch 71/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.2625 - acc: 0.6259 - val_loss: 2.9202 - val_acc: 0.4125\n",
      "Epoch 72/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.2164 - acc: 0.6393 - val_loss: 3.2836 - val_acc: 0.4030\n",
      "Epoch 73/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.3552 - acc: 0.6037 - val_loss: 3.3347 - val_acc: 0.3863\n",
      "Epoch 74/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.2115 - acc: 0.6405 - val_loss: 3.3857 - val_acc: 0.4137\n",
      "Epoch 75/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.2208 - acc: 0.6383 - val_loss: 3.0378 - val_acc: 0.3871\n",
      "Epoch 76/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.1677 - acc: 0.6506 - val_loss: 3.2078 - val_acc: 0.3065\n",
      "Epoch 77/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.1918 - acc: 0.6463 - val_loss: 3.6428 - val_acc: 0.2900\n",
      "Epoch 78/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.1615 - acc: 0.6536 - val_loss: 3.4566 - val_acc: 0.3718\n",
      "Epoch 79/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.3927 - acc: 0.5968 - val_loss: 2.9709 - val_acc: 0.4072\n",
      "Epoch 80/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.1777 - acc: 0.6514 - val_loss: 3.0278 - val_acc: 0.3849\n",
      "Epoch 81/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.1359 - acc: 0.6623 - val_loss: 3.2854 - val_acc: 0.4104\n",
      "Epoch 82/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.1083 - acc: 0.6689 - val_loss: 3.2311 - val_acc: 0.3928\n",
      "Epoch 83/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.2164 - acc: 0.6386 - val_loss: 3.4105 - val_acc: 0.3700\n",
      "Epoch 84/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.3255 - acc: 0.6088 - val_loss: 3.2677 - val_acc: 0.4159\n",
      "Epoch 85/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.1458 - acc: 0.6552 - val_loss: 3.0673 - val_acc: 0.4144\n",
      "Epoch 86/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.0775 - acc: 0.6740 - val_loss: 3.1611 - val_acc: 0.3979\n",
      "Epoch 87/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.0586 - acc: 0.6784 - val_loss: 2.9918 - val_acc: 0.3918\n",
      "Epoch 88/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.0708 - acc: 0.6736 - val_loss: 3.2294 - val_acc: 0.3894\n",
      "Epoch 89/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.0451 - acc: 0.6835 - val_loss: 3.0906 - val_acc: 0.4018\n",
      "Epoch 90/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.0392 - acc: 0.6840 - val_loss: 3.7483 - val_acc: 0.3733\n",
      "Epoch 91/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.3692 - acc: 0.6029 - val_loss: 3.4481 - val_acc: 0.3961\n",
      "Epoch 92/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.0680 - acc: 0.6784 - val_loss: 3.2589 - val_acc: 0.3749\n",
      "Epoch 93/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.0227 - acc: 0.6886 - val_loss: 3.2440 - val_acc: 0.3991\n",
      "Epoch 94/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.4931 - acc: 0.5751 - val_loss: 3.7505 - val_acc: 0.4014\n",
      "Epoch 95/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.1515 - acc: 0.6533 - val_loss: 3.1992 - val_acc: 0.4102\n",
      "Epoch 96/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.0205 - acc: 0.6927 - val_loss: 3.5298 - val_acc: 0.3942\n",
      "Epoch 97/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.0126 - acc: 0.6899 - val_loss: 3.4365 - val_acc: 0.3865\n",
      "Epoch 98/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.9598 - acc: 0.7065 - val_loss: 3.1818 - val_acc: 0.3655\n",
      "Epoch 99/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.3519 - acc: 0.6134 - val_loss: 4.8275 - val_acc: 0.4152\n",
      "Epoch 100/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.1491 - acc: 0.6561 - val_loss: 3.5327 - val_acc: 0.3994\n",
      "Epoch 101/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.9652 - acc: 0.7059 - val_loss: 3.6919 - val_acc: 0.4011\n",
      "Epoch 102/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.9397 - acc: 0.7122 - val_loss: 3.1137 - val_acc: 0.3921\n",
      "Epoch 103/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.8930 - acc: 0.7266 - val_loss: 3.4999 - val_acc: 0.3778\n",
      "Epoch 104/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.0096 - acc: 0.6930 - val_loss: 3.2096 - val_acc: 0.4202\n",
      "Epoch 105/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.9045 - acc: 0.7225 - val_loss: 3.3541 - val_acc: 0.3774\n",
      "Epoch 106/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.0763 - acc: 0.6695 - val_loss: 3.3746 - val_acc: 0.4022\n",
      "Epoch 107/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.8901 - acc: 0.7240 - val_loss: 3.4952 - val_acc: 0.4021\n",
      "Epoch 108/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.8509 - acc: 0.7368 - val_loss: 3.4778 - val_acc: 0.3793\n",
      "Epoch 109/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.8384 - acc: 0.7386 - val_loss: 3.1830 - val_acc: 0.3917\n",
      "Epoch 110/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.8156 - acc: 0.7482 - val_loss: 3.5601 - val_acc: 0.3473\n",
      "Epoch 111/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.1990 - acc: 0.6462 - val_loss: 3.5987 - val_acc: 0.4204\n",
      "Epoch 112/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.1007 - acc: 0.6691 - val_loss: 3.5847 - val_acc: 0.4069\n",
      "Epoch 113/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.8419 - acc: 0.7404 - val_loss: 3.7362 - val_acc: 0.3733\n",
      "Epoch 114/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.7839 - acc: 0.7596 - val_loss: 3.4642 - val_acc: 0.4021\n",
      "Epoch 115/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.7647 - acc: 0.7624 - val_loss: 3.3971 - val_acc: 0.3898\n",
      "Epoch 116/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.7430 - acc: 0.7706 - val_loss: 3.5537 - val_acc: 0.3768\n",
      "Epoch 117/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.7175 - acc: 0.7777 - val_loss: 3.5200 - val_acc: 0.3907\n",
      "Epoch 118/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.0886 - acc: 0.6750 - val_loss: 3.8279 - val_acc: 0.3609\n",
      "Epoch 119/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.9847 - acc: 0.7023 - val_loss: 3.4172 - val_acc: 0.4085\n",
      "Epoch 120/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.7220 - acc: 0.7772 - val_loss: 3.5113 - val_acc: 0.4024\n",
      "Epoch 121/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.6806 - acc: 0.7882 - val_loss: 3.6525 - val_acc: 0.3899\n",
      "Epoch 122/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.6817 - acc: 0.7864 - val_loss: 3.6783 - val_acc: 0.4124\n",
      "Epoch 123/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.8469 - acc: 0.7419 - val_loss: 4.0210 - val_acc: 0.3941\n",
      "Epoch 124/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.6502 - acc: 0.7951 - val_loss: 4.1240 - val_acc: 0.3782\n",
      "Epoch 125/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.0684 - acc: 0.6793 - val_loss: 3.5706 - val_acc: 0.4191\n",
      "Epoch 126/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.6998 - acc: 0.7821 - val_loss: 3.9202 - val_acc: 0.3972\n",
      "Epoch 127/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.6529 - acc: 0.7954 - val_loss: 3.9536 - val_acc: 0.3753\n",
      "Epoch 128/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.5950 - acc: 0.8142 - val_loss: 3.7447 - val_acc: 0.4068\n",
      "Epoch 129/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.5927 - acc: 0.8152 - val_loss: 3.6856 - val_acc: 0.4029\n",
      "Epoch 130/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.5481 - acc: 0.8281 - val_loss: 3.8811 - val_acc: 0.4114\n",
      "Epoch 131/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.5231 - acc: 0.8390 - val_loss: 4.1310 - val_acc: 0.4057\n",
      "Epoch 132/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.0561 - acc: 0.6942 - val_loss: 5.4005 - val_acc: 0.3765\n",
      "Epoch 133/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.0829 - acc: 0.6774 - val_loss: 3.7534 - val_acc: 0.4227\n",
      "Epoch 134/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.6127 - acc: 0.8073 - val_loss: 3.7278 - val_acc: 0.3952\n",
      "Epoch 135/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.5359 - acc: 0.8349 - val_loss: 3.6422 - val_acc: 0.3784\n",
      "Epoch 136/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.5167 - acc: 0.8403 - val_loss: 3.8564 - val_acc: 0.3932\n",
      "Epoch 137/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.4815 - acc: 0.8504 - val_loss: 4.1477 - val_acc: 0.3986\n",
      "Epoch 138/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.5152 - acc: 0.8400 - val_loss: 4.5723 - val_acc: 0.3931\n",
      "Epoch 139/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.9430 - acc: 0.7212 - val_loss: 3.7543 - val_acc: 0.4022\n",
      "Epoch 140/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.8931 - acc: 0.7276 - val_loss: 3.9227 - val_acc: 0.3900\n",
      "Epoch 141/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.7859 - acc: 0.7568 - val_loss: 4.1806 - val_acc: 0.3978\n",
      "Epoch 142/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.5112 - acc: 0.8419 - val_loss: 3.8555 - val_acc: 0.3891\n",
      "Epoch 143/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.4522 - acc: 0.8602 - val_loss: 3.8635 - val_acc: 0.4086\n",
      "Epoch 144/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.4070 - acc: 0.8786 - val_loss: 4.2267 - val_acc: 0.4034\n",
      "Epoch 145/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.3865 - acc: 0.8822 - val_loss: 4.3301 - val_acc: 0.4069\n",
      "Epoch 146/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.3813 - acc: 0.8849 - val_loss: 4.1937 - val_acc: 0.3972\n",
      "Epoch 147/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.3563 - acc: 0.8938 - val_loss: 4.2216 - val_acc: 0.3759\n",
      "Epoch 148/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.3381 - acc: 0.9004 - val_loss: 4.2263 - val_acc: 0.4029\n",
      "Epoch 149/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.3352 - acc: 0.9014 - val_loss: 4.5976 - val_acc: 0.3789\n",
      "Epoch 150/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 2.1275 - acc: 0.5111 - val_loss: 8.5882 - val_acc: 0.4068\n",
      "Epoch 151/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 2.2145 - acc: 0.4318 - val_loss: 8.0398 - val_acc: 0.1882\n",
      "Epoch 152/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 2.3222 - acc: 0.4289 - val_loss: 9.3580 - val_acc: 0.4072\n",
      "Epoch 153/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 2.4691 - acc: 0.3998 - val_loss: 9.1167 - val_acc: 0.4072\n",
      "Epoch 154/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 2.3217 - acc: 0.4153 - val_loss: 8.6813 - val_acc: 0.4072\n",
      "Epoch 155/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 2.1233 - acc: 0.4409 - val_loss: 7.7069 - val_acc: 0.4068\n",
      "Epoch 156/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.8124 - acc: 0.4983 - val_loss: 7.0671 - val_acc: 0.3346\n",
      "Epoch 157/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 2.4760 - acc: 0.3992 - val_loss: 8.5290 - val_acc: 0.4071\n",
      "Epoch 158/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 2.3486 - acc: 0.4104 - val_loss: 6.2157 - val_acc: 0.4071\n",
      "Epoch 159/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 2.1929 - acc: 0.4280 - val_loss: 3.9859 - val_acc: 0.3978\n",
      "Epoch 160/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 2.0403 - acc: 0.4537 - val_loss: 3.7281 - val_acc: 0.4051\n",
      "Epoch 161/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.7350 - acc: 0.5128 - val_loss: 4.2040 - val_acc: 0.4074\n",
      "Epoch 162/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.8374 - acc: 0.4954 - val_loss: 6.0834 - val_acc: 0.3840\n",
      "Epoch 163/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.9253 - acc: 0.4776 - val_loss: 4.3009 - val_acc: 0.4021\n",
      "Epoch 164/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.8835 - acc: 0.4947 - val_loss: 5.0372 - val_acc: 0.2626\n",
      "Epoch 165/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.8201 - acc: 0.4944 - val_loss: 4.4021 - val_acc: 0.3790\n",
      "Epoch 166/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.6976 - acc: 0.5282 - val_loss: 3.6483 - val_acc: 0.4000\n",
      "Epoch 167/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.5764 - acc: 0.5508 - val_loss: 3.7760 - val_acc: 0.3484\n",
      "Epoch 168/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.4435 - acc: 0.5793 - val_loss: 4.5973 - val_acc: 0.3825\n",
      "Epoch 169/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.0921 - acc: 0.6674 - val_loss: 4.6404 - val_acc: 0.3912\n",
      "Epoch 170/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.9789 - acc: 0.6992 - val_loss: 4.1122 - val_acc: 0.3806\n",
      "Epoch 171/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.0413 - acc: 0.6878 - val_loss: 3.7489 - val_acc: 0.3945\n",
      "Epoch 172/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.7785 - acc: 0.7566 - val_loss: 3.8315 - val_acc: 0.3872\n",
      "Epoch 173/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.8949 - acc: 0.7283 - val_loss: 3.8354 - val_acc: 0.4000\n",
      "Epoch 174/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.6311 - acc: 0.8043 - val_loss: 3.6747 - val_acc: 0.3869\n",
      "Epoch 175/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.6507 - acc: 0.7982 - val_loss: 3.7438 - val_acc: 0.3970\n",
      "Epoch 176/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.8633 - acc: 0.7380 - val_loss: 3.8845 - val_acc: 0.3400\n",
      "Epoch 177/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.5456 - acc: 0.8297 - val_loss: 3.7218 - val_acc: 0.3962\n",
      "Epoch 178/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.3940 - acc: 0.8826 - val_loss: 3.9863 - val_acc: 0.3989\n",
      "Epoch 179/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.3606 - acc: 0.8946 - val_loss: 3.8565 - val_acc: 0.3948\n",
      "Epoch 180/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.3200 - acc: 0.9096 - val_loss: 3.9892 - val_acc: 0.3700\n",
      "Epoch 181/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.3018 - acc: 0.9158 - val_loss: 4.2313 - val_acc: 0.4063\n",
      "Epoch 182/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.7215 - acc: 0.7852 - val_loss: 3.9062 - val_acc: 0.3941\n",
      "Epoch 183/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.3833 - acc: 0.8825 - val_loss: 4.1732 - val_acc: 0.3872\n",
      "Epoch 184/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.6365 - acc: 0.8104 - val_loss: 7.2917 - val_acc: 0.3095\n",
      "Epoch 185/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.8422 - acc: 0.7441 - val_loss: 3.9020 - val_acc: 0.3988\n",
      "Epoch 186/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.3946 - acc: 0.8806 - val_loss: 3.9022 - val_acc: 0.3976\n",
      "Epoch 187/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.2954 - acc: 0.9178 - val_loss: 4.3567 - val_acc: 0.4024\n",
      "Epoch 188/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.2552 - acc: 0.9307 - val_loss: 4.2306 - val_acc: 0.3941\n",
      "Epoch 189/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.2331 - acc: 0.9383 - val_loss: 4.3579 - val_acc: 0.4024\n",
      "Epoch 190/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.2202 - acc: 0.9419 - val_loss: 4.2404 - val_acc: 0.3970\n",
      "Epoch 191/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.2109 - acc: 0.9457 - val_loss: 4.3413 - val_acc: 0.3814\n",
      "Epoch 192/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.2006 - acc: 0.9482 - val_loss: 4.3230 - val_acc: 0.3934\n",
      "Epoch 193/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.1861 - acc: 0.9548 - val_loss: 4.5663 - val_acc: 0.4022\n",
      "Epoch 194/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.1849 - acc: 0.9548 - val_loss: 4.2573 - val_acc: 0.3655\n",
      "Epoch 195/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.1785 - acc: 0.9568 - val_loss: 4.5704 - val_acc: 0.4031\n",
      "Epoch 196/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.1657 - acc: 0.9595 - val_loss: 4.5424 - val_acc: 0.3782\n",
      "Epoch 197/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.1667 - acc: 0.9602 - val_loss: 4.6925 - val_acc: 0.4081\n",
      "Epoch 198/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.1470 - acc: 0.9643 - val_loss: 4.5145 - val_acc: 0.3653\n",
      "Epoch 199/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.1504 - acc: 0.9660 - val_loss: 4.7180 - val_acc: 0.4032\n",
      "Epoch 200/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.1517 - acc: 0.9641 - val_loss: 4.6441 - val_acc: 0.3890\n",
      "Epoch 201/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.1321 - acc: 0.9716 - val_loss: 4.7109 - val_acc: 0.3869\n",
      "Epoch 202/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.1249 - acc: 0.9729 - val_loss: 4.6112 - val_acc: 0.3859\n",
      "Epoch 203/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.1516 - acc: 0.9641 - val_loss: 4.9608 - val_acc: 0.4124\n",
      "Epoch 204/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.1393 - acc: 0.9683 - val_loss: 4.8316 - val_acc: 0.4123\n",
      "Epoch 205/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.1307 - acc: 0.9702 - val_loss: 4.5743 - val_acc: 0.3710\n",
      "Epoch 206/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.1246 - acc: 0.9715 - val_loss: 4.8091 - val_acc: 0.3900\n",
      "Epoch 207/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.1108 - acc: 0.9770 - val_loss: 4.6735 - val_acc: 0.3772\n",
      "Epoch 208/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.1128 - acc: 0.9757 - val_loss: 4.8602 - val_acc: 0.4005\n",
      "Epoch 209/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.1084 - acc: 0.9778 - val_loss: 4.7330 - val_acc: 0.3992\n",
      "Epoch 210/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.1017 - acc: 0.9789 - val_loss: 4.7994 - val_acc: 0.3835\n",
      "Epoch 211/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.1041 - acc: 0.9787 - val_loss: 4.8475 - val_acc: 0.3892\n",
      "Epoch 212/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0967 - acc: 0.9815 - val_loss: 4.8193 - val_acc: 0.3413\n",
      "Epoch 213/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.1080 - acc: 0.9768 - val_loss: 4.7597 - val_acc: 0.3930\n",
      "Epoch 214/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 2.0722 - acc: 0.5072 - val_loss: 9.4231 - val_acc: 0.0973\n",
      "Epoch 215/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 2.5277 - acc: 0.4034 - val_loss: 8.8025 - val_acc: 0.2350\n",
      "Epoch 216/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 2.4348 - acc: 0.4056 - val_loss: 8.4037 - val_acc: 0.3016\n",
      "Epoch 217/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 2.2506 - acc: 0.4191 - val_loss: 7.7337 - val_acc: 0.4073\n",
      "Epoch 218/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 2.0968 - acc: 0.4435 - val_loss: 8.1370 - val_acc: 0.1770\n",
      "Epoch 219/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.9584 - acc: 0.4677 - val_loss: 6.7155 - val_acc: 0.4022\n",
      "Epoch 220/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.8216 - acc: 0.4949 - val_loss: 5.2066 - val_acc: 0.4082\n",
      "Epoch 221/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.6793 - acc: 0.5225 - val_loss: 4.2424 - val_acc: 0.4098\n",
      "Epoch 222/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.5334 - acc: 0.5573 - val_loss: 3.8377 - val_acc: 0.4073\n",
      "Epoch 223/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.7426 - acc: 0.5207 - val_loss: 5.5874 - val_acc: 0.4065\n",
      "Epoch 224/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.7680 - acc: 0.5126 - val_loss: 6.0167 - val_acc: 0.3927\n",
      "Epoch 225/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.2043 - acc: 0.6399 - val_loss: 5.8936 - val_acc: 0.3937\n",
      "Epoch 226/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.1659 - acc: 0.6532 - val_loss: 4.7078 - val_acc: 0.3784\n",
      "Epoch 227/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.1675 - acc: 0.6582 - val_loss: 4.4053 - val_acc: 0.3905\n",
      "Epoch 228/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.7501 - acc: 0.7647 - val_loss: 3.9689 - val_acc: 0.4044\n",
      "Epoch 229/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.5905 - acc: 0.8181 - val_loss: 4.2257 - val_acc: 0.3912\n",
      "Epoch 230/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.6249 - acc: 0.8072 - val_loss: 4.4009 - val_acc: 0.3837\n",
      "Epoch 231/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.5284 - acc: 0.8394 - val_loss: 4.2599 - val_acc: 0.4021\n",
      "Epoch 232/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.2359 - acc: 0.9371 - val_loss: 4.3268 - val_acc: 0.3888\n",
      "Epoch 233/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.1710 - acc: 0.9596 - val_loss: 4.3041 - val_acc: 0.3952\n",
      "Epoch 234/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.1488 - acc: 0.9677 - val_loss: 4.2756 - val_acc: 0.3876\n",
      "Epoch 235/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.1290 - acc: 0.9739 - val_loss: 4.4614 - val_acc: 0.4051\n",
      "Epoch 236/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.1237 - acc: 0.9739 - val_loss: 4.5668 - val_acc: 0.4083\n",
      "Epoch 237/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.1124 - acc: 0.9778 - val_loss: 4.5067 - val_acc: 0.4017\n",
      "Epoch 238/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.1043 - acc: 0.9803 - val_loss: 4.4646 - val_acc: 0.3959\n",
      "Epoch 239/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0997 - acc: 0.9809 - val_loss: 4.6807 - val_acc: 0.3963\n",
      "Epoch 240/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0913 - acc: 0.9847 - val_loss: 4.6264 - val_acc: 0.3958\n",
      "Epoch 241/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0868 - acc: 0.9855 - val_loss: 4.5862 - val_acc: 0.3928\n",
      "Epoch 242/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0855 - acc: 0.9848 - val_loss: 4.9278 - val_acc: 0.4039\n",
      "Epoch 243/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.1057 - acc: 0.9784 - val_loss: 4.7591 - val_acc: 0.3946\n",
      "Epoch 244/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0917 - acc: 0.9825 - val_loss: 4.8462 - val_acc: 0.3980\n",
      "Epoch 245/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0851 - acc: 0.9854 - val_loss: 4.8660 - val_acc: 0.3839\n",
      "Epoch 246/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0892 - acc: 0.9826 - val_loss: 4.7088 - val_acc: 0.3960\n",
      "Epoch 247/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0844 - acc: 0.9850 - val_loss: 4.8002 - val_acc: 0.3911\n",
      "Epoch 248/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0770 - acc: 0.9858 - val_loss: 4.8267 - val_acc: 0.3937\n",
      "Epoch 249/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0751 - acc: 0.9866 - val_loss: 4.8560 - val_acc: 0.3892\n",
      "Epoch 250/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0812 - acc: 0.9854 - val_loss: 4.8963 - val_acc: 0.3945\n",
      "Epoch 251/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0787 - acc: 0.9862 - val_loss: 4.8752 - val_acc: 0.3916\n",
      "Epoch 252/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0700 - acc: 0.9883 - val_loss: 4.7777 - val_acc: 0.3832\n",
      "Epoch 253/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0693 - acc: 0.9883 - val_loss: 4.9011 - val_acc: 0.3833\n",
      "Epoch 254/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0668 - acc: 0.9890 - val_loss: 4.8265 - val_acc: 0.3932\n",
      "Epoch 255/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0677 - acc: 0.9880 - val_loss: 4.9930 - val_acc: 0.3708\n",
      "Epoch 256/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0765 - acc: 0.9853 - val_loss: 5.0230 - val_acc: 0.4002\n",
      "Epoch 257/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0696 - acc: 0.9878 - val_loss: 4.9052 - val_acc: 0.3905\n",
      "Epoch 258/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0655 - acc: 0.9885 - val_loss: 5.0179 - val_acc: 0.3919\n",
      "Epoch 259/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0689 - acc: 0.9878 - val_loss: 4.9898 - val_acc: 0.3890\n",
      "Epoch 260/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0638 - acc: 0.9898 - val_loss: 5.0200 - val_acc: 0.3845\n",
      "Epoch 261/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0602 - acc: 0.9901 - val_loss: 4.9217 - val_acc: 0.3869\n",
      "Epoch 262/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0587 - acc: 0.9913 - val_loss: 5.0286 - val_acc: 0.3887\n",
      "Epoch 263/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0563 - acc: 0.9915 - val_loss: 5.0448 - val_acc: 0.3450\n",
      "Epoch 264/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0613 - acc: 0.9899 - val_loss: 4.8827 - val_acc: 0.3902\n",
      "Epoch 265/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0604 - acc: 0.9898 - val_loss: 5.0212 - val_acc: 0.3867\n",
      "Epoch 266/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0578 - acc: 0.9910 - val_loss: 5.0520 - val_acc: 0.3849\n",
      "Epoch 267/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0571 - acc: 0.9908 - val_loss: 5.0443 - val_acc: 0.3875\n",
      "Epoch 268/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0612 - acc: 0.9894 - val_loss: 5.0635 - val_acc: 0.3757\n",
      "Epoch 269/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0557 - acc: 0.9908 - val_loss: 5.2137 - val_acc: 0.3926\n",
      "Epoch 270/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0536 - acc: 0.9911 - val_loss: 5.1388 - val_acc: 0.3906\n",
      "Epoch 271/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0585 - acc: 0.9899 - val_loss: 5.0752 - val_acc: 0.3791\n",
      "Epoch 272/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0546 - acc: 0.9914 - val_loss: 5.1761 - val_acc: 0.3826\n",
      "Epoch 273/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0550 - acc: 0.9912 - val_loss: 5.0338 - val_acc: 0.3778\n",
      "Epoch 274/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0593 - acc: 0.9905 - val_loss: 5.0908 - val_acc: 0.3721\n",
      "Epoch 275/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0621 - acc: 0.9889 - val_loss: 5.1250 - val_acc: 0.3914\n",
      "Epoch 276/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0730 - acc: 0.9867 - val_loss: 5.0079 - val_acc: 0.3835\n",
      "Epoch 277/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0735 - acc: 0.9855 - val_loss: 4.9134 - val_acc: 0.3874\n",
      "Epoch 278/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0631 - acc: 0.9882 - val_loss: 5.0453 - val_acc: 0.3827\n",
      "Epoch 279/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0591 - acc: 0.9899 - val_loss: 5.2016 - val_acc: 0.3937\n",
      "Epoch 280/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0610 - acc: 0.9891 - val_loss: 5.1657 - val_acc: 0.3865\n",
      "Epoch 281/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0523 - acc: 0.9918 - val_loss: 5.1316 - val_acc: 0.3851\n",
      "Epoch 282/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0541 - acc: 0.9914 - val_loss: 5.2322 - val_acc: 0.3946\n",
      "Epoch 283/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0507 - acc: 0.9923 - val_loss: 5.1845 - val_acc: 0.3797\n",
      "Epoch 284/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0500 - acc: 0.9922 - val_loss: 5.2390 - val_acc: 0.3871\n",
      "Epoch 285/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0490 - acc: 0.9923 - val_loss: 5.2701 - val_acc: 0.3880\n",
      "Epoch 286/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0498 - acc: 0.9920 - val_loss: 5.1948 - val_acc: 0.3849\n",
      "Epoch 287/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0491 - acc: 0.9922 - val_loss: 5.2153 - val_acc: 0.3735\n",
      "Epoch 288/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0478 - acc: 0.9926 - val_loss: 5.1891 - val_acc: 0.3730\n",
      "Epoch 289/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0468 - acc: 0.9925 - val_loss: 5.1934 - val_acc: 0.3773\n",
      "Epoch 290/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0456 - acc: 0.9929 - val_loss: 5.2822 - val_acc: 0.3777\n",
      "Epoch 291/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0469 - acc: 0.9930 - val_loss: 5.2084 - val_acc: 0.3815\n",
      "Epoch 292/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0465 - acc: 0.9930 - val_loss: 5.2255 - val_acc: 0.3721\n",
      "Epoch 293/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0451 - acc: 0.9931 - val_loss: 5.2155 - val_acc: 0.3745\n",
      "Epoch 294/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0430 - acc: 0.9934 - val_loss: 5.1938 - val_acc: 0.3731\n",
      "Epoch 295/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0425 - acc: 0.9933 - val_loss: 5.2955 - val_acc: 0.3760\n",
      "Epoch 296/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0413 - acc: 0.9938 - val_loss: 5.2027 - val_acc: 0.3765\n",
      "Epoch 297/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0410 - acc: 0.9940 - val_loss: 5.4040 - val_acc: 0.3847\n",
      "Epoch 298/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0418 - acc: 0.9937 - val_loss: 5.2429 - val_acc: 0.3810\n",
      "Epoch 299/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0415 - acc: 0.9939 - val_loss: 5.3038 - val_acc: 0.3832\n",
      "Epoch 300/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0412 - acc: 0.9940 - val_loss: 5.3325 - val_acc: 0.3795\n",
      "Epoch 301/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0420 - acc: 0.9936 - val_loss: 5.3854 - val_acc: 0.3770\n",
      "Epoch 302/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0427 - acc: 0.9934 - val_loss: 5.3613 - val_acc: 0.3784\n",
      "Epoch 303/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0433 - acc: 0.9936 - val_loss: 5.3257 - val_acc: 0.3833\n",
      "Epoch 304/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0435 - acc: 0.9934 - val_loss: 5.4893 - val_acc: 0.3912\n",
      "Epoch 305/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0479 - acc: 0.9922 - val_loss: 5.2987 - val_acc: 0.3868\n",
      "Epoch 306/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0526 - acc: 0.9908 - val_loss: 5.1832 - val_acc: 0.3753\n",
      "Epoch 307/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0569 - acc: 0.9901 - val_loss: 5.7169 - val_acc: 0.3959\n",
      "Epoch 308/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 2.1428 - acc: 0.5052 - val_loss: 8.3931 - val_acc: 0.1400\n",
      "Epoch 309/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.8548 - acc: 0.5005 - val_loss: 9.2037 - val_acc: 0.4072\n",
      "Epoch 310/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.4738 - acc: 0.5795 - val_loss: 5.4871 - val_acc: 0.4167\n",
      "Epoch 311/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.8116 - acc: 0.7463 - val_loss: 4.0496 - val_acc: 0.3988\n",
      "Epoch 312/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.7380 - acc: 0.7943 - val_loss: 5.7990 - val_acc: 0.3867\n",
      "Epoch 313/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 1.2836 - acc: 0.6435 - val_loss: 4.2654 - val_acc: 0.4060\n",
      "Epoch 314/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.6577 - acc: 0.8048 - val_loss: 4.1115 - val_acc: 0.4031\n",
      "Epoch 315/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.1959 - acc: 0.9494 - val_loss: 4.1630 - val_acc: 0.3978\n",
      "Epoch 316/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.1163 - acc: 0.9768 - val_loss: 4.4489 - val_acc: 0.3906\n",
      "Epoch 317/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0923 - acc: 0.9832 - val_loss: 4.5088 - val_acc: 0.3927\n",
      "Epoch 318/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0772 - acc: 0.9878 - val_loss: 4.6645 - val_acc: 0.3977\n",
      "Epoch 319/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0694 - acc: 0.9890 - val_loss: 4.6518 - val_acc: 0.3941\n",
      "Epoch 320/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0648 - acc: 0.9901 - val_loss: 4.6960 - val_acc: 0.4014\n",
      "Epoch 321/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0578 - acc: 0.9915 - val_loss: 4.7611 - val_acc: 0.3998\n",
      "Epoch 322/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0589 - acc: 0.9915 - val_loss: 4.8374 - val_acc: 0.4022\n",
      "Epoch 323/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0551 - acc: 0.9920 - val_loss: 4.8882 - val_acc: 0.3988\n",
      "Epoch 324/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0520 - acc: 0.9924 - val_loss: 4.8490 - val_acc: 0.3958\n",
      "Epoch 325/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0526 - acc: 0.9928 - val_loss: 4.8601 - val_acc: 0.3945\n",
      "Epoch 326/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0498 - acc: 0.9932 - val_loss: 4.9424 - val_acc: 0.3893\n",
      "Epoch 327/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0494 - acc: 0.9932 - val_loss: 4.8904 - val_acc: 0.3875\n",
      "Epoch 328/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0491 - acc: 0.9928 - val_loss: 4.9275 - val_acc: 0.3948\n",
      "Epoch 329/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0484 - acc: 0.9932 - val_loss: 5.0477 - val_acc: 0.4006\n",
      "Epoch 330/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0508 - acc: 0.9924 - val_loss: 5.0443 - val_acc: 0.3912\n",
      "Epoch 331/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0476 - acc: 0.9932 - val_loss: 5.0207 - val_acc: 0.3972\n",
      "Epoch 332/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0455 - acc: 0.9935 - val_loss: 5.0219 - val_acc: 0.3887\n",
      "Epoch 333/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0453 - acc: 0.9935 - val_loss: 5.1386 - val_acc: 0.3990\n",
      "Epoch 334/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0451 - acc: 0.9934 - val_loss: 5.1161 - val_acc: 0.3973\n",
      "Epoch 335/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0449 - acc: 0.9935 - val_loss: 5.0958 - val_acc: 0.3880\n",
      "Epoch 336/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0438 - acc: 0.9941 - val_loss: 5.1066 - val_acc: 0.3918\n",
      "Epoch 337/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0430 - acc: 0.9937 - val_loss: 5.1224 - val_acc: 0.3848\n",
      "Epoch 338/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0435 - acc: 0.9937 - val_loss: 5.0992 - val_acc: 0.3921\n",
      "Epoch 339/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0430 - acc: 0.9937 - val_loss: 5.0287 - val_acc: 0.3914\n",
      "Epoch 340/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0420 - acc: 0.9940 - val_loss: 5.1112 - val_acc: 0.3952\n",
      "Epoch 341/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0412 - acc: 0.9940 - val_loss: 5.0602 - val_acc: 0.3912\n",
      "Epoch 342/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0410 - acc: 0.9938 - val_loss: 5.1275 - val_acc: 0.3950\n",
      "Epoch 343/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0414 - acc: 0.9941 - val_loss: 5.1788 - val_acc: 0.3890\n",
      "Epoch 344/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0447 - acc: 0.9932 - val_loss: 5.0639 - val_acc: 0.3895\n",
      "Epoch 345/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0440 - acc: 0.9935 - val_loss: 5.0725 - val_acc: 0.3802\n",
      "Epoch 346/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0414 - acc: 0.9941 - val_loss: 5.1791 - val_acc: 0.3901\n",
      "Epoch 347/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0419 - acc: 0.9942 - val_loss: 5.1021 - val_acc: 0.3768\n",
      "Epoch 348/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0472 - acc: 0.9928 - val_loss: 5.1664 - val_acc: 0.3890\n",
      "Epoch 349/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0432 - acc: 0.9935 - val_loss: 5.2257 - val_acc: 0.3976\n",
      "Epoch 350/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0423 - acc: 0.9937 - val_loss: 5.0622 - val_acc: 0.3814\n",
      "Epoch 351/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0427 - acc: 0.9934 - val_loss: 5.1791 - val_acc: 0.3993\n",
      "Epoch 352/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0450 - acc: 0.9931 - val_loss: 5.1393 - val_acc: 0.3890\n",
      "Epoch 353/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0420 - acc: 0.9936 - val_loss: 5.2403 - val_acc: 0.3891\n",
      "Epoch 354/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0415 - acc: 0.9937 - val_loss: 5.1441 - val_acc: 0.3851\n",
      "Epoch 355/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0444 - acc: 0.9928 - val_loss: 5.1052 - val_acc: 0.3786\n",
      "Epoch 356/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0425 - acc: 0.9937 - val_loss: 5.1735 - val_acc: 0.3822\n",
      "Epoch 357/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0424 - acc: 0.9936 - val_loss: 5.2530 - val_acc: 0.3929\n",
      "Epoch 358/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0420 - acc: 0.9932 - val_loss: 5.2668 - val_acc: 0.3872\n",
      "Epoch 359/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0400 - acc: 0.9940 - val_loss: 5.2369 - val_acc: 0.3901\n",
      "Epoch 360/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0403 - acc: 0.9941 - val_loss: 5.2013 - val_acc: 0.3833\n",
      "Epoch 361/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0386 - acc: 0.9944 - val_loss: 5.2098 - val_acc: 0.3808\n",
      "Epoch 362/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0382 - acc: 0.9942 - val_loss: 5.3043 - val_acc: 0.3861\n",
      "Epoch 363/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0393 - acc: 0.9942 - val_loss: 5.3199 - val_acc: 0.3890\n",
      "Epoch 364/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0398 - acc: 0.9943 - val_loss: 5.2735 - val_acc: 0.3869\n",
      "Epoch 365/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0377 - acc: 0.9942 - val_loss: 5.3187 - val_acc: 0.3804\n",
      "Epoch 366/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0375 - acc: 0.9946 - val_loss: 5.4103 - val_acc: 0.3941\n",
      "Epoch 367/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0379 - acc: 0.9944 - val_loss: 5.3568 - val_acc: 0.3882\n",
      "Epoch 368/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0378 - acc: 0.9942 - val_loss: 5.2795 - val_acc: 0.3808\n",
      "Epoch 369/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0374 - acc: 0.9945 - val_loss: 5.3274 - val_acc: 0.3900\n",
      "Epoch 370/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0374 - acc: 0.9943 - val_loss: 5.2855 - val_acc: 0.3782\n",
      "Epoch 371/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0382 - acc: 0.9942 - val_loss: 5.3037 - val_acc: 0.3852\n",
      "Epoch 372/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0376 - acc: 0.9944 - val_loss: 5.3770 - val_acc: 0.3881\n",
      "Epoch 373/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0367 - acc: 0.9947 - val_loss: 5.3301 - val_acc: 0.3860\n",
      "Epoch 374/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0366 - acc: 0.9948 - val_loss: 5.2693 - val_acc: 0.3752\n",
      "Epoch 375/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0374 - acc: 0.9944 - val_loss: 5.3016 - val_acc: 0.3821\n",
      "Epoch 376/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0368 - acc: 0.9945 - val_loss: 5.3914 - val_acc: 0.3793\n",
      "Epoch 377/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0380 - acc: 0.9944 - val_loss: 5.3911 - val_acc: 0.3903\n",
      "Epoch 378/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0374 - acc: 0.9944 - val_loss: 5.4000 - val_acc: 0.3852\n",
      "Epoch 379/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0420 - acc: 0.9936 - val_loss: 5.3906 - val_acc: 0.3881\n",
      "Epoch 380/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0439 - acc: 0.9932 - val_loss: 5.3916 - val_acc: 0.3875\n",
      "Epoch 381/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0441 - acc: 0.9931 - val_loss: 5.3148 - val_acc: 0.3772\n",
      "Epoch 382/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0412 - acc: 0.9934 - val_loss: 5.4190 - val_acc: 0.3899\n",
      "Epoch 383/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0405 - acc: 0.9937 - val_loss: 5.2216 - val_acc: 0.3765\n",
      "Epoch 384/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0401 - acc: 0.9939 - val_loss: 5.2663 - val_acc: 0.3700\n",
      "Epoch 385/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0433 - acc: 0.9934 - val_loss: 5.4049 - val_acc: 0.3757\n",
      "Epoch 386/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0423 - acc: 0.9935 - val_loss: 5.3572 - val_acc: 0.3800\n",
      "Epoch 387/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0416 - acc: 0.9936 - val_loss: 5.2923 - val_acc: 0.3776\n",
      "Epoch 388/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0412 - acc: 0.9937 - val_loss: 5.3948 - val_acc: 0.3854\n",
      "Epoch 389/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0406 - acc: 0.9940 - val_loss: 5.2922 - val_acc: 0.3822\n",
      "Epoch 390/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0395 - acc: 0.9942 - val_loss: 5.4143 - val_acc: 0.3782\n",
      "Epoch 391/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0391 - acc: 0.9939 - val_loss: 5.5497 - val_acc: 0.3912\n",
      "Epoch 392/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0384 - acc: 0.9945 - val_loss: 5.4178 - val_acc: 0.3737\n",
      "Epoch 393/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0375 - acc: 0.9942 - val_loss: 5.3095 - val_acc: 0.3764\n",
      "Epoch 394/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0370 - acc: 0.9944 - val_loss: 5.3945 - val_acc: 0.3764\n",
      "Epoch 395/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0372 - acc: 0.9945 - val_loss: 5.3919 - val_acc: 0.3847\n",
      "Epoch 396/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0365 - acc: 0.9945 - val_loss: 5.4058 - val_acc: 0.3797\n",
      "Epoch 397/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0376 - acc: 0.9944 - val_loss: 5.4456 - val_acc: 0.3807\n",
      "Epoch 398/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0362 - acc: 0.9948 - val_loss: 5.4679 - val_acc: 0.3804\n",
      "Epoch 399/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0353 - acc: 0.9949 - val_loss: 5.4537 - val_acc: 0.3909\n",
      "Epoch 400/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0363 - acc: 0.9948 - val_loss: 5.4366 - val_acc: 0.3868\n",
      "Epoch 401/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0357 - acc: 0.9947 - val_loss: 5.4809 - val_acc: 0.3856\n",
      "Epoch 402/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0353 - acc: 0.9946 - val_loss: 5.4916 - val_acc: 0.3684\n",
      "Epoch 403/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0356 - acc: 0.9946 - val_loss: 5.4708 - val_acc: 0.3762\n",
      "Epoch 404/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0353 - acc: 0.9946 - val_loss: 5.5217 - val_acc: 0.3825\n",
      "Epoch 405/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0365 - acc: 0.9945 - val_loss: 5.4856 - val_acc: 0.3827\n",
      "Epoch 406/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0364 - acc: 0.9944 - val_loss: 5.4740 - val_acc: 0.3811\n",
      "Epoch 407/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0362 - acc: 0.9944 - val_loss: 5.4648 - val_acc: 0.3840\n",
      "Epoch 408/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0359 - acc: 0.9946 - val_loss: 5.4772 - val_acc: 0.3789\n",
      "Epoch 409/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0353 - acc: 0.9947 - val_loss: 5.5462 - val_acc: 0.3863\n",
      "Epoch 410/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0354 - acc: 0.9946 - val_loss: 5.4886 - val_acc: 0.3833\n",
      "Epoch 411/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0357 - acc: 0.9946 - val_loss: 5.5140 - val_acc: 0.3866\n",
      "Epoch 412/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0351 - acc: 0.9947 - val_loss: 5.5192 - val_acc: 0.3875\n",
      "Epoch 413/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0346 - acc: 0.9948 - val_loss: 5.5073 - val_acc: 0.3799\n",
      "Epoch 414/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0349 - acc: 0.9947 - val_loss: 5.5822 - val_acc: 0.3826\n",
      "Epoch 415/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0355 - acc: 0.9946 - val_loss: 5.4344 - val_acc: 0.3783\n",
      "Epoch 416/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0355 - acc: 0.9947 - val_loss: 5.4897 - val_acc: 0.3788\n",
      "Epoch 417/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0345 - acc: 0.9950 - val_loss: 5.5713 - val_acc: 0.3827\n",
      "Epoch 418/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0359 - acc: 0.9946 - val_loss: 5.5220 - val_acc: 0.3764\n",
      "Epoch 419/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0357 - acc: 0.9946 - val_loss: 5.4982 - val_acc: 0.3779\n",
      "Epoch 420/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0346 - acc: 0.9946 - val_loss: 5.5257 - val_acc: 0.3783\n",
      "Epoch 421/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0352 - acc: 0.9945 - val_loss: 5.5653 - val_acc: 0.3752\n",
      "Epoch 422/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0355 - acc: 0.9947 - val_loss: 5.4918 - val_acc: 0.3826\n",
      "Epoch 423/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0345 - acc: 0.9950 - val_loss: 5.4784 - val_acc: 0.3763\n",
      "Epoch 424/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0342 - acc: 0.9949 - val_loss: 5.5487 - val_acc: 0.3863\n",
      "Epoch 425/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0347 - acc: 0.9948 - val_loss: 5.5203 - val_acc: 0.3754\n",
      "Epoch 426/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0342 - acc: 0.9949 - val_loss: 5.5149 - val_acc: 0.3800\n",
      "Epoch 427/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0348 - acc: 0.9949 - val_loss: 5.5151 - val_acc: 0.3825\n",
      "Epoch 428/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0343 - acc: 0.9948 - val_loss: 5.5828 - val_acc: 0.3808\n",
      "Epoch 429/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0349 - acc: 0.9948 - val_loss: 5.5621 - val_acc: 0.3700\n",
      "Epoch 430/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0411 - acc: 0.9934 - val_loss: 5.3820 - val_acc: 0.3721\n",
      "Epoch 431/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0393 - acc: 0.9940 - val_loss: 5.6987 - val_acc: 0.3885\n",
      "Epoch 432/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0468 - acc: 0.9915 - val_loss: 5.5735 - val_acc: 0.3826\n",
      "Epoch 433/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0444 - acc: 0.9927 - val_loss: 5.3575 - val_acc: 0.3713\n",
      "Epoch 434/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0422 - acc: 0.9929 - val_loss: 5.4143 - val_acc: 0.3751\n",
      "Epoch 435/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0380 - acc: 0.9943 - val_loss: 5.4676 - val_acc: 0.3810\n",
      "Epoch 436/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0383 - acc: 0.9939 - val_loss: 5.4845 - val_acc: 0.3795\n",
      "Epoch 437/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0382 - acc: 0.9939 - val_loss: 5.5065 - val_acc: 0.3713\n",
      "Epoch 438/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0393 - acc: 0.9939 - val_loss: 5.5043 - val_acc: 0.3712\n",
      "Epoch 439/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0370 - acc: 0.9944 - val_loss: 5.5338 - val_acc: 0.3667\n",
      "Epoch 440/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0379 - acc: 0.9941 - val_loss: 5.6627 - val_acc: 0.3831\n",
      "Epoch 441/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0392 - acc: 0.9938 - val_loss: 5.5677 - val_acc: 0.3856\n",
      "Epoch 442/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0367 - acc: 0.9942 - val_loss: 5.5123 - val_acc: 0.3627\n",
      "Epoch 443/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0382 - acc: 0.9940 - val_loss: 5.5365 - val_acc: 0.3580\n",
      "Epoch 444/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0369 - acc: 0.9943 - val_loss: 5.5620 - val_acc: 0.3732\n",
      "Epoch 445/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0392 - acc: 0.9939 - val_loss: 5.4341 - val_acc: 0.3775\n",
      "Epoch 446/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0417 - acc: 0.9930 - val_loss: 5.5137 - val_acc: 0.3812\n",
      "Epoch 447/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0453 - acc: 0.9922 - val_loss: 5.5458 - val_acc: 0.3851\n",
      "Epoch 448/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0429 - acc: 0.9922 - val_loss: 5.4904 - val_acc: 0.3787\n",
      "Epoch 449/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0432 - acc: 0.9929 - val_loss: 5.7068 - val_acc: 0.3782\n",
      "Epoch 450/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0471 - acc: 0.9919 - val_loss: 5.6575 - val_acc: 0.3970\n",
      "Epoch 451/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0406 - acc: 0.9935 - val_loss: 5.5232 - val_acc: 0.3811\n",
      "Epoch 452/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0386 - acc: 0.9939 - val_loss: 5.4430 - val_acc: 0.3804\n",
      "Epoch 453/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0367 - acc: 0.9942 - val_loss: 5.5384 - val_acc: 0.3811\n",
      "Epoch 454/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0371 - acc: 0.9942 - val_loss: 5.5187 - val_acc: 0.3811\n",
      "Epoch 455/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0368 - acc: 0.9943 - val_loss: 5.5591 - val_acc: 0.3880\n",
      "Epoch 456/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0388 - acc: 0.9941 - val_loss: 5.5575 - val_acc: 0.3792\n",
      "Epoch 457/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0411 - acc: 0.9937 - val_loss: 5.7204 - val_acc: 0.3851\n",
      "Epoch 458/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0386 - acc: 0.9938 - val_loss: 5.4796 - val_acc: 0.3767\n",
      "Epoch 459/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0366 - acc: 0.9943 - val_loss: 5.4975 - val_acc: 0.3758\n",
      "Epoch 460/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0395 - acc: 0.9939 - val_loss: 5.5387 - val_acc: 0.3871\n",
      "Epoch 461/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0383 - acc: 0.9940 - val_loss: 5.6760 - val_acc: 0.3909\n",
      "Epoch 462/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0377 - acc: 0.9939 - val_loss: 5.5947 - val_acc: 0.3786\n",
      "Epoch 463/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0358 - acc: 0.9945 - val_loss: 5.5777 - val_acc: 0.3757\n",
      "Epoch 464/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0361 - acc: 0.9945 - val_loss: 5.5753 - val_acc: 0.3847\n",
      "Epoch 465/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0367 - acc: 0.9943 - val_loss: 5.6006 - val_acc: 0.3772\n",
      "Epoch 466/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0363 - acc: 0.9941 - val_loss: 5.6052 - val_acc: 0.3899\n",
      "Epoch 467/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0350 - acc: 0.9944 - val_loss: 5.6130 - val_acc: 0.3898\n",
      "Epoch 468/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0360 - acc: 0.9945 - val_loss: 5.5861 - val_acc: 0.3837\n",
      "Epoch 469/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0346 - acc: 0.9948 - val_loss: 5.5627 - val_acc: 0.3792\n",
      "Epoch 470/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0358 - acc: 0.9945 - val_loss: 5.7043 - val_acc: 0.3863\n",
      "Epoch 471/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0360 - acc: 0.9945 - val_loss: 5.6348 - val_acc: 0.3807\n",
      "Epoch 472/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0351 - acc: 0.9946 - val_loss: 5.5914 - val_acc: 0.3822\n",
      "Epoch 473/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0357 - acc: 0.9946 - val_loss: 5.6358 - val_acc: 0.3804\n",
      "Epoch 474/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0367 - acc: 0.9944 - val_loss: 5.6881 - val_acc: 0.3833\n",
      "Epoch 475/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0369 - acc: 0.9941 - val_loss: 5.7117 - val_acc: 0.3768\n",
      "Epoch 476/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0368 - acc: 0.9945 - val_loss: 5.5985 - val_acc: 0.3854\n",
      "Epoch 477/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0370 - acc: 0.9942 - val_loss: 5.6539 - val_acc: 0.3811\n",
      "Epoch 478/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0346 - acc: 0.9948 - val_loss: 5.6147 - val_acc: 0.3793\n",
      "Epoch 479/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0361 - acc: 0.9945 - val_loss: 5.6525 - val_acc: 0.3769\n",
      "Epoch 480/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0352 - acc: 0.9946 - val_loss: 5.6170 - val_acc: 0.3824\n",
      "Epoch 481/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0353 - acc: 0.9947 - val_loss: 5.6371 - val_acc: 0.3786\n",
      "Epoch 482/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0348 - acc: 0.9948 - val_loss: 5.6501 - val_acc: 0.3901\n",
      "Epoch 483/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0358 - acc: 0.9945 - val_loss: 5.6690 - val_acc: 0.3828\n",
      "Epoch 484/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0353 - acc: 0.9948 - val_loss: 5.6474 - val_acc: 0.3765\n",
      "Epoch 485/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0360 - acc: 0.9946 - val_loss: 5.5601 - val_acc: 0.3774\n",
      "Epoch 486/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0357 - acc: 0.9947 - val_loss: 5.6796 - val_acc: 0.3775\n",
      "Epoch 487/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0355 - acc: 0.9946 - val_loss: 5.5876 - val_acc: 0.3759\n",
      "Epoch 488/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0356 - acc: 0.9947 - val_loss: 5.6008 - val_acc: 0.3811\n",
      "Epoch 489/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0349 - acc: 0.9948 - val_loss: 5.6936 - val_acc: 0.3825\n",
      "Epoch 490/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0348 - acc: 0.9946 - val_loss: 5.6742 - val_acc: 0.3874\n",
      "Epoch 491/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0340 - acc: 0.9950 - val_loss: 5.6136 - val_acc: 0.3857\n",
      "Epoch 492/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0339 - acc: 0.9951 - val_loss: 5.6226 - val_acc: 0.3807\n",
      "Epoch 493/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0342 - acc: 0.9948 - val_loss: 5.6295 - val_acc: 0.3775\n",
      "Epoch 494/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0345 - acc: 0.9948 - val_loss: 5.6731 - val_acc: 0.3822\n",
      "Epoch 495/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0344 - acc: 0.9949 - val_loss: 5.6883 - val_acc: 0.3809\n",
      "Epoch 496/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0346 - acc: 0.9948 - val_loss: 5.6371 - val_acc: 0.3858\n",
      "Epoch 497/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0331 - acc: 0.9948 - val_loss: 5.6778 - val_acc: 0.3815\n",
      "Epoch 498/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0337 - acc: 0.9948 - val_loss: 5.6613 - val_acc: 0.3825\n",
      "Epoch 499/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0338 - acc: 0.9949 - val_loss: 5.6643 - val_acc: 0.3764\n",
      "Epoch 500/500\n",
      "22708/22708 [==============================] - 33s 1ms/step - loss: 0.0328 - acc: 0.9951 - val_loss: 5.6824 - val_acc: 0.3791\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f51bee3e2b0>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 20
    }
   ]
  }
 ]
}